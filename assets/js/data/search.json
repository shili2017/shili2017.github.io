[ { "title": "《安全处理器架构设计原理》笔记（一）：基础", "url": "/posts/PSPAD1/", "categories": "读书笔记, 安全处理器架构设计原理", "tags": "", "date": "2023-06-14 03:41:00 +0000", "snippet": "《安全处理器架构设计原理》（英文名为Principles of Secure Processor Architecture Design）隶属于Synthesis Lectures on Computer Architecture系列。硬件不同于软件那样容易被破解，且基于硬件的安全保护效率更高，对系统性能的影响更小，但在硬件中增加安全功能也充满挑战，一旦实现之后就很难改变，需要仔细设计，且设...", "content": "《安全处理器架构设计原理》（英文名为Principles of Secure Processor Architecture Design）隶属于Synthesis Lectures on Computer Architecture系列。硬件不同于软件那样容易被破解，且基于硬件的安全保护效率更高，对系统性能的影响更小，但在硬件中增加安全功能也充满挑战，一旦实现之后就很难改变，需要仔细设计，且设计时需要考虑到适用于未来的功能和算法。背景安全处理器架构在设计上提供了额外功能，这些功能可以是纯硬件的，也可以是硬件和软件结合实现的。安全处理器架构一般是商业处理器的扩展，基于x86等架构实现，这些功能的出现基于以下三个因素： 软件的复杂性和缺陷：诸如操作系统、虚拟机之类的软件复杂性和规模逐渐上升，仅仅基于软件来提供安全性是不现实甚至不可能的，有必要实现硬件安全功能（如特权级或可信执行环境）来提供一个特殊的执行环境来执行部分可信代码。 侧信道攻击：许多应用运行在云上，不同的用户共享相同的物理硬件，攻击者可以通过时间、功率、电磁等侧信道来攻击其他用户应用中的敏感信息，只有通过架构和硬件层面的改进才能缓解不同侧信道带来的影响。 物理攻击：可以通过对内存总线甚至内存本身的监测来实现攻击，且随着云计算、嵌入式设备、物联网的普及，用户可能不再能够控制软件运行的硬件，需要新的机制来保护代码和数据。 Trusted Computing Base (TCB)TCB是指为实现计算机系统安全保护的所有安全保护机制的集合。在安全处理器架构中，TCB由硬件和软件共同组成来提供一些安全功能。硬件安全架构中，所有安全机制都仅由硬件实现，硬件-软件安全架构中，硬件和软件（通常是操作系统、虚拟机等）一起工作来保护其他软件，后者可以提供更大的灵活性，但是会增大TCB，可能会导致更多的安全漏洞。安全处理器架构的目标是确保可信的硬件和可信的软件共同工作，为系统上运行的软件提供安全性和保护。不可信的部分（包括硬件和软件）不一定是恶意的，只是不被信任。我们需要假设每个不受信任的硬件或软件都可能会破坏系统安全，同时也要考虑到外部攻击者，也就是对计算机系统的物理攻击。可信的部分也不一定是安全的，其中也可能包含漏洞和错误，但是在设计和实现过程中，需要确保这些部分确实是安全的。柯克霍夫原则（Kerckhoffs’s Principle）该原则说明，安全系统的TCB应该是公开的，除了加密密钥本身，其他部分不应该有任何隐藏。已经有许多反例，通过隐藏实现细节来实现安全功能，希望攻击者难以通过对系统进行逆向工程来破解，但设计者不应该低估任何攻击者和攻击形式。安全威胁攻击面安全处理器的潜在攻击面攻击面是所有可用于攻击系统的载体的集合，攻击向量是攻击者破坏系统安全的方式，上图展示了不同类型的攻击向量（左边）和系统中可能成为攻击目标的部分（右边）。载体可以是硬件或软件，可以由外部或内部进行攻击，硬件攻击可能来自不受信任的硬件或外部物理攻击，软件攻击可能来自不受信任的软件或本应受保护的软件，攻击目标可以是TCB硬件、软件、或其他受保护的软件。软件对软件的攻击可能是不受信任的操作系统攻击受保护的软件，软件对硬件的攻击可能是不受信任的软件使用缓存侧信道攻击以获取信息，硬件对硬件的攻击可能是不受信任的外设试图禁用内存加密。被动和主动攻击被动攻击是只观察系统的运行情况的攻击类型，比如可以监听系统内部或外部的通信来试图推断敏感信息或代码，典型的例子包括侧信道攻击，从系统中收集时间、功耗等数据。主动攻击指攻击者修改系统代码或数据，比如写入一些内存位置（如尝试改变内存中的加密密钥）或执行一系列的指令（如加热芯片导致其失效），对处理器的物理攻击也属于这一类攻击。这些攻击可以进一步分类为窥探（snooping）、欺骗（spoofing）、拼接（splicing）、重放（replay）、干扰（disturbance）。中间人（Man-in-the-middle）攻击中间人攻击是对通信的攻击，拦截两个受信任的组件之间的通信。中间人攻击可以是被动的（接收数据，读取数据，然后发送到目的地，不做任何修改），也可以是主动的（接收数据后修改数据或注入一些新数据）。侧信道和隐蔽信道攻击隐蔽信道是一种本来并非用于在发送方和接收方之间传递信息的通信信道，通常利用一些特殊方法进行通信，比如计时、功率、热辐射、电磁辐射、声音等。设计时可以进行预防，例如通过适当的进程隔离或对缓存进行分区，但是许多隐蔽信道在设计时是没有考虑到的，有许多方法可以绕开这些隔离机制。侧信道类似于隐蔽信道，但是发送方并不打算向接收方传递信息。类似于隐蔽信道，攻击者也可以通过计时等信息来获取信息。对隐蔽信道攻击而言，发送方和接收方都在攻击者控制之下，而对侧信道来说，发送方不受攻击者的控制。同样，设计时很难考虑到各种侧信道。处理器的侧信道和隐蔽信道一般可分为基于时间的信道（例如一个进程大量访问内存导致其他进程内存访问变慢）、基于访问的信道（例如一个进程通过对自己的内存访问进行计时来探测缓存状态）、基于痕迹（trace）的信道（例如攻击者获得内存访问的序列，并通过测量功率来推断缓存是否命中）。设计阶段后的硬件威胁安全处理器设计的重点是最小化TCB，并对其进行保护，所有的安全保护措施都取决于TCB的可信度，在设计时应该尝试验证TCB。然而，当硬件和软件被设计出来后，仍然有许多潜在的威胁： TCB中的缺陷或漏洞：按照定义，TCB是完全可信的，但软件和硬件设计安全验证本身就是个很大的研究领域，TCB中仍然可能存在缺陷和漏洞，需要注意如何避免。 硬件木马和供应链攻击：处理器可能包含多个供应商的IP块，整个系统会在多个环节中进行设计和制造，然后再被交付给客户，这个环节中的每一方都可能在其中插入硬件木马，这也是个很大的研究领域。 物理探测和侵入性攻击：真实设备中的处理器可能被轻易用物理手段进行攻击，比如通过标准接口从设备中读出数据（例如移除DRAM芯片，放在另一台计算机中来读取其中的数据）。侵入性攻击如移除处理器或存储器的封装来直接访问芯片上的电路，可能通过蚀刻（etching）或钻孔（drilling），使用光学显微镜和小型金属探针来检查电路，更复杂的攻击如可以使用聚焦离子束（focused ion beam, FIB）来探测芯片上的金属层和CMOS线路，可以通过增加互连线甚至添加新的晶体管来修改芯片结构。 基本安全概念保密性（Confidentiality）、完整性（Integrity）和可用性（Availability）保密性指防止将敏感信息泄露给未经授权的用户，完整性指防止随意修改敏感信息而不被发现，可用性指在有需求时可以向用户提供服务。对保密性来说，注意即使攻击者只获得部分敏感信息也是危险的，可以根据这些部分信息来进行暴力破解或猜测。完整性攻击不要求攻击者了解任何信息，只需要修改某些数据就可以破坏系统，比如某个保护位或是内存系统中的部分数据。攻击者可以通过耗尽系统中的内存或其他资源来使受保护的代码无法在合理时间内执行，让系统无法为用户提供服务。认证（Authentication）认证也就是说确定用户是谁，通常的方法是密码。认证本质上需要完整性，攻击者不能修改认证信息，也不能编造自己的信息。安全性和可靠性安全性的前提是假设系统已经可以提供可靠性，也就是对随机故障或错误的保护，比如宇宙射线导致DRAM中数据出现错误。当系统不够可靠时，系统仍然不应该向潜在的攻击者传递任何信息，也不应该允许修改信息。对称密钥加密、公钥加密、随机数生成、哈希运算、物理不可克隆函数（PUF）这一部分属于密码学的范畴，感兴趣的读者可以阅读原书相关章节或查阅资料。有四个数据叶节点的哈希树，根节点的哈希值取决于所有数据节点的值" }, { "title": "Chisel笔记（三）：一生一芯第三期项目中使用Chisel语言的一些心得体会", "url": "/posts/CHISEL3/", "categories": "Chisel", "tags": "", "date": "2023-04-23 19:41:00 +0000", "snippet": "知乎原回答：https://www.zhihu.com/question/468593551/answer/2996304673差不多正好两年前开始接触体系结构这个领域，通过一生一芯第一期项目和rocket-chip了解到了chisel这个语言，在2021年下半年参加了一生一芯第三期，也许也算是有资格来回答这个问题吧w，chisel的优缺点楼上各位大佬已经讨论的非常充分了，或许我会更多从自己...", "content": "知乎原回答：https://www.zhihu.com/question/468593551/answer/2996304673差不多正好两年前开始接触体系结构这个领域，通过一生一芯第一期项目和rocket-chip了解到了chisel这个语言，在2021年下半年参加了一生一芯第三期，也许也算是有资格来回答这个问题吧w，chisel的优缺点楼上各位大佬已经讨论的非常充分了，或许我会更多从自己项目经历和主观体验的角度，作为chisel语言的用户来聊一聊，希望各位大佬前辈多多批评。首先放上自己一生一芯第三期的项目链接：https://github.com/OSCPU-Zhoushan/Zhoushan我自己的背景是从大二的数电课开始接触verilog，本科毕设做处理器设计的时候有考虑过用chisel，和队友讨论下来还是不要选这些稀奇古怪的新语言了。不过后来8月中旬开始一生一芯第三期之后，还是没忍住诱惑，花了几天的时间快速入门了一下chisel，大概两三天的时间就写出了一个基本的五级流水线，最后10月初提交的版本是一个RV64I的乱序流水线。Chisel本身作为一个HDL generator似乎很容易引起一些误解。首先chisel本身仍然是HDL，不是HLS，仍然需要很清楚地描述每一个wire和每一个register，因此有verilog背景的同学上手chisel是很快的（唯一的障碍可能是scala语言本身），我自己也非常不推荐cs的同学把chisel作为自己的第一个HDL语言去学。用chisel做IC开发本身的流程也和传统的流程没任何区别，仅仅是在RTL设计这一步换了个语言而已，就像写OS写数据库，并不会因为我把C换成rust，我就立马功力大增，写出了性能很好非常可靠的软件一样。题外话：题主以及很多参加一生一芯的同学会觉得这个项目里cs背景的人更多也是很合理的，项目本身并不单纯是写写verilog、设计一个处理器就结束了，还包括了很多其他方面的学习，比如当我在程序里写下int x = 10这一句话的时候，实际上到底发生了什么事情，完整学习完一生一芯之后应该可以很从容回答这个问题了。（这个例子出自CMU 15-418 Parallel Computer Architecture &amp; Programming）出自 CMU 15-418 Spring 2023 Lecture 12 Snooping-based multi-processor implementation我自己在写chisel的时候，初印象还不错，有时候会想HDL早就该这么设计了。一开始觉得不用像verilog那样手动连线了当然很爽，然后马上也会发现，因为是基于scala的一个DSL，scala的好东西chisel也都有，比如各种面向对象、类型推断、函数式编程之类的特性。比如ICache和DCache的接口可能非常相近，但还是有点差别，完全可以通过面向对象的特性来简化代码，提高可读性。一生一芯结束之后，后续的一些RTL项目我基本都选择用chisel开发，还有很多用上了就回不去了的东西，比如chisel的高度参数化和很多好用的IP。在一生一芯项目期间我基本还只是把chisel当成了一个比较好写的HDL来使用，后面进一步了解之后发现rocket-chip本身就是个宝藏项目，比如有了diplomacy之后，不再需要去考虑手动计算某个总线的地址宽度应该是多少，或者某个crossbar应该怎么去连之类的问题，这些确实是verilog本身暂时做不到的事情。如果是做RISC-V处理器或者基于TileLink/AXI4总线的IP开发的话，rocket-chip和chipyard还有非常多好用的IP可以直接拿来用（https://chipyard.readthedocs.io/en/stable/TileLink-Diplomacy-Reference/index.html）。可以说确实在SoC集成的工作上chisel能节省不少工作量。我和同学在聊到这个语言的时候偶尔会拿来和rust比较，但是个人看来chisel在HDL领域似乎远没有rust在编程语言领域那样成功，并且虽然自己非常喜欢这个语言，但是显然很长一段时间内chisel/firrtl这一套语言还不太会在工业界大规模应用。首先chisel毕竟还是个和verilog站在同一层次的语言，大家确实没有很强的动力把verilog换成chisel，有的事情chisel做起来和verilog同样麻烦，比如跨时钟域，比如异步复位（似乎目前还没有一个选项可以调同步或异步复位？）。其次chisel这个语言本身目前还在快速迭代，很多chisel 3.4还跑的好好的代码，到了3.5就说马上要deprecate了，原来0 warning的代码，更新了个版本马上几百个warning，说不准下次再更新一次chisel版本就变error了，很担心现在很多开源IP在10年之后还能不能顺利集成到那会的项目里面去。提到开源IP，也真的非常想吐槽一下rocket-chip、boom、香山以及各种各样的开源项目，似乎都有这样的问题（也可能仅仅是我觉得这样做有问题…）。虽然很多chisel的语言特性用上确实很不错，但也能看得出来有些地方单纯是在炫技，比如明明可以用for循环用三四行实现的代码，一定要用函数式的写法在一行里写完，不只是降低可读性，还很难看出来哪根线连到哪。顺着这一点继续说，chisel本身生成的verilog可读性就比较差，因为上述提到的各种新特性，生成的verilog几乎不可控，之后想debug做验证反而要花更多时间。以前verilog可能是有bug去看波形，找到代码的位置，改代码，现在是有bug先去看chisel代码，对着chisel代码找到verilog里的代码，找不到的话有时候还要加上dontTouch或者改写chisel代码，重新编译重新仿真，基本用chisel在RTL开发上省下来的时间全都在验证上找补回来了。最后回到主题，问chisel会不会影响verilog工程师，有点像是问rust会不会影响C++工程师，我一直觉得使用什么语言只能决定一个工程师的水平下限，就好比rust编译器可以保证一个人不会写出内存泄漏满天飞的代码，chisel可以保证不会写出带latch的数字电路一样，但是决定一个人水平上限仍然还有很多其他东西。" }, { "title": "2023年读书记录", "url": "/posts/BOOKS/", "categories": "其他", "tags": "", "date": "2023-03-19 17:08:00 +0000", "snippet": "很久没有读专业书以外的书了，也确实很久没有给自己喘一口气的机会了，决定开个新帖，记录一下2023年已经看完的书。 2023/03/13 《海外征程3》 2023/03/19 《挽救计划》 2023/04/16 《可能性的艺术：比较政治学30讲》 2023/05/09 《圆圈正义：作为自由前提的信念》 ", "content": "很久没有读专业书以外的书了，也确实很久没有给自己喘一口气的机会了，决定开个新帖，记录一下2023年已经看完的书。 2023/03/13 《海外征程3》 2023/03/19 《挽救计划》 2023/04/16 《可能性的艺术：比较政治学30讲》 2023/05/09 《圆圈正义：作为自由前提的信念》 " }, { "title": "《虚拟内存的架构和操作系统支持》笔记（六）：异构系统与虚拟化", "url": "/posts/VM6/", "categories": "读书笔记, 虚拟内存的架构和操作系统支持", "tags": "", "date": "2022-11-03 05:04:00 +0000", "snippet": "虚拟内存系统的设计空间正在不断扩展，也带来了全新的挑战，涵盖ISA设计、内存布局和管理、虚拟化等多个方面，虚拟内存实现的复杂度正在成倍增长。由异构系统和虚拟化带来的一些特殊挑战包括以下方面： 当被访问的数据在另一个设备甚至另一台机器上时，内存访问可能有更高的延迟； 设备的虚拟地址空间可能与某些主机进程的虚拟地址空间相同，也可能是其子集，也可能不相关； 很...", "content": "虚拟内存系统的设计空间正在不断扩展，也带来了全新的挑战，涵盖ISA设计、内存布局和管理、虚拟化等多个方面，虚拟内存实现的复杂度正在成倍增长。由异构系统和虚拟化带来的一些特殊挑战包括以下方面： 当被访问的数据在另一个设备甚至另一台机器上时，内存访问可能有更高的延迟； 设备的虚拟地址空间可能与某些主机进程的虚拟地址空间相同，也可能是其子集，也可能不相关； 很难处理page fault，通常加速器上不会运行操作系统； 实现跨设备的缓存一致性是非常困难和昂贵的，但是省略硬件实现的一致性会给程序员带来更大的负担； 面对不同的内存性能特征，对物理内存中的数据进行优化管理（分配、迁移和驱逐）成为一个不可忽视的难题； 在虚拟化系统中，内存访问和虚拟内存管理必须要么通过hypervisor（即增加一个额外的抽象层次），要么依靠专用硬件支持来避免额外的延迟； … 加速器和共享虚拟内存虚拟内存提供在设备间共享基于（虚拟地址）指针的数据结构的能力，GPU、DSP等加速器可以和CPU共享同一个虚拟地址空间。我们现在主要以GPU为例讨论共享虚拟内存，但大部分内容也同样适用于其他设备。早期的GPU需要从CPU以粗粒度发送数据到GPU，但现在的GPU可以共享虚拟地址空间，可以管理自己的内存并动态启动新任务，在某些特殊情况下可以和CPU或其他GPU以细粒度共享数据。对程序员来说，也不再需要单独的内存分配，而是可以直接访问和管理映射的内存。目前关键的限制之一是缺乏对虚拟内存管理的系统级支持，如GPU上没有操作系统，就没有简单的方法来处理page fault。即使是CPU上的操作系统对跨设备共享虚拟内存的支持也有一定的局限性。GPU通常还提供很弱的内存一致性模型，这使得通信更加难以推理，也使如何实现同步指令更有挑战性。GPU微架构的设计师为了最大程度地提高吞吐量，会激进地融合、重排各个内存请求，并且也不一定总是和CPU保持硬件上的一致性，也就是说GPU要么不允许CPU和GPU在一开始就同时访问数据，要么就需要一些特殊的机制（如按需的页面级数据移动），或者甚至根本不缓存那些存在于其他设备上的数据。内存的异构性我们之前可能总是隐含假设了内存访问的延迟和带宽是一致的，也就是统一内存访问（UMA），但在一个系统中有可能某些区域的行为和其他区域不同，即非统一内存访问（NUMA），同一集群内的内存访问延迟较低，不同集群间的内存访问则延迟较高。UMA和NUMA的例子如下图所示。UMA和NUMA系统的例子NUMA虚拟机可能会在分布式系统中的多个不同节点上共享。早期分布式NUMA实现不支持缓存一致性，因此会依赖于过于复杂的编程模型，程序员使用起来比较困难。斯坦福大学的DASH项目是第一个建立可扩展的分布式缓存一致性NUMA（ccNUMA）系统。现在更常见的是，当内存分布在单个系统中的多个处理器上时，就会出现NUMA系统。例如，现在许多服务器级系统在一块主板上有多个处理器，使用板载总线互连，如AMD HyperTransport或Intel QPI来在处理器之间进行通信。单一系统的NUMA通常通过使用复杂的缓存一致性协议（如MOESI、MESIF）来保持一致性，并尽可能减少处理器间的通信。单系统共享虚拟内存空间也正在扩展到外围设备，如GPU。NUMA带来的主要挑战是为优化系统性能而必须进行一系列更复杂的内存管理。最简单的选择是忽略NUMA，但往往会导致性能低下，数据可能被放在很远的地方。相反，在理想情况下，所有的内存都要尽可能地靠近访问的处理器，尽可能减少延迟、提高吞吐量。例如，系统可以选择尽可能靠近某个处理器来分配内存区域，然后采用某种动态页面迁移策略，根据不断变化的系统状态来移动页面。新兴内存技术一个系统中还可能使用不同的物理内存类型，如GPU使用GDDR来取代传统的DDR DRAM，吞吐量更高但功耗也更大，甚至有些高端GPU已经开始采用HBM内存，虚拟内存管理策略需要考虑到这些不同种类的物理内存。甚至有些非易失性的存储器也正在进入内存系统。跨设备通信DMADMA的一个重要方面是，硬件并不总是自动保证DMA操作和CPU的缓存保持一致性。在缓存一致的DMA系统中，DMA请求会自动探测缓存内容，如果需要的话会在执行DMA之前将缓存中的内容更新到内存，但如果不保证缓存一致性的话，则需要软件来手动执行这些操作。IOMMU加速器上的虚拟内存是通过IOMMU来支持的，所有从外部设备到CPU内存的访问，如果未命中设备自己的TLB，都要通过IOMMU来进行地址翻译，且page fault时进行处理。IOMMU和普通MMU的区别在于其相对于管理设备的位置，IOMMU也位于CPU片上，和加速器设备有一定距离，也是所有设备地址翻译请求的中央枢纽，如下图所示。IOMMU为外部设备和加速器进行虚拟内存管理IOMMU使加速器可以从虚拟内存中获益，不仅为外部设备提供虚拟内存功能，也可以确保其不会访问操作系统没有明确定义的任何其他内存区域。MMIO虚拟内存的另一个特殊而重要的用途是MMIO，一些外部通信机制，最常见的是一些外部设备的配置寄存器，被映射到CPU的物理地址空间，映射到进程的虚拟地址空间后，就可以使用一般的读写命令来与设备进行通信。尽管MMIO的抽象对于编程模型和实现的简单性来说是很好的，但把IO请求当作内存请求来处理会带来一些额外的麻烦，如需要小心这些请求不会无限期停留在缓存中，且需要注意这些MMIO请求可能违反内存通常遵循的其他规则，如MMIO的读取可能不会返回最近写到同一地址的值。为了使MMIO能正常工作，大多数处理器都提供了一些机制来指示某些内存区域与某些内存访问应该被视为不可缓存的，会完全绕过各种缓冲区，和fence类似，会保证请求的顺序，其访问通常也是很慢的，需要尽可能减少使用。虚拟化虚拟化是当今实现云基础设施的关键技术，好处是提高了安全性、隔离性、服务器整合能力和容错性，但也带来了性能挑战。理想情况下，系统在虚拟机上运行应用程序时，其性能与在本地运行应用程序时相同，然而，虚拟内存是造成本地和虚拟化应用性能差距的主要因素之一，主要问题在于虚拟化需要两级地址转换。在第一层，guest虚拟地址（gVA）通过guest操作系统页表（gPT）转换为guest物理地址（gPA），然后，gPA通过主机页表（hPT）转换为主机物理地址（hPA）。有两种方法来管理这些页表，分别为嵌套页表和影子页表。嵌套页表虚拟化系统的二维页表遍历，圆圈是guest页表，TLB（没有画在图上）缓存了从请求的gVP到hPP的转换大多数虚拟化系统使用嵌套页表，当guest虚拟机中运行的进程访问内存时，需要把gVA翻译成hPA，如上图所示。Guest的CR3寄存器和请求的gVP一同得出页表第四级的gPP，并开始访问嵌套页表，这个过程如图所示总共需要24次内存访问，会带来一些性能问题，且这些内存访问是顺序、有依赖性的。CPU使用三种类型的结构来加速这一过程： 每个CPU的私有TLB缓存了gVP到hPP的映射，避免了整个页表访问过程； 每个CPU的私有MMU缓存存储中间页表信息来加速部分页表遍历的过程； 每个CPU的私有n级TLB跳过了从GPP到HPP的页表访问过程（如上图弧线箭头所示）。 影子页表影子页表是嵌套页表的替代方法，hypervisor创建一个影子页表，合并了gPT和hPT，存储gVA到hPA的转换。TLB命中的过程和嵌套页表类似，但TLB未命中时，和二维页表遍历不同，影子页表可以通过标准的4个内存访问来进行遍历。影子页表的缺点在于这种页表更新的开销很大，主要问题在于影子页表必须和guest和主机页表保持一致，尤其是guest页表的更新往往很频繁，需要通过hypervisor来更新影子页表，这种机制会严重影响性能。" }, { "title": "《虚拟内存的架构和操作系统支持》笔记（五）：缓存与内存一致性", "url": "/posts/VM5/", "categories": "读书笔记, 虚拟内存的架构和操作系统支持", "tags": "", "date": "2022-10-22 05:27:00 +0000", "snippet": "我们在本章中讨论的基本内容是TLB通常不与内存系统的其他部分保持一致性，也就是说更新页表的操作不会自动传播到TLB中，且旧的TLB条目也不会自动失效。这种不一致性给程序员和操作系统增添了额外的负担，当虚拟内存的状态被更新时，需要明确地进行同步。我们在本章仔细研究TLB和一般情况下指令缓存不保证硬件上的一致性的额原因，接下来研究这种不一致性对程序的同步要求，包括单线程和多线程，最后我们简单讨论...", "content": "我们在本章中讨论的基本内容是TLB通常不与内存系统的其他部分保持一致性，也就是说更新页表的操作不会自动传播到TLB中，且旧的TLB条目也不会自动失效。这种不一致性给程序员和操作系统增添了额外的负担，当虚拟内存的状态被更新时，需要明确地进行同步。我们在本章仔细研究TLB和一般情况下指令缓存不保证硬件上的一致性的额原因，接下来研究这种不一致性对程序的同步要求，包括单线程和多线程，最后我们简单讨论即使是保持了一致性的缓存也会在弱内存一致性模型中发生意外的行为。不一致缓存和TLB尽管大多数CPU都用硬件实现缓存一致性，但通常不会对TLB保持同样的一致性，这也是架构设计中对性能、功耗、面积、易用性之间的复杂权衡。数据缓存保存了频繁读写的数据，且有时由一个以上的核心同时读写，要求软件管理的话对程序员负担过重，因此架构师会使用硬件来实现一致性。相比之下，TLB和指令缓存存储的数据大多是只读的，而且变化的频率相对较低。此外，指令获取和页表遍历经常通过和正常内存数据访问不同的途径进行。因此，将一致性扩展到所有缓存和TLB的开销是很大的，但带来的好处很小。事实上，即使不考虑开销，我们也很难保持TLB的一致性，这是因为TLB条目本身一般不会加上物理地址的tag，但即使加上这一信息，我们注意到组相联TLB是由虚拟地址来索引的，而非页表所在的物理地址，也就是说如果TLB需要参与一致性协议，就必须搜索整个TLB以找到与一致性消息tag相匹配的条目，这一开销是比较大的。在实践中，大多数TLB条目最终都会因为上下文切换或替换而被自然evict，但如果一个旧的TLB条目在哪怕一小段时间内被访问，也会导致错误的行为，因此软件需要确保虚拟系统的同步，我们接下来仔细讨论这一点。TLB Shootdowns在我们讨论TLB如何与页表更新同步之前，让我们先考虑一下，如果不执行这种同步，会出现什么问题。假设一个属于某个多线程进程的页帧正在被换出到磁盘，然后使页表中指向该物理地址范围的PTE失效，此时页表被更新，然而并没有更新TLB，如果操作系统认为原来的物理地址范围是可用的，然后将其他虚拟页面映射到相同的物理地址范围，那么运行在旧TLB条目的线程就有可能非法访问新映射的内存。任何期望访问原始页面的线程的读取都会看到来自新页面的不相关的数据，而任何针对原始页面的写入都不会到达最初的目标物理页面，并且会破坏新页面的数据。这显然是一个正确性的问题，但也会引入安全问题，破坏了虚拟内存抽象的关键要求。如果一个线程能够以某种方式延迟TLB的失效，就可以利用这种特性实现侧信道攻击。例如，一个线程可以在不刷新TLB的情况下释放一个虚拟内存的范围，然后就可以继续从现在未映射的虚拟地址范围中进行读取，同时等待其他进程的页面被载入到该页面对应的原始物理页面中，并访问原本不应该访问到的数据。移除旧TLB条目的过程被称为TLB shootdown，如何执行这一过程的细节因架构不同可能会有很大差异，如可以使用特定的指令或CSR来触发。每个核心可能可以从自己的TLB移除任何旧的条目，但是可能没有机制帮助移除其他核心的TLB旧条目，修改页表的核心需要负责向其他核心发出信号来实现shootdown。无效化的粒度如果一个特定的PTE被修改了，那么其实只有这一项需要从其他核心的TLB中失效，因此架构经常提供一种机制使TLB可以在一个条目的粒度上失效。另一方面，在没有ASID的TLB上进行上下文切换（或在不使用ASID的操作系统下），通常需要将所有非全局TLB条目刷新，如果每次使TLB的一个条目失效，会使上下文切换变得很慢，因此架构也经常提供一条指令来更有效地使整个TLB（或所有非全局TLB条目）失效。在这两个粒度之间有一个灰色区域，粒度的选择可能并不明确。假设一个中等大小的内存范围正在从一个进程的虚拟地址空间中被取消分配，是否需要每次都在这个范围内一页一页地进行循环，还是直接使整个TLB失效更好？这个问题仍然是一个开放的研究和发展领域。下面的代码（/arch/x86/mm/tlb.c）展示了Linux是如何确定逐页无效还是全局无效的。/** See Documentation/x86/tlb.txt for details. We choose 33* because it is large enough to cover the vast majority (at* least 95%) of allocations, and is small enough that we are* confident it will not cause too much overhead. Each single* flush is about 100 ns, so this caps the maximum overhead at* _about_ 3,000 ns.** This is in units of pages.*/static unsigned long tlb_single_page_flush_ceiling __read_mostly = 33;void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start, unsigned long end, unsigned long vmflag){ /* skipping some code... */ if (base_pages_to_flush &gt; tlb_single_page_flush_ceiling) { base_pages_to_flush = TLB_FLUSH_ALL; count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL); local_flush_tlb(); } else { /* flush range by one by one 'invlpg' */ for (addr = start; addr &lt; end; addr += PAGE_SIZE) { count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE); __flush_tlb_single(addr); } } /* skipping some code... */}处理器间中断（IPI）在一些架构上，一个核心只能使自己的TLB无效，可能需要向其他核心发送TLB无效的消息，不同的架构对此的选择不尽相同。ARMv8采用了可以在所有核心上生效的指令，IBM Power提供了不同的指令，既可以使本地TLB失效，也可以使所有核心的TLB都失效。x86架构则两者都不是，而是使用处理器间中断（IPI）。IPI是一种特殊的中断，直接从一个核心发送到另一个核心，或者发送到多个核心，也可能是所有核心，包括发出请求的核心本身。使用IPI可以确保请求被及时处理，大多数处理器采用可编程中断控制器（PIC）来进行处理。下面两个算法是两种不同的使用IPI来执行TLB shootdown的过程。优化TLB ShootdownsTLB shootdown是非常耗时的，占普通进程运行时间的5%到10%，占一个虚拟化进程运行时间的25%，且会在整个系统中增加很多额外开销，因此研究人员提出了很多不同方法来优化TLB shootdown机制。具体的细节比较复杂，可以参考原书。其他细节我们在讨论TLB shootdown时忽略了两个细节。首先是synonym的问题，比如如果一个页帧被换出到磁盘，那么所有指向该页的虚拟地址都需要设为无效，只shootdown同一进程中或者第一个匹配的虚拟地址是不够的，可能还需要执行反向映射查找，遍历所有指向该物理页帧的虚拟地址。另一个需要注意的点是，除了TLB之外，许多架构也有专门的页表缓存，这些缓存也经常会与内存以及TLB不一致，且对软件不可见，在这些系统上当TLB失效时，缓存内容会被简单地清空，这不影响shootdown机制本身，但是也是值得注意的一点。自我修改的代码指令缓存通常是只读的，且通常也不维护硬件缓存一致性。即使指令缓存可以保持一致性，在指令存储器被更新之前，可能处理器已经取走了大量指令，这些指令可能无法保证一致性。尽管指令内存可能很少更新，但是确实会在现实中发生。比如动态linker经常使用PLT来把代码和动态库中的函数链接起来，对PLT的每一次更新都是对指令内存的一次写入；JIT也动态生成代码，并存储到内存中进行执行。更广泛地说，自我修改的代码一般可用于执行任何运行时优化或实现各种低级调试或优化机制。然而，由于指令缓存或流水线可能已经不再保持一致性，需要可以由软件进行刷新，保证更新后的指令可以被正确获取。由于W^X保护，自我修改的代码往往需要操作系统的协助，在代码生成和执行之间的某个时间，存储代码的内存区域的权限需要由R+W切换到R+X。在x86处理器中，硬件可以自动处理大多数情况，然而有两种情况需要更强的同步指令。首先，如果自我修改的代码是在多线程环境下进行的，那么必须以类似于TLB shootdown的机制来通知远程线程，并使用同步指令来更新代码；其次，如果代码修改是通过虚拟地址的synnonym完成的，也同样需要同步指令来进行更新。相比之下，ARM和Power处理器通常无法自动处理大多数自我修改的代码，并需要用户插入明确的fence指令。内存一致性模型我们讨论的最后一个主题是虚拟内存系统中的内存一致性模型。内存模型提供了一套规则，规定了load和store指令的执行顺序，然而精确而完整地定义内存模型是一件非常困难的事情，引入虚拟内存之后则变得甚至更加困难。大多数现代处理器实现了某种形式的弱内存一致性模型，内存访问的执行顺序可能和代码中的顺序不同，如x86的TSO内存模型，或Power和ARM更弱的内存模型，ISA通常会提供一种或多种机制来保证内存访问的顺序。内存模型与虚拟内存系统讨论这个主题时出现的第一个问题是，内存模型是否适用于虚拟内存、或物理内存、或两者同时适用。研究人员认为虚拟内存的一致性和物理内存一致性在本质上是不同的，因为虚拟内存需要处理synnonym、映射变化等问题，这些问题并不存在于物理内存之中。第二个主要问题是如何理解特殊的内存读取，如访问页表和取指，这些和读取数据内存有什么不同。我们已经注意到TLB、页表缓存和指令缓存不一定需要保证缓存一致性，这本身就引入了一些新的行为。在此基础上，访问页表的内存访问可以相对于正常的内存访问进行重排，不需要考虑内存模型本身是否允许这种重排。在许多架构上，通常的fence可能不会对页表访问进行任何约束，如果需要对页表访问也进行约束，可能需要更进一步的fence，如使指令缓存无效、使TLB无效、修改PTW FSM的状态等。例如，ARM架构区分的DMB和DSB两种fence，前者只对普通的load和store进行排序，但后者对所有内存访问（包括取指和访问页表）都进行排序。虚拟内存系统和内存模型之间复杂的相互作用的例子现代虚拟内存系统非常容易出现一些非常微妙的内存访问排序错误。我们观察上图中的例子，这来自于2016年在Linux内核中发现的一个bug。Linux使用cpu_mask来追踪当前进程上下文中活跃的CPU，这个mask用来筛选出需要进行TLB shootdown的CPU，但是可能会出现竞争。假设线程0正在更新当前上下文的页表，并根据cpu_mask来发送TLB shootdown请求，与此同时线程1正在从其他上下文切换到与CPU 0相同的上下文，设置cpu_mask，刷新TLB，并开始执行，假设碰到了刚刚更新的PTE所引用的页面，就会在此时将PTE读取到TLB中。从内存系统的角度来看，访问页表只是特殊的内存读取，简单起见我们就只假设使用单级页表。竞争出现在上图中四个标记的内存访问之间。假设我们现在使用x86-TSO内存模型，这个例子中cpu_mask作为了一种特殊的同步变量，但是(a)和(b)之间没有任何联系，因此(b)可以放到(a)之前，在写入pte指针之前访问cpu_mask指针，这使得(b)、(c)、(d)、(a)这样的内存访问顺序是可以的，在这种情况下，(d)发生在(a)之前，因此线程1会得到旧的PTE，然而线程1的CPU并没有收到shootdown，原因是(b)发生在(c)之前，因此线程0没有看到对应于线程1的CPU被更新。" }, { "title": "《虚拟内存的架构和操作系统支持》笔记（四）：现代虚拟内存的软件实现", "url": "/posts/VM4/", "categories": "读书笔记, 虚拟内存的架构和操作系统支持", "tags": "", "date": "2022-10-20 20:24:00 +0000", "snippet": "在介绍了虚拟内存的硬件细节之后，我们现在来关注操作系统层面对虚拟内存的支持。通常操作系统的支持分为两部分，首先操作系统必须和虚拟内存的硬件部分相匹配（如TLB、PTW等），其次操作系统需要从低级的硬件细节抽象出高级的软件操作。在这一节中我们重点讨论操作系统如何合理高效地管理虚拟内存。我们首先回顾一下分配内存的各个抽象层次。在进程的虚拟地址空间分配虚拟内存块是操作系统负责的问题，不过在单个虚拟...", "content": "在介绍了虚拟内存的硬件细节之后，我们现在来关注操作系统层面对虚拟内存的支持。通常操作系统的支持分为两部分，首先操作系统必须和虚拟内存的硬件部分相匹配（如TLB、PTW等），其次操作系统需要从低级的硬件细节抽象出高级的软件操作。在这一节中我们重点讨论操作系统如何合理高效地管理虚拟内存。我们首先回顾一下分配内存的各个抽象层次。在进程的虚拟地址空间分配虚拟内存块是操作系统负责的问题，不过在单个虚拟内存块内分配对象与处理碎片则是用户代码以及运行时系统负责的内容，虽然内存分配器对性能也非常重要，但是我们在此暂时不展开讨论。我们更需要关注的是页帧的分配，由于物理内存空间通常很有限且很容易碎片化，如何高效管理页帧是操作系统的一大重要任务。虚拟内存管理由操作系统维护的最重要的数据结构之一是跟踪每个进程相关虚拟内存的结构，例如Linux使用一个由VMA区域组成的VM区域树，指向VMA树的指针由进程的mm_struct数据结构维护，每个地址空间有一个内存描述符。VMA树记录了进程所使用的所有VMA区域，每个VMA区域是一个连续的虚拟地址范围且互相不重叠，VMA区域的大小是系统页面大小的整数倍。Linux在VMA树中跟踪两种类型的虚拟内存映射：使用mmap系统调用分配的、有文件支持的映射，用来表示代码页面、库、数据文件和设备；匿名映射，表示没有文件支持的区域，如栈和堆。VMA区域维护一个指向每个区域的虚拟地址开始和结束的指针，以及表明该页是否可读、可写或可执行的页面保护位。下图展示了一个mmaped VMA区域的例子。内存描述符指向一个区域，该区域包含了多个mmaped VMA内存区域在程序的执行过程中，VMA区域会不断增加或删除，如下图所示。调用mmap时，VMA区域会被扩大，内核试图将这些新的页面与现有的相邻VMA合并。VMA树是一个频繁访问的数据结构，每一次page fault、每一个映射操作等都需要对VMA树进行查找，由于一个进程可能有多个VMA，VMA树数据结构必须能够快速查找。在Linux中，和其他许多数据结构一样，VMA树是用红黑树来实现的，可以实现O(log(n))的搜索时间。VMA的增加或删除按需分页和懒分配有两种方法来管理新的虚拟内存分配。第一种方法是所有新添加的虚拟页面可以立即分配新的物理页帧，且页表立即改变，但这种方法的问题是浪费内存，因为我们还不知道进程是否会真的访问新分配的所有虚拟页面。第二种更主流的方法是懒分配，是一种按需分页的方式，在这种方法中，只有当程序第一次试图访问新的虚拟页面时才会进行物理页帧的分配。动态内存管理与page fault情况下的处理上图显示了Linux如何处理新虚拟页面分配的page fault。程序首先使用malloc来分配内存，malloc会调用brk/sbrk系统调用来增加堆的大小；然后，brk扩大了堆的VMA；接下来，程序试图第一次访问新的页面，但是会触发一次page fault，在这个例子中，堆的增长不涉及磁盘访问，是一个次要page fault；最后，操作系统为此分配一个物理页面，并在页表中创建PTE。写时复制当一个页面被复制时，新的页面可以不立即复制新的物理页帧，只有在该新页面需要被写入时才进行分配。首先，原始页帧仍然存在，两个虚拟页面都指向该页帧，并被标记为只读。当写入发生时，页帧被复制，虚拟页面被重新分配到自己的页面副本上，并恢复原始的权限。写时复制是通过控制页面权限位来实现的，操作系统会把PTE标记为不可写，但是在操作系统内部的数据结构中，该页相关的虚拟内存区域标记为可写，因此处理器对该区域执行store指令时，TLB或页表会触发一个page fault，然后由操作系统的page fault处理程序发现该页被标记为写时复制，然后由操作系统进行后续的处理。地址空间布局随机化（ASLR）ASLR通过使攻击者更难预测进程的目标地址来阻止攻击，随机安排关键数据区域的地址空间位置，包括可执行文件的基地址、堆、栈、库的位置。由于ASLR依赖于攻击者猜测到运行中的区域位置的低概率，通过使搜索空间更大来提高安全性，因此，当虚拟地址空间熵更高时，ASLR更有效，一般来说，熵可以通过增加发生随机化的VMA区域的数量来增加。局部性管理当物理内存容量有限时，一些页面必须被交换到磁盘，为其他页面腾出空间，因而一个关键的问题是如何识别一个页面是否有用，为此，我们引入工作集的概念。工作集一个程序的工作集是计算机科学中最重要的概念之一，决定了一个进程在给定时间间隔内所需要的内存量。一个进程在时间t的工作集W(t,a)是该进程在时间间隔(t-a, t)内所引用的数据集合。根据程序的局部性原理，进程在时间(t, t+a)内访问的页面集可以通过在(t-a,t)期间访问的页面来近似。看似简单的工作集概念对系统设计却有着很重要的影响，如果一个进程有太多的页被保存在容量有限的主存储器中，那么其他的进程就没有足够的空间，但如果一个进程在内存中的页面数太少，page fault的频率就会增加，进程就会变得不活跃或被阻塞，以等待磁盘数据传输。用来识别程序工作集的关键机制是区分被程序引用过的内存页和没有被引用过的内存页。被引用的页面被保留在主内存中，而那些很久没有被引用的页面则是被驱逐出内存的候选者。为了实现虚拟内存，我们需要实现以下三个目标的机制：（1）检测对内存或磁盘上的页面的引用，以确定程序的工作集；（2）选择内存中的哪些页面要被驱逐，以便为从磁盘上载入的页面腾出空间；（3）选择何时将页面从磁盘载入内存。我们现在介绍解决这三个问题的方法。最简单的页面置换策略我们首先讨论最优方案，尽管在实践中无法实现，但可以是实际算法的一个参考。过去的工作表明，最优算法，也称为Belady算法，会驱逐最长时间内不会被使用的内存页，也就是说，最优算法依赖于对未来访问模式的准确预测，并驱逐在未来最不会被用到的页面。下图展示了这个算法的例子。最优的页面置换算法相比之下，我们也可以实现随机替换，优势在于非常简单，但缺点是可能非常影响性能，在实践中很少使用。FIFO算法实现简单且也可以提供比较良好的页面替换候选者，需要驱逐页面时，最老的页面成为最主要的替换对象。这一算法的动机在于最早被分配的页面也可能在未来不再被使用，但实际情况是，FIFO的性能通常低于更复杂的方案。下图展示了FIFO算法的例子。FIFO页面置换算法LRU页面置换策略最常用的替换算法是LRU置换策略，实现方法通常是维护每个页面的计数器或时间戳，在内存访问时更新时间戳，然后页面替换时扫描所有的时间戳以确定最旧的页面，不过这样占用的内存空间较大，实际上可能有其他方法来进行优化。下图展示了理想的LRU算法的例子，但大多数操作系统实际上使用了近似的LRU算法以提升性能。LRU页面置换算法最广泛采用的近似LRU算法是时钟算法，具体的实现细节可以参考操作系统的教材。下图展示了时钟算法的一个例子。时钟页面置换算法页面缓冲驱逐页面并不是唯一需要进行优化的操作，由于IO可能延迟很高，操作系统通常会实施高效的分页方案，避免在关键路径上执行IO操作。比如如果一个dirty页面很快就要被驱逐，那么可以在实际驱逐之前就写回到磁盘。通常我们可以维护一个空闲的物理页帧池，当池中的页面数量减少到预设的阈值时，操作系统可以根据替换算法预先写回和/或驱逐页面，并加入到空闲物理页帧池中，页面可以再磁盘访问量小的时候进行写回，并把该页在页表中标记为clean。页面有时也不一定要马上写回到磁盘，而是可以分配页面缓冲区来缓冲对IO的访问，且内存中的缓冲区本身也可以被快速访问。物理内存分配设计内存分配器的主要挑战是如何有效地管理碎片化。外部碎片是一种对分配系统可见的内存碎片形式，指的是内存中的空闲区域，由于任何一个区域都小于正在分配的新内存区域的大小，所以不能重复使用；内部碎片只对进程可见，指的是单个分配单元中浪费的空间数量。例如，如果一个想要的分配单元不能被容纳在现有页面的空闲空间内，那么这个不可用的空间就是浪费的。动态内存分配器的目标就是减少这两种类型的碎片。最简单的内存分配器最简单的内存分配器包括best fit、first fit、worst fit，如下图所示。最简单的内存分配器，第一个例子伙伴分配Linux等现代操作系统采用的是伙伴分配器，动机在于：（1）内存分配请求可以按大小来预先分类到不同的类别，解决通用分配器的查找问题；（2）数据结构不仅要快速分配，还要快速释放内存；（3）尽可能保持空闲物理页面的连续性，减少外部碎片。下面两张图展示了伙伴分配器是如何设计和访问的，具体的设计细节同样可以参考操作系统教材。伙伴分配器伙伴分配器内存池和Slab分配伙伴分配器的主要问题是无法消除内部碎片，因此用户程序可能通常会在较大的虚拟内存块中建立内存池，然后为用户的内存分配对象提供单独的用户级分配器。大多数操作系统还维护了专门用于内核内存分配的特殊slab分配器，slab分配器是由几个软件的slab缓存组成的，这些slab位不同类型的内核数据结构预留内存空间，每个slab的大小是一个物理页，通过bitmap来记录使用情况。页面着色内存分配器根据每个页面在缓存中的位置（即位于哪个set）来进行页面着色。页面着色通常被认为是一种改善非完全组相联的缓存性能的机制，假设在最坏情况下，一个进程的工作集中每一个页帧最终都被映射到同一个set中，在这种情况下，该set将被反复刷新，导致低命中率，而其他set则没有被充分利用。为了避免这种情况，我们可以进行页面着色，使工作集中的页面均匀分布在不同set上。在实践中，页面着色对性能的好处还没有被普遍证明超过带来的额外成本，目前还没有被普遍推广，研究人员仍然在继续探索改进页面着色的新方法。反向映射反向映射解决的问题是，虚拟内存页面替换算法可能选择换出某些映射到一个或多个进程虚拟地址的物理页面，也就是说多个PTE指向同一个物理页，但页面替换算法选择换出该页面的时候，并不知道这些信息，反向映射可以帮助识别这样的映射。反向映射数据结构的设计空间和页表非常相似，但性能没有那么关键，通常以优先搜索树的形式来实现，使查询过程尽可能简单。" }, { "title": "《虚拟内存的架构和操作系统支持》笔记（三）：现代虚拟内存的硬件实现", "url": "/posts/VM3/", "categories": "读书笔记, 虚拟内存的架构和操作系统支持", "tags": "", "date": "2022-10-19 17:45:00 +0000", "snippet": "虚拟内存系统通常处于每个指令和数据访问的关键路径上，我们需要使用硬件来尽可能加速这一过程。本节我们深入讨论现代虚拟内存系统的硬件设计空间的细节，包括架构（如ISA规定的页表项格式）和微架构（如TLB的物理布局）。反转页表节省页表空间的一种方法是采用反转页表，为系统中的每个物理页面维护一个页表项，该页表项表示哪个进程使用这个物理页面，以及该进程的哪个虚拟页面映射到该物理页面，也就是说我们为系统...", "content": "虚拟内存系统通常处于每个指令和数据访问的关键路径上，我们需要使用硬件来尽可能加速这一过程。本节我们深入讨论现代虚拟内存系统的硬件设计空间的细节，包括架构（如ISA规定的页表项格式）和微架构（如TLB的物理布局）。反转页表节省页表空间的一种方法是采用反转页表，为系统中的每个物理页面维护一个页表项，该页表项表示哪个进程使用这个物理页面，以及该进程的哪个虚拟页面映射到该物理页面，也就是说我们为系统中的所有进程维护一个单一的反转页表。具体的实现和访问过程如下图所示。反转页表的访问找到正确的页表项需要搜索反转页表的数据结构，最简单的是采用线性扫描，但显然速度很慢。我们通常采用哈希表以加快查找速度，首先计算哈希值，然后从哈希值开始搜索表，大大减少了需要搜索的页表项的数量。哈希碰撞的情况和软件上的哈希表处理方式类似，如果哈希值对应的位置上不匹配，那么就沿着碰撞链去搜索。碰撞链可以用不同的方式实现，例如可以放在表内，或者更常用的方法是采用HAT（Hash Anchor Table），结构和访问过程如下图所示，其关键创新点在于增加HAT的大小可以减少平均的碰撞链长度，无需改变反转页表的大小，更为灵活方便。使用HAT的哈希页表TLB的设计多级TLB现代处理器普遍实现了多级TLB，一个是实现上的问题是，最后一级TLB应该是私有的还是共享的。私有TLB可以减少竞争、降低延迟，而共享TLB可以更有效地利用存储资源。同时，TLB可以设计为统一的或分离的，即是否要分离指令和数据。相对于缓存的位置TLB放置策略从概念上来说，现代TLB有四种不同的放置策略，如上图所示。PIPT的好处是简单的缓存管理，在多程序的情况下可以无缝运行，很容易处理缓存一致性请求，但性能较差，因此通常只将L2缓存和LLC作为PIPT实现，在简单的嵌入式系统之外，L1缓存通常不会被实现为PIPT。VIVT则可以不经过TLB而直接访问缓存，提高了性能，但也引入了一系列的问题，需要复杂的的硬件结构来解决。首先，TLB在缓存命中时不被查询，也就是说VIVT缓存不容易检测到内存访问违规并触发异常，这样的非法访问要么永远不会被捕获，要么只有在该行被evict的时候才会被捕获，一个解决方案是即使VIVT缓存命中时也要访问TLB，这需要在指令提交前完成，但不一定要在执行的时候完成。其次，相同的虚拟地址可能指代不同的物理地址，一个解决方法是在每个缓存行中增加PID或ASI字段，但需要增加额外的存储空间，且需要存储权限信息，并与tag和PID匹配并行查询。最后，即使在同一个进程中也存在synonym的问题。由于上述问题，VIVT缓存在通用处理器中并不十分常见。VIPT缓存结合了上述两者的优点，关键点在于缓存查找实际上由两部分组成，首先进行索引，然后匹配tag。VPN用来查询TLB，VPO用来对缓存索引，注意在这种方式中缓存索引必须由不需要地址翻译的位来进行，其实VPO也就是PPO。这种方式的好处在于完全隐藏了TLB访问的延迟，且由于使用物理tag，避免了多程序和synonym的问题，但缺点是只能支持有限set数量的缓存，如果违反了这一规则，就会再次出现synonym的问题。一个解决方案是操作系统实现页面着色，内存分配器根据每个页面在缓存中的位置为其着色。PIVT缓存没有存在的必要。多种页面大小许多现代操作系统和体系结构支持多种页面大小。然而，与VIPT缓存对应的组相联TLB不能很容易地支持多种页面大小，因为查找时我们需要VPN的低位来选择TLB set，但这些位只有在页面大小确定时才能确定下来，但页面大小在TLB查找成功前都是未知的，也就是说TLB查找时需要知道页面大小，但页面大小只有查找后才能确定。访问不同大小的页面时查找多个TLB大多数处理器采用的方法是分割TLB，如上图所示，每个页面大小一个TLB，虚拟地址可以并行查找所有TLB，根据TLB支持的页面大小，每个TLB使用不同的索引位，如4 KB、2 MB和1 GB页面，有16个set的TLB集合索引分别是15-12、24-21和33-30位。如果其中一个TLB命中，我们也就知道了页面大小，如果都没有命中，那么就访问下一级TLB。然而这种方式比较消耗能量，且分散的TLB也难以被充分利用。页表项本节我们重点讨论页表项中的metadata。访问权限页表项和TLB存储了对虚拟地址空间中每个区域的访问信息。一些架构明确提供特定的读取权限位，而另一些可能假设所有有效映射的区域都是可读的；一些架构可能提供W^X保护，而另一些把决定权留给操作系统。一些架构可能提供用户态或内核态控制状态，分别给予用户进程或操作系统访问页面的权限。Accessed和Dirty位这两个位对于良好的页面替换决策至关重要。Accessed位标记最近访问过的页面，现代CPU上，PTW负责设置accessed位，访问某一页面并将其映射带入TLB前需要将accessed位置1，注意TLB本身通常不会维护accessed位。唯一重置的方式是由操作系统置0，通常用accessed位来近似LRU，定期重置accessed位。Dirty位用来识别需要写回磁盘的数据，通常也需要在TLB中维护。注意即使TLB命中，也要通过PTW查询页表并设置dirty位。注意accessed位主要是用于性能方面，但dirty位是为了确保正常功能的实现。ASID与Global位如果TLB不跟踪上下文信息，操作系统切换上下文时需要完全刷新TLB的内容，在ARM和x86架构上，大范围的TLB刷新会导致高达10%的性能下降。TLB通过ASID和global位的硬件支持来减少TLB的刷新频率，上下文切换时没有必要刷新TLB，注意TLB的ASID不一定与操作系统的PID相同，前者往往位数更少，且需要操作系统跟踪ASID和软件上下文信息之间的映射。除了ASID外，global位用来识别对所有进程都是全局的转换，如内核地址空间。PTW英特尔Skylake L2 TLB有1536个条目和4KB页，可访问范围只有6MB，比今天典型应用的工作集小得多，因此我们也需要注意TLB缺失情况下的处理机制。软件管理的TLB在虚拟内存实现的早期，软件管理的TLB是很常见的，TLB缺失会触发异常，操作系统运行中断处理程序来访问页表，然后通过专用指令填充TLB。概念上来说这样实现很简单，但软件管理的TLB性能很差，每次TLB缺失都需要操作系统进行上下文切换。硬件管理的TLB这种方式的好处是减小了需要陷入内核态来处理页表的成本，CPU硬件维护PTW状态机来处理TLB缺失，不再需要冲刷流水线、保存上下文来进入中断处理程序。绝大多数PTW直接使用物理内存地址而不是虚拟地址来访问页表，避免了需要为页表进行虚拟到物理地址转换的尴尬问题。PTW需要知道页表的基址，因此ISA通常提供一个控制寄存器，由操作系统作为上下文切换的一部分来进行配置，如x86的CR3、ARM的TTBR0和TTBR1。缺点在于PTW状态机是硬件实现的，减少了改变页表组织的潜在灵活性。MMU缓存TLB不是唯一用于缓存地址转换信息的结构，为了减小TLB缺失的延迟，可以缓存多级页表前几级的页表项，一般称为MMU缓存。TLB缓存的是页表最后一级的PTE，而MMU缓存，相比之下存储的是L4、L3和L2 PTE。TLB缺失时访问MMU缓存，PTW状态机首先检查是否存在于该级的MMU缓存，在最好的情况下，如果每一级都可以命中MMU缓存，那么PTW访问的延迟是很小的。设计MMU缓存的动机是，为前几级PTE提供缓存是有意义的，可以映射地址空间的较大部分，如x86-64标准四级分页方案中，页表的每一级分别覆盖512 GB、1 GB、2 MB和4 KB。基本的MMU缓存有很多变种，英特尔使用分页结构缓存（PSC），如下图所示，所有的PSC条目都可以并行查找，如果命中的话，最长的匹配用作PTW访问页表其余部分的起点，减少了内存访问次数并降低了延迟。x86-64虚拟地址0x5c8315cc2016访问页表，TLB缓存L1 PTE，MMU缓存存储L2-L4 PTEAMD则设计了页行缓存（PWC），和PSC不同的是，PWC只为每个页表级别提供专用的PIPT缓存，因此每一级都需要按顺序查找，但可能会有更低的延迟。目前MMU缓存仍然是一个活跃的研究领域。" }, { "title": "《虚拟内存的架构和操作系统支持》笔记（二）：基本实现", "url": "/posts/VM2/", "categories": "读书笔记, 虚拟内存的架构和操作系统支持", "tags": "", "date": "2022-10-19 02:38:00 +0000", "snippet": "本节我们讨论虚拟内存在操作系统和硬件中的实现，重点解释基本原理和机制，然后在之后的章节中深入讨论细节和设计空间。典型的基于分页的虚拟内存系统大多数虚拟内存系统采用分页策略，虚拟地址空间呗划分为页和页帧（page frame，或简称为帧），页是虚拟内存管理的最小单位，而页帧是物理内存管理的最小单位。现代操作系统普遍采用按需分页，也就是说只有用户需要时才会分配页面。现在的CPU普遍采用4 KB的...", "content": "本节我们讨论虚拟内存在操作系统和硬件中的实现，重点解释基本原理和机制，然后在之后的章节中深入讨论细节和设计空间。典型的基于分页的虚拟内存系统大多数虚拟内存系统采用分页策略，虚拟地址空间呗划分为页和页帧（page frame，或简称为帧），页是虚拟内存管理的最小单位，而页帧是物理内存管理的最小单位。现代操作系统普遍采用按需分页，也就是说只有用户需要时才会分配页面。现在的CPU普遍采用4 KB的页面，但也开始利用超级页来缓解虚拟内存系统中的压力，直到今天如何有效利用超级页仍然是一个活跃的研究领域。页面大小也是一个关键的实现细节，Linux的mprotect或Windows的VirtualProtect等syscall需要以页面大小为粒度来进行内存管理。下图展示了页面大小如何影响地址转换的过程。虚拟地址转换到物理地址的过程页表在一个基于分页的实现中，每个进程的虚拟到物理的映射集合被存储在一个被称为页表的数据结构中，有许多方法可以实例化页表结构，我们在下面介绍其中的一些方法。x86-64的页表项页表的基本单元是页表项，存储了一个特定页的虚拟到物理地址转换信息的所有相关信息，如上图例子所示。页表必须能覆盖整个虚拟地址空间，但为了减少页表占用的空间，现在最流行的设计是分层或多级页表，如下图所示。多级页表注意中间的页表项也是有权限和状态位的，可能在中途就触发page fault。页表的层数在不同的ISA中可能也是不一样的，并根据地址空间的大小来决定，例如32位x75采用两级或三级页表，将32位虚拟地址转换为40或52位物理地址，而64位x86和ARM架构采用四级页表，将48位虚拟地址转换为52位物理地址。多级页表也能很好地实现混合页面大小。TLB如果没有TLB，虚拟内存实现的成本会很高，内存带宽的要求将提升2-5倍。不过为了保持速度，TLB本身不能使很大的结构，许多处理器也会使用类似缓存的层次结构来实现TLB。Page and Segmentation FaultsPage fault一般分为两类，次要和主要的。次要的page fault表示页帧存在于物理内存中，但是在页表中没有对应的页表项，或者没有设置合适的权限。这种可能发生在如下一些情况：页面已经分配但还没有被访问、页帧被赋予了多次映射（经常发生在共享库或进程间共享内存中）、写时复制等。主要的page fault表示所需的数据不在内存中，需要从磁盘上获取，访问该数据会产生主要的page fault，并将数据带入内存。当page fault发生时，CPU通常通过一些控制状态寄存器来协助操作系统定位问题，且这些细节是和ISA的设计紧密联系的。Segmentation fault表示访问请求的虚拟地址是非法的，如用户可能访问一个未分配过的虚拟地址，或者打破了权限，这些通常是由于程序员的错误造成的，唯一安全的做法是终止进程。" }, { "title": "《虚拟内存的架构和操作系统支持》笔记（一）：基础", "url": "/posts/VM1/", "categories": "读书笔记, 虚拟内存的架构和操作系统支持", "tags": "", "date": "2022-10-17 20:08:00 +0000", "snippet": "《虚拟内存的架构和操作系统支持》一书（英文名为Architectural and Operating System Support for Virtual Memory），隶属于Synthesis Lectures on Computer Architecture系列。本节简单回顾了虚拟内存抽象的基础知识。典型的虚拟地址空间32位和64位进程的地址空间上图简单描述了一个32位和64位进程的地...", "content": "《虚拟内存的架构和操作系统支持》一书（英文名为Architectural and Operating System Support for Virtual Memory），隶属于Synthesis Lectures on Computer Architecture系列。本节简单回顾了虚拟内存抽象的基础知识。典型的虚拟地址空间32位和64位进程的地址空间上图简单描述了一个32位和64位进程的地址空间，下图详细展示了一个名为/opt/test的32位Linux进程的地址空间，这是通过打印虚拟文件/proc/&lt;pid&gt;/maps的内容收集的。一个Linux进程的地址空间内容用户和内核内存区域的划分是由操作系统决定的，在32位系统中由于内存容量有限，这一点很重要，例如Linux通常将3GB以下的内存作为用户空间，剩下1GB留给内核，而Windows采用2GB+2GB的分割方式。相比之下，64位系统这一点就不再那么关键，且大多数空间没有使用，例如x86-64架构目前要求虚拟地址的48-63位保持一致。上图中我们注意到还有一段空间叫vDSO，即virtually dynamically linked shared object，是一个特殊的性能优化，可以加速用户和内核代码之间的一些交互，比如内核管理了各种计时器数据结构，原来我们需要通过syscall来进行访问，但通过vDSO可以直接访问这些结构。这也说明现代虚拟内存系统中，各种复杂的机制都是围绕内存保护来定义、执行和优化的。内存权限各种类型的内存区域访问权限多线程程序一个进程中的所有线程共享同一个地址空间，但每个线程都有自己的stack共享内存、Synonym和Homonym虚拟内存并不总是在虚拟和物理内存之间执行一对一的映射。一个被多个进程重复使用的单一虚拟地址可以指向多个物理地址，这叫做homonym。反之，如果多个虚拟地址指向同一个物理地址，则被称为synonym。共享内存更进一步，因为它允许多个进程设置不同的虚拟地址，这些虚拟地址指向同一个物理地址。Homonym虚拟地址0xa4784000是homonymTLB需要处理homonym的问题，第一种方法是在两个地址可能被先后使用（例如内核执行上下文切换）时刷新TLB，第二种方法是在虚拟地址上附加进程或地址空间的ID。不过TLB并不是唯一需要考虑homonym的结构，在VIVT缓存中也存在问题，对于数据不是只读的缓存等结构，使用进程ID的解决方案可能是不完整的。Load buffer和store buffer等处理器的内部结构也会受到影响，不能仅仅根据虚拟地址来进行数据转发。Synonym虚拟地址0xb974c000和0x39297000是synonymSynonym也会带来很多问题。例如假设有两个虚拟页，其中第一个标记为dirty，但第二个页在访问时可能会认为是clean的，并在没有写回dirty数据的情况下直接覆盖了这一页。VIPT缓存会因为synonym导致一致性协议无法满足。和之前一样，load buffer和store buffer也会受到影响，例如尽管load的地址无法在store buffer中找到，但其实可能映射到了同一个物理页，从而错过了数据的转发，一个简单的解决方案是基于物理地址来进行转发，但会让TLB位于性能的关键路径上，更常见的方法是进行推测性的load，然后再进行物理地址检查。线程本地存储一些线程实现还提供了线程本地存储（TLS）的机制，TLS提供了进程的虚拟地址空间的一些子集，这些子集只能由该线程进行访问。在编程时，用户通过一些API来初始化一些数据结构来表示线程的本地存储数据。在运行时，TLS实现会给每个线程分配寄存器（如ARM的CP15 c13、x86的FS或GS）或基址指针。TLS改变了虚拟地址在每个线程上被转换为物理地址的方式，在这个例子中，两个线程都使用虚拟地址0x90ed4c000，但每个线程在地址翻译之前都会加上自己的offset" }, { "title": "CMU 18-643可重构计算笔记-4：类C语言硬件综合的挑战", "url": "/posts/RL4/", "categories": "Course, CMU 18-643可重构计算", "tags": "", "date": "2022-09-19 03:13:00 +0000", "snippet": "原文：The Challenges of Synthesizing Hardware from C-Like Languages.我们之所以选择将C语言作为硬件综合的目标，主要原因是大家对C语言已经比较熟悉了。人们也认为通过这种方式，我们很容易实现硬件-软件的协同设计。然而本文认为纯C语言并不是一个面向硬件编程的合适的语言，硬件的一大优势是并行性，但是C不提供原生的并行编程支持，因此综合器要...", "content": "原文：The Challenges of Synthesizing Hardware from C-Like Languages.我们之所以选择将C语言作为硬件综合的目标，主要原因是大家对C语言已经比较熟悉了。人们也认为通过这种方式，我们很容易实现硬件-软件的协同设计。然而本文认为纯C语言并不是一个面向硬件编程的合适的语言，硬件的一大优势是并行性，但是C不提供原生的并行编程支持，因此综合器要么自动挖掘并行性，要么允许使用语言扩展来显式地定义并行性。本文的主要观点是，高效的硬件设计通常很难用传统的C语言来描述，就算真的可以用C语言来编程，其语义上也和软件上的C几乎没有相似之处。本文主要讨论使用类C语言进行硬件综合，不讨论验证和算法设计等内容。对后者而言，类C语言（特别是SystemC和其他变体）已经在广泛使用。类C硬件综合语言用于硬件综合的类C语言上表列出了1980年代后期以来提出的一些类C硬件综合语言，如下面的代码是Cones语言的一个例子，函数中的内容会被翻译为组合电路。INPUTS: IN[5];OUTPUT: OUT[3];rd53(){ int count, i; count = 0; for (i=0 ; i&lt;5 ; i++) if (IN[i] == 1) count = count + 1; for (i=0 ; i&lt;3 ; i++) { OUT[i] = count &amp; 0x01; count = count &gt;&gt; 1; }}下面的代码是HardwareC中的最大公约数算法，HardwareC是一种行为硬件语言。#define SIZE 8process gcd (xi, yi, rst, ou) in port xi[SIZE], yi[SIZE]; in port rst; out port ou[SIZE];{ boolean x[SIZE], y[SIZE]; write ou = 0; if ( rst ) &lt; x = read(xi); y = read(yi); &gt; if ((x != 0) &amp; (y != 0)) repeat { while (x &gt;= y) x = x – y; &lt; x = y; /* swap x and y */ y = x; &gt; } until (y == 0); else x = 0; write ou = x;}SystemC是一种支持硬件和系统建模的C++变体，通过C++的类来对层次结构进行建模，并通过组合和时序过程来描述硬件，下面的代码是SystemC描述的硬件，功能是两位十进制数字到七段数码管的解码器，解码器产生组合逻辑，计数器产生时序逻辑。#include “systemc.h”#include &lt;stdio.h&gt;struct decoder : sc_module { sc_in&lt;sc_uint&lt;4&gt; &gt; number; sc_out&lt;sc_bv&lt;7&gt; &gt; segments; void compute() { static sc_bv&lt;7&gt; codes[10] = { 0x7e, 0x30, 0x6d, 0x79, 0x33, 0x5b, 0x5f, 0x70, 0x7f, 0x7b }; if (number.read() &lt; 10) segments = codes[number.read()]; } SC_CTOR(decoder) { SC_METHOD(compute); sensitive &lt;&lt; number; }};struct counter : sc_module { sc_out&lt;sc_uint&lt;4&gt; &gt; tens; sc_out&lt;sc_uint&lt;4&gt; &gt; ones; sc_in_clk clk; void tick() { int one = 0, ten = 0; for (;;) { if (++one == 10) { one = 0; if (++ten == 10) ten = 0; } ones = one; tens = ten; wait(); } } SC_CTOR(counter) { SC_CTHREAD(tick, clk.pos()); }};SpecC语言则是ANSI C的超集，增加了许多系统和硬件建模结构，包括FSM、并发、流水线等，下面的代码展示了SpecC的可综合RTL语言描述的状态机，其中wait(clk)表示时钟周期边界。behavior even( in event clk, in unsigned bit[1] rst, in bit[31:0] Inport, out bit[31:0] Outport, in bit[1] Start, out bit[1] Done, out bit[31:0] idata, in bit[31:0] iocount, out bit[1] istart, in bit[1] idone, in bit[1] ack_istart, out bit[1] ack_idone){ void main(void) { bit[31:0] ocount; bit[31:0] mask; enum state { S0, S1, S2, S3 } state; state = S0; while (1) { wait(clk); if (rst == 1b) state = S0; switch (state) { case S0: Done = 0b; istart = 0b; ack_idone = 0b; if (Start == 1b) state = S1; else state = S0; break; case S1: mask = 0x0001; idata = Inport; istart = 1b; if (ack_istart == 1b) state = S2; else state = S1; break; case S2: istart = 0b; ocount = iocount; if (idone == 1b) state = S3; else state = S2; break; case S3: Outport = ocount &amp; mask; ack_idone = 1b; Done = 1b; if (idone == 0) state = S0; else state = S3; break; } } }};并发硬件和软件最大的区别在于执行模型，软件遵循冯诺依曼结构的基于内存的顺序执行模型，而硬件则从本质上来说就是并发的。并发编程仍然很困难，一方面是因为难度较大，另外一方面则是关于并行编程模型的分歧，如选择共享内存还是消息传递。在顺序代码中挖掘并行性有三种主要方法，分别为指令级并行（已经在乱序超标量处理器中广泛应用）、流水线（会涉及到数据与控制依赖的问题）、进程级并行（取决于算法，且很难自动识别，需要程序员手动控制）。有两种方法在C中实现并发。第一种方法在语言中添加并行结构，让程序员手动控制，另一种方法是让编译器自动识别并行性。这两种方法有各自的缺点，后者为编译器开发增加了难度，前者则要求程序员以一种全新的思维方式去编程，一个好的硬件规范语言必须能够有效表达并行算法。时序C语言中很难实现时序约束，也没有规定每条指令需要的时间，但为了实现硬件设计的性能目标，我们需要合适的指定并实现时序约束的机制。下面的代码就很难说明这个循环需要多少周期，不同的语言对此的解释并不一致。for (i = 0 ; i &lt; 8 ; i++) { a[i] = c[i]; b[i] = d[i] || f[i];}编译器使用各种技术插入时钟周期边界，手动或自动。一个好的硬件综合语言需要可以明确或通过约束来指定时序，但也不应该要求程序员提供太多细节。数据类型数据类型是硬件和软件语言之间的另一个主要区别。C几乎不支持小于一个字节的类型，因此纯C的代码很容易被翻译成不必要的硬件。编译器采用三种方法将硬件类型引入C程序。第一种是允许在语言之外调整数据的宽度，第二种是在C中添加硬件类型，基于C++语言的第三种方法是通过C++的类型系统提供类似硬件的类型。一个好的硬件语言需要一个允许精确定义硬件类型的类型系统，C++相比C显得更合适。通信类C语言建立在非常灵活的内存通信模型之上，隐含地将所有内存位置视为访问成本相同，但在现代内存层次结构中并非如此。设计人员通常可以预测这些存储器的行为并更有效地使用，但非常困难，且类C语言对内存访问提供的辅助支持很少。为了避免过长且无法预测的通信延迟，硬件设计人员根据系统的需要使用各种机制，从简单的线路到复杂的通信协议。指针的问题由于指针的不确定性，软件中的通信模式通常很难事先确定，指针别名也是一个很严重的问题。尽管可以使用指针分析来估计C程序的通信模式，但是精确的分析是不可能的，只能采取保守的方法，也会导致额外不必要的开销。内存与通信我们可以把类C语言大致分为两组。第一组忽略C的内存模型，不支持数组和指针，只关注局部变量；第二组则尽可能实现C的所有数据类型，支持struct、数组等多种存储方式。硬件和软件在这方面的设计差异也是巨大的。软件的通信方式是面向事务的，但硬件则是通过物理上的连线的。软件设计者通常会忽略内存访问的模式，但硬件设计者一开始就要详细描述每个通信的通道，并最小化通信的开销。元数据硬件语言面临的巨大挑战由于硬件的层次远低于软件，有更多方法可以在硬件中实现特定的C结构。例如加法运算，CPU可能只有一条有用的加法指令，而在硬件中却有大量不同的加法器结构。因此，硬件的翻译过程比软件的翻译需要做出更多的决定，此外，正确的决策因设计约束而异，期望所有这些决策都是自动化的仍然是不现实的。尽管我们似乎可以使用C++的运算符重载机制来手动指定各种约束，但这种机制的实现可能仍然非常困难。C++的重载机制是使用参数类型来定义的，但硬件中算法的选择通常由资源限制决定。我们可以采用两种方法来指定各种元数据：放置在程序中（注释、pragma、添加的结构等）或者程序之外（文本文件、GUI等）。一个好的硬件规范语言需要一种方法来指导综合过程在不同的实现中进行选择，在功率和速度之间进行权衡。" }, { "title": "CMU 18-643可重构计算笔记-3：FPGA架构的原理与演进", "url": "/posts/RL3/", "categories": "Course, CMU 18-643可重构计算", "tags": "", "date": "2022-09-02 03:09:00 +0000", "snippet": "原文：FPGA Architecture: Principles and Progression.FPGA已被广泛用于实现来自不同领域的无数应用。由于其低级硬件可重构性，与定制设计的芯片相比，FPGA具有更快的设计周期和更低的开发成本。FPGA架构的设计涉及许多不同的设计选择，从高级架构参数到晶体管级实现细节，目标是制造高度可编程的器件，同时最大限度地减少可重构性的面积和性能成本。本文介绍了...", "content": "原文：FPGA Architecture: Principles and Progression.FPGA已被广泛用于实现来自不同领域的无数应用。由于其低级硬件可重构性，与定制设计的芯片相比，FPGA具有更快的设计周期和更低的开发成本。FPGA架构的设计涉及许多不同的设计选择，从高级架构参数到晶体管级实现细节，目标是制造高度可编程的器件，同时最大限度地减少可重构性的面积和性能成本。本文介绍了现代商业FPGA架构关键组成部分的演进，以及设计决策背后的原理与挑战。包含了可编程逻辑与IO的早期FPGA架构与包含RAM、DSP和其他硬件的现代异构FPGA架构本文首先概述了评估FPGA架构理念的CAD流程和方法，然后介绍了FPGA每个关键组件的架构挑战和设计原则，重点包括过去三年中这些组件的设计和实现中的关键创新与前沿研究领域。FPGA架构评估FPGA架构评估流程FPGA架构评估流程主要由一套基准测试、一个架构模型以及一个CAD系统组成。测试套件可能和ASIC不太一样，通常每个FPGA供应商都有一套自己收集的基准测试，还有一些开源的测试，如经典的MCNC20（规模很小）、VTR和Titan23等。随着 FPGA 容量和应用程序复杂性的持续增长，经常需要新的基准套件。FPGA的设计涉及许多不同的决策，从架构级别的组织（例如逻辑块的数量和类型、线长分布、逻辑集群和逻辑元件的大小）到晶体管级别的电路实现（例如可编程开关类型、路由缓冲晶体管尺寸，寄存器实现），还涉及不同的实现风格。一些模块（RAM、IO）甚至包括模拟电路。所有这些不同的组件都需要仔细建模，以全面评估FPGA架构。通常使用架构描述文件来捕获，该文件指定不同FPGA模块的组织和类型以及布线架构，以及从每个组件的电路级实现获得的面积、时序和功率模型。最后CAD系统将基准测试映射到指定的FPGA架构上，由一系列复杂的优化算法组成，这些算法将用HDL编写的基准综合到电路网表中，并进行布局布线，然后评估关键指标，包括总面积、时序、功耗等，也包括一些其他指标，包括CAD工具运行时间、是否容易进行布线等。FPGA架构经常需要考虑的问题是：应该特别强化哪些功能（即作为硬件块的形式实现）？这些块的灵活程度应该是什么样的？灵活性和效率往往无法兼得。FPGA架构师可能会尝试许多想法，然后才能找到正确的设计选择组合，在正确的位置添加适量的可编程块和硬件IP，也要评估对布局布线的影响，对成本和收益进行量化。FPGA架构演进可编程逻辑PAL架构最早的可重构计算设备是可编程阵列逻辑（PAL）架构，如上图所示，但是在之前的文章中我们也提过，PAL不能很好地进行扩展。随后，复杂可编程逻辑器件（CPLD）将and/or阵列作为基本逻辑元件，试图通过集成多个PAL 并在它们之间使用交叉互连来解决可扩展性挑战，但代价是更复杂的设计工具。Xilinx在1984年率先推出了第一个基于LUT的FPGA，由一系列基于SRAM的LUT组成。(a) 4-LUT的晶体管级实现，在第二和第三LUT级之间有内部缓冲器，(b) 两级Mux电路，(c) 基本逻辑元件（BLE），(d) 逻辑块（LB）内部架构随着器件逻辑容量的增长，LUT（K）和LB（N）的大小逐渐增加。随着K的增加，可以将更多功能打包到单个LUT中，不仅减少了所需的LUT数量，还减少了关键路径上的逻辑层数，从而提高了性能，此外，随着通过增加N将更多连接捕获到快速本地互连中，对LB间路由的需求减少。然而，LUT的面积随着K的增加呈指数增长并且其速度线性下降，同时如果将LB本地互连实现为Crossbar，则其大小呈平方增加，其速度随LB中的BLE数量N线性下降。研究表明，4-6大小的LUT和3-10大小的LB最为合适，4-LUT面积最小但6-LUT会有更好的性能。6-LUT可拆分成两个 5-LUT：(a) 没有额外的输入端口，有5个共享输入；(b) 两个额外的输入端口和转向用Mux，有2个共享输入架构的下一个重大变化是，在2003年，Altera在Stratix II架构中首次引入可拆分LUT，追求实现较大LUT的性能和较小LUT的面积效率。上图中，第二种设计增加了面积，但是更容易找到两个可以打包在一起的函数，在Stratix II中，ALM具有8个输入和2个输出，可以实现一个6-LUT或两个共享2个输入的5-LUT，也可以在没有共享输入的情况下实现一对较小的LUT，兼顾了性能和面积效率。Xilinx后来在Virtex-5架构中也采用了类似的方法。在LB大小方面，Altera和Xilinx在最近几代都使用了相对较大的LB，分别包含10个和8个BLE。(a) 正沿触发器 (b) 脉冲锁存器另一个重要的架构选择是每个BLE的FF数量，早期FPGA为每一个LUT配置一个FF，但当开始使用可拆分LUT时，Altera和Xilinx都为每个BLE添加了第二个FF，可供两个输出同时使用。除此之外，Stratix V实现了上图中的脉冲锁存器。(a) Xilinx 和 (b) Altera/Intel FPGA逻辑块中的硬件算术电路（红色）算术运算（加法和减法）在FPGA设计中非常常见，因而所有现代FPGA架构在其逻辑块中都包含硬件算术电路，如上图所示。A[i]和B[i]是两个加法操作数A和B的第i位，Xilinx LE计算进位传播并在LUT中生成，而Altera使用LUT将输入传递给硬件加法器。可编程路由可编程路由通常占结构面积和关键路径延迟的50%以上，因此其效率至关重要。FPGA路由架构主要有两类，分层和岛式。分层路由架构会导致布线较长，物理上相邻的块实际布线很长，因此主要用于较小的FPGA。另一种是岛式，由Xilinx首创，包括三个组件，即布线段、将功能块输入连接到布线的连接块（Mux）、以及开关块（可编程开关）。分层路由架构岛式路由架构（实线是物理导线，虚线是可编程开关）创建一个好的路由架构涉及许多复杂的权衡，应该包含足够的可编程开关和线段，可以实现绝大多数电路，但是过多的电线和开关会浪费空间。一些布线架构参数包括：每个逻辑块输入或输出可以连接多少条布线（FC）、每根线可以连接多少其他路由线（Fs）、布线段的长度、开关模式、电线和开关本身的电气设计、以及每个通道的布线数量。例如在上图中，FC=3，Fs= 3，通道宽度为4根线，一些布线的长度为1，其他的长度为2。早期的岛式架构仅包含在可编程开关之间穿过单个逻辑块的短线，但是会导致比必要更多的可编程开关，现代架构包括多种长度的布线段，以更好地满足短连接和长连接的需求，但最丰富的布线段保持中等长度，通常为四个逻辑块。更长距离的连接可以使用更长的线段来实现更低的延迟，但在最近的工艺节点中，跨越许多（如16个）逻辑块的线必须在上层金属层上使用宽厚的金属线才能实现可接受的电阻，这种长距离布线的数量是有限的。为了最好地利用这种稀缺的布线，Stratix FPGA允许长线段仅连接到短线段，而不是功能块输入或输出，在岛式FPGA中建立了一种布线层次结构。使用传输晶体管（左）、三态缓冲器（中）或缓冲Mux（右）的SRAM控制可编程开关的不同实现可编程开关的电气设计也有多种选择，如上图所示。早期的FPGA使用由SRAM单元控制的传输晶体管来连接导线，虽然这是传统CMOS工艺中可能的最小开关，但通过传输晶体管串联连接的布线延迟呈平方增长。三态缓冲器会占用空间，但提升速度。大多数最新的FPGA采用最后一种设计。FPGA布线的一个主要挑战是长线的延迟并没有随着工艺扩展而改善，这意味着即使时钟频率上升，跨芯片的延迟也会停滞或增加。这导致FPGA应用开发人员在其设计中增加了流水线的数量，从而允许多个时钟周期用于长路径。为了使这种策略更有效，一些FPGA制造商在路由网络内集成了寄存器。可编程IOFPGA 包含独特的可编程IO结构，允许与各种其他设备进行通信，使FPGA成为许多系统的通信枢纽。一组物理IO以可编程方式支持许多不同的IO接口和标准具有挑战性，因为需要适应不同的电压电平、电气特性、时序规范和命令协议。FPGA上用于IO的大面积区域凸显了可编程IO的价值和挑战。实现可编程IO的不同技术片上存储器FPGA架构中片上存储器元件的第一种形式是集成在FPGA逻辑块中的 FF，然而随着FPGA逻辑容量的增长，FPGA被用于实现更大的系统，这些系统几乎总是需要内存来缓冲和复用芯片上的数据，因此非常需要更密集的片上存储，且在FPGA上实现的应用程序的RAM需求非常多样化，很难决定应将哪种类型的RAM块添加到FPGA以使其运行高效用途广泛。FPGA逐渐集成了更大、更多样化的BRAM，通常现代FPGA 25%的区域用于BRAM。具体的工作原理如下图所示，可以参考原文。基于双端口SRAM的FPGA BRAM的电路结构（蓝色组件在任何SRAM内存中都很常见，而绿色是FPGA独有的，此BRAM最大数据宽度为8位，但输出crossbar配置为4位输出模式）设计FPGA BRAM的主要架构决策是选择容量、数据宽度和读/写端口的数量。功能更强大的BRAM需要更多面积，因此必须仔细平衡BRAM设计选择，同时考虑最常见的应用。SRAM单元占用的面积随着BRAM的容量线性增长，但外围电路的面积和路由端口的数量呈亚线性增长，较大的BRAM具有较低的每比特面积，从而使大型片上缓冲区更有效，但同时，如果应用程序只需要小RAM，则可能会浪费较大BRAM的大部分容量。类似，具有更大数据宽度的BRAM可以为下游逻辑提供更高的数据带宽，但比容量相同但数据宽度更小的BRAM占用更多的面积。最后，增加BRAM的读/写端口的数量会增加SRAM单元和外围电路的面积，但又会增加BRAM可以提供的数据带宽并允许更多样化的用途。FPGA片上存储器必须满足在该FPGA上实现的每个应用程序的需求，所以通常为BRAM添加额外的可配置性以使其适应应用程序的需求。不同数量和类型的BRAM读/写端口所需的路由端口数量（W为数据宽度，D为BRAM深度）与传统内存块相比，FPGA BRAM的另一个独特组件是它们与可编程路由结构的接口，如上图所示。除了构建BRAM之外，FPGA供应商还可以添加电路，使设计人员能够将构成逻辑结构的LUT重新用于RAM块。将2048×32位2r+1w逻辑RAM映射到1024×8位1r+1w物理BRAM设计人员在典型设计中需要许多不同的RAM，必须通过芯片上的固定BRAM和LUT-RAM资源来实现。迫使设计人员为他们需要的每种内存配置确定组合BRAM和LUT-RAM的最佳方式并编写RTL来实现是不现实的，而且还会将设计与特定的FPGA架构联系起来，因此，FPGA CAD工具包括一个RAM映射阶段，将用户使用的逻辑存储器映射到物理存储器，如上图所示。Altera FPGA每个LE需要的内存位数的趋势，图上也注明了每种架构中BRAM的大小在过去的25年中，FPGA存储器架构有了长足的发展，也变得越来越重要，因为FPGA芯片上的存储器与逻辑的需求的比值显着增长，如上图所示。为了深入了解不同RAM块的相关面积和效率，下表显示了Quartus在Stratix IV器件上实现的2048×72位逻辑RAM的资源使用、硅片面积和频率。在Stratix IV上使用BRAM、LUT-RAM和寄存器的2048×72位1r+1w RAM的实现结果DSP模块最初，商业FPGA架构中唯一的专用算术电路是进位链以实现高效的加法器，使用LUT和进位链在软逻辑中实现乘法器会产生很大的面积和延迟损失。由于高乘法器密度信号处理和通信应用构成了主要的FPGA市场，设计人员提出了新颖的实现方案，以减轻软逻辑中乘法器实现的低效率。Altera/Intel和Xilinx FPGA的DSP模块演进（增量添加的功能以红色突出显示）多年来，DSP模块架构不断发展，以最适合FPGA关键应用领域的要求，并提供更高的灵活性，使许多不同的应用程序都可以从其功能中受益。这一演进的所有步骤的共同焦点是尽可能重用乘法器阵列和路由端口，以最好地利用这两种昂贵的资源。具体的演进过程可以参考原文。系统级互联：片上网络FPGA在容量和其外部IO接口（例如DDR、PCIe和以太网）的带宽方面都在不断增加，在这些高速接口和越来越大的逻辑结构之间分配数据流量是一项挑战。这种系统级互联传统上是通过配置部分FPGA逻辑和路由来实现相关端口之间的多路复用、仲裁、流水线和布线的总线，这些外部接口的工作频率高于FPGA架构所能达到的频率，因此匹配其带宽的唯一方法是使用更宽的总线，会使用大量FPGA逻辑和布线资源。此外，系统级互联往往会跨越很远的距离。这两者导致时序收敛很难，并且通常需要对软总线实现深度流水线，进一步增加资源使用。我们可以在FPGA架构中嵌入一个硬件实现的片上网络来实现更高效且易于使用的系统级互联。为FPGA设计NoC具有挑战性，因为FPGA架构师必须对芯片做出许多选择（如路由器的数量、链路宽度、NoC拓扑），但仍要保持FPGA的灵活性，以使用许多不同的外部设备来实现各种应用，需要能灵活地连接到FPGA架构中实现的用户逻辑。NoC可以在固定高频率下运行，与具有不同速度和带宽要求的FPGA逻辑和IO接口连接。NoC也很适合数据中心FPGA，数据中心中的FPGA通常配置为两部分，包括提供与外部接口的系统级互联以及应用程序加速器，NoC可以显著提高资源利用率、频率和路由拥塞状况。(a) Xilinx Versal (b) Achronix Speedster7t架构中的片上网络系统级互联其他FPGA组件现代FPGA也包含了其他重要组件。一个例子是将比特流加载到数百万个LUT、路由开关、配置位SRAM单元中的配置电路。上电时，配置控制器从板载闪存或PCIe接口等串行加载比特流，当缓冲了足够多的配置位时，并行写入一组配置SRAM单元。该配置电路也可以由FPGA逻辑访问，允许对器件的一部分进行部分重新配置，而另一部分继续处理。FPGA CAD工具也可以选择性地加密比特流，使比特流只能由密钥正确的FPGA使用。FPGA应用程序通常以不同的速度与许多不同的设备进行通信，通常包含数十个时钟，这些时钟大多由可编程锁相环（PLL）、延迟锁定环（DLL）和时钟数据恢复（CDR）电路在片上生成。" }, { "title": "CMU 18-643可重构计算笔记-2：Altera FPGA架构白皮书（2006）", "url": "/posts/RL2/", "categories": "Course, CMU 18-643可重构计算", "tags": "", "date": "2022-08-30 21:22:00 +0000", "snippet": "原文：Altera FPGA Architecture Whiter Paper.这篇文章是Altera在2006年发布的FPGA架构白皮书，简单介绍了其当时的FPGA架构，包括LUT与ALM的设计、相比基本的LUT的优势、路由架构等内容。Logic Fabric在Altear的FPGA器件上，一个最基本的模块是Adaptive Logic Module，即ALM，如上图所示。整个ALM由一...", "content": "原文：Altera FPGA Architecture Whiter Paper.这篇文章是Altera在2006年发布的FPGA架构白皮书，简单介绍了其当时的FPGA架构，包括LUT与ALM的设计、相比基本的LUT的优势、路由架构等内容。Logic Fabric在Altear的FPGA器件上，一个最基本的模块是Adaptive Logic Module，即ALM，如上图所示。整个ALM由一个组合逻辑块、两个Adder、两个Register和一些其他连线组成，一个ALM可以实现一个完整的6输入LUT，或者两个独立的4输入LUT、一个5输入与一个3输入LUT、或其他更复杂的组合，两个LUT之间也可以共享输入，也可以实现7输入函数的一个子集，具体可以参考原文。构建LUT一个LUT由包含了configuratoin memory（CRAM）LUT-mask的SRAM以及一些负责选择CRAM输出的Mux组成，为了实现一个k输入LUT，我们需要2^k个SRAM位以及一个2^k输入的Mux，上图展示了一个4-LUT的结构，除了直接构建之外，也可以由两个3-LUT组合而成。同理，更大的LUT也可以由小LUT组合而成，如下图所示。不过，我们仍然要注意上图中4-LUT和6-LUT的一些区别，若基本逻辑块的LUT大小不同，架构中LUT的数量、每个LUT可用的输入、通过LUT的延迟都会有所区别。即便6-LUT可以通过4个4-LUT实现，这种结构的效率是很低的，在上图中我们注意到，我们只用了16个输入线（应该是19？）中的6个，并产生了额外的延迟。不过并不是说直接构建6-LUT一定就好，我们也需要为6-LUT做出额外的优化。设计ALMALM的设计需要详细了解客户需求并研究各种架构的权衡，Altera的研究表明，6-LUT的设计可以缩短关键路径，提高14%的性能，但这种性能提升也会带来面积损失，由于更大的LUT设计和更多的LUT输入，面积增加了17%，下图说明了不同尺寸的LUT成本和延迟之间的权衡。设计ALM的基本思路是构建更大的LUT来提升性能，但也需要合理地划分为更小的LUT来避免面积增加，这种划分LUT的能力使其具有自适应性。Altera对LUT与ALM的设计进行了复杂的建模，包括逻辑和存储器资源、架构的分层描述、FPGA时序与物理细节等。为了更好地理解使用6-LUT带来的面积损失，Altera分析了每个逻辑函数需要的输入数量，下图展示了目标为4-LUT、5-LUT和6-LUT架构时，综合过程中产生的LUT大小分布。我们注意到绝大多数函数不需要用到6-LUT，即使目标是6-LUT，但只有约30%的函数真正完整用到了6-LUT，除此之外，较小的函数在仅支持基本6-LUT的架构中无法有效实现，会浪费面积并增加成本，如果可以将6-LUT划分为更小的LUT，就可以在6-LUT中实现多个较小的功能，提高资源利用效率。最终Altera的设计是ALM的8输入可拆分LUT，也就是之前ALM的图中展示的样子，下图详细展示了ALM的结构，说明了LUT-mask如何被拆分并在两个不同的逻辑块之间共享，Altera的FPGA综合、布局和布线结果显示，这种结构是最有成本效益的。除此之外，ALM还包含了两个寄存器和两个加法器，添加寄存器是因为实验表明许多应用程序需要高于1:1的寄存器与LUT比值，下图中，几乎一半的设计中寄存器的数量比LUT更多。ALM还包含了加法器以增强ALM的算术计算能力，允许每个ALM进行两位的加法或一个三进制的加法。ALM的优势ALM相比基本的6-LUT效率更高，上图中，Virtex-5实现多个逻辑功能的能力有限，通常只能用作6-LUT，很难用作两个5-LUT，但ALM中的2个额外输入允许其用作两个完整的的5-LUT，提供显著的面积优势。下表中给出了一些功能组合需要的共享输入的数量，例如ALM可以实现2个独立的4输入函数，但Virtex-5 LUT需要3个共享输入。下图展示了另外一个例子，说明ALM可以再没有共享输入的情况下实现一个5输入函数和一个3输入函数，但Virtex-5的LUT需要3个共享输入。路由架构FPGA另一个关键特性是其路由架构，Stratix系列器件引入了MultiTrack互联，尽可能提升连接性能。路由架构提供不同逻辑块集群之间的连接，称为逻辑阵列块（logic array blocks，LAB），可以通过一个LAB到另一个LAB所需的跳数（hop）来衡量，跳数越少，模式越容易预测，性能越好，也越容易在CAD工具中优化。下图展示了Stratix中的路由架构，使用三边架构，通道包含了长度为4、8、16、24的导线，现在假设我们只考虑长度为4的线，下图展示了不同LAB连接到原点所在的LAB的跳数。" }, { "title": "CherrySprings项目", "url": "/posts/CS-RECRUIT/", "categories": "Project, CherrySprings", "tags": "", "date": "2022-08-29 06:04:00 +0000", "snippet": "CherrySprings项目取名于宾州的Cherry Springs State Park（当然现在只是随便取的名字）。我希望可以设计一个可以运行Linux且可以调度多个进程运行的多核系统，在这个过程中了解诸如处理器设计、内存模型与缓存一致性协议、总线与片上网络等多个主题的内容，也许是一个比较综合的大项目（也可能到最后都是大饼），因此也希望邀请志同道合的小伙伴一同加入。设计选择项目可能有如...", "content": "CherrySprings项目取名于宾州的Cherry Springs State Park（当然现在只是随便取的名字）。我希望可以设计一个可以运行Linux且可以调度多个进程运行的多核系统，在这个过程中了解诸如处理器设计、内存模型与缓存一致性协议、总线与片上网络等多个主题的内容，也许是一个比较综合的大项目（也可能到最后都是大饼），因此也希望邀请志同道合的小伙伴一同加入。设计选择项目可能有如下设计选择。开发语言与工具比较流行的开发语言有Verilog、SystemVerilog、Chisel等。我计划采用Chisel语言，原因在于项目本身比较复杂，需要Chisel的语言特性进行类型检查、一部分的逻辑综合等功能，简化工作量，并可以复用Berkeley与Sifive开发的开源IP。开发工具方面，我倾向于尽可能使用已有的开源工具，如香山社区使用的敏捷开发工具。处理器指令集与微架构目前最活跃的开源指令集应该只有RISC-V可供选择，运行完整的Linux或其他类Unix系统需要至少支持RV64IMA + SU + Sv39 + Zicsr + Zifencei。微架构方面，既然重点在多核上，应该尽量简化单个处理器的设计，避免内存模型方面出现过于复杂的问题。我个人认为采用五级流水线即可，我计划直接使用去年自己的处理器设计项目，在此基础上进一步修改。内存模型与缓存一致性协议RISC-V支持RVWMO内存模型，但是既然采用五级流水线的顺序结构，直接使用顺序一致性（SC）模型应该也是自然而然的，如果之后有机会开发乱序的处理器，也可以考虑RISC-V的TSO扩展，但应该是很久之后的事情了。缓存一致性协议方面，可以采用MSI、MESI、MOSI、MOESI等多种状态机，但是也要考虑到状态数量的增加也会导致过渡状态数量大量增加，也许MSI或MESI是比较合理的选择。在基于总线嗅探和目录的协议中，或许目录协议会更容易实现且具有可扩展性，可以很好地在片上网络上进行实现。在缓存一致性方面可能还有很大的设计空间，待进一步补充。另外，在L2缓存或LLC上，可以选择学习借鉴或复用Sifive的block-inclusivecache-sifive或香山的HuanCun等开源IP，不过都有一些缺点，例如可能会有一些增加了设计复杂度的功能（如为了支持non-blocking实现的MSHR），或者像后者可能为香山处理器额外增加了一些可能用不到的功能（如处理缓存别名问题的逻辑）。总线与片上网络有多种总线协议支持片上缓存一致性协议的实现，如AMBA的ACE、CHI和Sifive的TileLink协议。TileLink的spec只有100页不到，实现起来比ACE、CHI简单很多，容易理解，最大的好处在于可以复用rocket-chip的TL接口与相关模块设计，如Crossbar、AXI Bridge等（这也是另一个原因关于为什么我倾向于采用Chisel开发）。片上网络方面，如果核心数较少，可以直接使用Crossbar连接，不过如果需要支持任意数量的核心，需要实现片上网络。这一工作的优先级我认为是最低的，因为多核不需要复杂的片上网络一样可以进行设计，且初期不太可能一下子设计8核16核等过于复杂的情况。片上网络可以自行设计，但也可以采用开源的NoC设计，如Berkeley即将在今年10月1日开源的Constellation片上网络生成器（Chisel Community Conference 2022上的报告）。参考资料有非常多的入门与进阶的课程、资料与教材可以参考。课程 CMU 18-447 Intro to Computer Architecture &amp; 18-740 Modern Computer Architecture. Princeton ELE475 Computer Architecture. 一生一芯项目相关的课程讲义，包括处理器设计讲义、南京大学计算机系统基础课程讲义。 资料与教材 A Primer on Memory Consistency and Cache Coherence. 待补充 " }, { "title": "CMU 18-643可重构计算笔记-1：FPGA的三个时代", "url": "/posts/RL1/", "categories": "Course, CMU 18-643可重构计算", "tags": "", "date": "2022-08-29 05:34:00 +0000", "snippet": "原文：Three Ages of FPGAs: A Retrospective on the First Thirty Years of FPGA Technology.Xilinx在1984年首次推出第一款FPGA，在随后的30年中，FPGA容量增加了10000倍，速度提高了100倍，单位成本和能耗降低了1000倍，如下图所示。这种进步很大程度上是由工艺进步推动的，但不完全是。本文回顾了F...", "content": "原文：Three Ages of FPGAs: A Retrospective on the First Thirty Years of FPGA Technology.Xilinx在1984年首次推出第一款FPGA，在随后的30年中，FPGA容量增加了10000倍，速度提高了100倍，单位成本和能耗降低了1000倍，如下图所示。这种进步很大程度上是由工艺进步推动的，但不完全是。本文回顾了FPGA的三个时代，分别为： 发明时代 1984-1991 扩张时代 1992-1999 积累时代 2000-2007 FPGA与其他器件FPGA与ASICASIC有所谓的Non-recurring Engineering即NRE成本，但FPGA没有，前期没有NRE成本保证了FPGA在一定数量上比ASIC成本更低，如下图所示。有趣的是，当工艺不断提升时，ASIC和FPGA都会受益，但两者的交叉点会往右移动，进一步增大FPGA的成本优势。这种特性推动FPGA不断满足ASIC的要求，不仅降低了前期成本，也降低了设计成本，包括设计、测试、信号完整性等方面的成本，且没有ASIC生产方面的风险。FPGA与PALFPGA并非第一种可编程逻辑器件，之前还有EPROM编程的可编程阵列逻辑（PAL），但FPGA有架构优势。为了理解这种优势，我们首先来看PAL的结构，如下图所示。PAL由两级逻辑结构组成，可以生成任意组合逻辑，从制造的角度来看，PAL和EPROM存储器阵列非常相似。不过当考虑扩展时，PAL的架构问题很明显，因为阵列中可编程点的数量随着输入数量以平方增长，且延迟会增加，为保持速度，功耗也上升。Altera在1980年代推出了CPLD，由多个PAL模块组成，但FPGA则更有扩展性。FPGA的特点在于通过存储单元分布在阵列周围来控制功能和布线，架构如下图所示，可以很容易地提升容量和性能，不过也产生了一些附加的影响，如FPGA架构和内存有很大不同、逻辑单元更小、性能取决于布局和布线的结果、需要复杂的EDA软件等。发明时代 1984-1991第一个FPGA，Xilinx XC2064，仅包含64个逻辑块，每个逻辑块包含两个三输入LUT和一个寄存器。尽管容量很小，但却是一个非常大的芯片，比当时的商用微处理器还大。在早期，成本控制对FPGA的成功至关重要，比如芯片尺寸增加5%可能会使成本翻倍，甚至直接将良率降为0。同时，在1980年代，Xilinx基于LUT的四输入架构被认为是粗粒度的，许多LUT配置未被使用，或者有未使用的输入，浪费了空间，因而也有其他公司设计了包含固定功能的、更细粒度的架构。在这一时代FPGA规模很小，并不会考虑自动布局布线，且当时个人计算机计算能力有限，ASIC布局布线都是在大型计算机上完成的，相比之下FPGA的布局布线通常都需要手工完成。完全不同的FPGA架构的结果是，FPGA供应商需要开发自己的EDA工具，不过优势在于可以充分利用器件的资源和架构，减少客户的NRE成本。发明时代的结束以FPGA的厂商大量减少而告终，摩尔定律引起的量变引起了FPGA技术的质变，FPGA进入了下一个时代。扩张时代 1992-1999在1990年代，摩尔定律继续快速推进，FPGA初创公司在90年代初通常无法跟上最新的工艺，但之后代工厂一旦可以采用新的技术生产晶体管，就可以制造基于SRAM的FPGA。如下图所示，在90年代FPGA和LUT和Wire的数量以指数级增长，这种增长带来了很多影响。面积不再那么重要在1980年代，晶体管利用效率非常重要，但是到了1990年代，尽管面积仍然很重要，但是可以用来换取性能、功能和易用性。EDA变得必不可少在这一时代，FPGA上的设计已经无法手动布局布线了，在1992年，Xilinx XC4010上最多可以有10000个门，但到了1999年，Virtex XCV1000上已经有一百万个门了，FPGA公司的生存逐渐取决于EDA工具的能力。与此同时，工艺进步导致布线资源增加，自动布局工具可以完成不太精确地布局。自动化的设计工具需要自动化友好的架构，即具有丰富的互联资源的架构，简化算法的决策。更丰富的布线资源也允许了跨越多个逻辑块的互连线的出现，使物理上距离较远的逻辑在逻辑上也可以更接近，提高性能。下图显示了工艺技术进步和互联范围提升带来的巨大性能提升。不过不利的一面是，当不使用线的整个长度时，一部分布线资源被浪费了。类似的问题也出现在逻辑块的设计上，比如当设计三输入的逻辑时，四输入LUT中一半的配置单元就被浪费了。设计人员可以手动设计复杂的逻辑结构，但是自动化工具并没有那么聪明，且对于更复杂的设计，逻辑块的互联也变得更复杂。在这一时代，FPGA的幸存者是那些充分利用工艺进步并实现设计自动化的公司。SRAM成为首选技术EPROM、闪存、反熔丝等非易失性可编程技术通常无法跟上最新的工艺，但SRAM可以。反熔丝设备在特定技术节点上更高效，但是在新节点上进行验证需要几个月甚至几年时间，此时下一代的SRAM FPGA已经开始交付了。反熔丝的另一个缺点是缺乏可重复编程性，像ASIC一样需要有验证阶段，抵消了FPGA的优势。LUT成为首选的逻辑单元LUT架构很容易进行综合，尽管传统的ASIC综合器在基于LUT的FPGA上表现不佳，但是可以利用LUT能实现任意函数的特性来进行综合。LUT还提升了互联的效率，FPGA上的互联不是ASIC上的简单金属线，而是包含了缓冲器、路由器、负责控制的存储单元等一系列器件，由于LUT可以实现任何形式的函数，EDA工具只需要在LUT上所需的信号进行路由，输入可以任意互换，基于LUT的逻辑可以减少实现功能所需的互联数量，未使用的LUT功能所造成的浪费可以少于互联需求减少所节省的资源。回顾FPGA容量跟随摩尔定律而增长，引出了对设计自动化的需求，SRAM器件引领了新工艺技术的应用。FPGA设备容量的增长速度逐渐超过许多应用程序的需求，FPGA开始占领低端ASIC的领域。同时，FPGA供应商也始终保持自己对EDA工具的控制，如果依赖外部EDA工具，可能会导致FPGA的NRE成本变高，破坏FPGA的成本优势。积累时代 2000-2007FPGA已逐渐成为数字系统的常见组件，随着容量和设计规模不断增长，FPGA在数据通信行业发现了巨大的市场。2000年代初期的互联网泡沫破灭产生了对降低成本的需求，ASIC制造成本和复杂性让希望定制芯片的小团队望而却步，这些团队也就成为了FPGA的用户。这一阶段FPGA的规模也随着摩尔定律不断变大，但是客户并不一定愿意为过大的FPGA支付高额的溢价。仅增加产能也不足以保证市场增长，当FPGA容量超过一定程度之后，容量的增加带来的应用程序数量的增加程度逐渐减少，也就是说需要用到这么大容量的应用程序也不是很多了，也不再能吸引到越来越多的新客户。FPGA供应商以两种方式应对这一挑战。对于低端市场，他们重新关注效率并生产出容量较低、性能较低的“低成本”FPGA系列：Xilinx的Spartan、Altera的Cyclone和Lattice的EC/ECP。对于高端产品，FPGA供应商希望让客户更容易填满大容量的FPGA，为重要功能设计了软逻辑（IP）库，这些IP包括微处理器（Xilinx MicroBlaze、Altera Nios）、内存控制器和各种通信协议栈。在Virtex-4以硬件实现Ethernet MAC IP之前，这个功能是在LUT中作为Virtex-II的软核实现的。IP组件的标准接口消耗了额外的LUT，但与节省的设计工作量相比，这种低效率并不是一个大问题。从2000年代开始，FPGA用户不再是单纯地实现逻辑，而是把FPGA作为大型系统的一部分，FPGA设计需要符合系统标准，可以和外部组件进行通信。遵守标准、降低成本和降低功耗的压力导致设计策略从简单地添加可编程逻辑、遵循摩尔定律增大容量（如扩展时代）转变为添加专用逻辑块，通常包括存储器、微处理器、乘法器、I/O等，可以减少面积、功耗以及设计工作方面的开销。为了减轻使用新功能的负担并满足系统标准，FPGA供应商提供了逻辑生成器，为软核和硬核处理器上的外围设备提供CoreConnect、AXI和其他总线的接口。为了简化微处理器系统的创建，Xilinx提供了嵌入式设计套件（EDK），Altera也发布了嵌入式系统设计套件（ESDK），可以在FPGA处理器上运行Linux，并在FPGA架构中进行视频压缩和解压缩。但是对于那些不需要固定功能的客户，这些处理器、存储器或乘法器的面积就被浪费了。FPGA供应商一开始尝试利用这些面积，例如Xilinx可以将状态机映射到微处理器的代码中，但这些措施最终被认为并不重要，FPGA供应商和客户都接受了这种形式的浪费。应用程序这一时代FPGA最大的变化是目标应用程序的变化，FPGA业务的增长并非来自通用ASIC替代品，而是来自通信基础设施的采用。Cisco Systems等公司使用FPGA设计自定义数据路径，以通过其交换机和路由器控制大量互联网和语音流量，新的网络路由架构和算法可以在FPGA中快速实现并在现场进行更新，通信行业部门的销售额迅速增长，占FPGA业务销售总额的一半以上。这一成功促使主要的FPGA制造商为通信行业定制FPGA，专为通信而设计的FPGA包含高速I/O、数千个专用高性能乘法器等。为更好地满足通信应用要求而添加的专用块和路由减少了可用的通用逻辑区域。多核处理器和通用图形处理器单元（GPGPU）开始成为主流，但FPGA仍然是高吞吐量、实时计算的首选，且仍然保持了其通用性。摩尔定律在这个阶段，FPGA技术仍人在容量和成本方面继续改进，但功率也继续提高，性能的提升已经需要进行权衡，需要考虑低功耗的问题，在本文的第一张图中可以看到2000年代性能增长的放缓。这种权衡也推动了FPGA功能的积累，仅仅依靠工艺技术进步不足以满足功耗和性能的要求，需要在其他方面进一步做出改进。当前时代：不再是可编程逻辑在积累时代结束时，FPGA仍然是可编程的，但不再局限于可编程。FPGA与ASIC相比仍然有很多优势，但是与多核处理器和GPU相比，仍然有劣势。FPGA开发人员的压力也持续增加，2008年开始的经济增长放缓继续推动降低成本的需求，不仅表现在对降低功能价格的需求上，也表现在降低功耗上。应用程序随着工艺进步，ASIC NRE的成本越来越高，下图中的纵轴是以百万美元为单位的。前文中提到ASIC与FPGA成本的交叉点，已经增长到了几百万台，销售量如此之大的芯片很少，可能仅限CPU、存储器等，再加上对成本的要求以及销售的不确定性，可编程逻辑成为的首选的芯片解决方案。ASIC也在不断扩展，通过片上系统（SoC）设备的形式增加可编程性。随着ASIC向SoC迁移，FPGA供应商开发了可编程SoC，即完全可编程的片上系统，包括存储器、处理器、模拟信号接口、片上网络和可编程逻辑块。Xilinx All-Programmable Zynq、Altera SoC FPGA和Actel/Microsemi M1就是这种新型FPGA的例子。设计工具这些新的FPGA具有新的设计要求。最重要的是，它们是软件可编程的以及硬件可编程的。微处理器不是像积累时代那样被放入FPGA中的简单硬件块，而是包括一个包含高速缓存、总线、片上网络和外围设备的完整环境。捆绑软件包括操作系统、编译器和中间件——一个完整​​的生态系统，而不是一个集成的功能块，编程软件和硬件一起增加了设计的复杂性。工艺技术尽管在过去的30年中工艺规模一直在稳步增长，但摩尔定律对FPGA架构的影响在不同时期却大相径庭。为了在发明时代取得成功，FPGA需要积极的架构和工艺创新。在扩张时代，紧紧跟随摩尔定律是增长市场份额最有效的方式。随着FPGA成长为系统中的重要组件，FPGA需要满足系统标准，并且是以低成本提供系统接口，FPGA行业依靠工艺技术扩展来满足许多系统要求。然而，工艺技术改进速度放慢，要求FPGA提出新型的电路和架构，且不能降低FPGA的易用性，为FPGA电路和应用工程师带来了更大的挑战。" }, { "title": "Chisel笔记（二）：使用Rocket-chip的TLXbar", "url": "/posts/CHISEL2/", "categories": "Chisel", "tags": "", "date": "2022-08-22 20:03:00 +0000", "snippet": "Rocket-chip提供了非常多TileLink协议相关的模块，可以直接使用，但是rocket-chip并没有提供比较完善的文档可以参考。本节以Xbar为例，用一个小例子来说明如何使用rocket-chip中TileLink的模块。本节参考的文档如下： Chipyard文档第9节：TileLink and Diplomacy Reference Rocket-chi...", "content": "Rocket-chip提供了非常多TileLink协议相关的模块，可以直接使用，但是rocket-chip并没有提供比较完善的文档可以参考。本节以Xbar为例，用一个小例子来说明如何使用rocket-chip中TileLink的模块。本节参考的文档如下： Chipyard文档第9节：TileLink and Diplomacy Reference Rocket-chip中diplomacy相关的文档 我们首先创建主设备，如下所示。主设备每隔8个周期发送一次Get请求，且每次请求的地址递增8。class MyClient(implicit p: Parameters) extends LazyModule { val node = TLClientNode( Seq( TLMasterPortParameters.v1( clients = Seq( TLMasterParameters.v1( name = \"my-client\" ) ) ) ) ) lazy val module = new LazyModuleImp(this) { val (tl, edge) = node.out(0) dontTouch(tl) val cycle = RegInit(0.U(32.W)) cycle := cycle + 1.U tl.a.valid := (cycle(2, 0) === 0.U) tl.d.ready := true.B val base = RegInit(\"h20000\".U(32.W)) val (_, gbits) = edge.Get(0.U, base + cycle, 2.U) tl.a.bits := gbits }}接下来创建从设备，如下所示。从设备的地址基址作为一个参数传入，但范围固定。在TLSlaveParameters参数中我们可以指定从设备的一些性质，如地址范围、支持的请求类型等。该从设备只能接受Get请求，且d通道内返回的数据就是a通道内传过来的地址。class MyManager(val base: BigInt)(implicit p: Parameters) extends LazyModule { val device = new SimpleDevice(\"my-device\", Seq(\"my-device0\")) val beatBytes = 8 val node = TLManagerNode( Seq( TLSlavePortParameters.v1( Seq( TLSlaveParameters.v1( address = Seq(AddressSet(base, 0xff)), resources = device.reg, regionType = RegionType.UNCACHED, executable = true, supportsGet = TransferSizes(1, beatBytes) ) ), beatBytes = beatBytes ) ) ) lazy val module = new LazyModuleImp(this) { val (tl, edge) = node.in(0) dontTouch(tl) tl.d.valid := tl.a.valid tl.a.ready := tl.d.ready tl.d.bits := edge.AccessAck(tl.a.bits, tl.a.bits.address) }}最后我们创建Top模块，演示如何使用Xbar，如下所示。我们有一个主设备和两个从设备，从设备地址范围分别为0x20000-0x200ff和0x20100-0x201ff。class Top()(implicit p: Parameters) extends LazyModule { val xbar = LazyModule(new TLXbar) val client0 = LazyModule(new MyClient) val manager0 = LazyModule(new MyManager(0x20000)) val manager1 = LazyModule(new MyManager(0x20100)) xbar.node := client0.node manager0.node := xbar.node manager1.node := xbar.node lazy val module = new LazyModuleImp(this) {}}以下是生成verilog与DAG的代码，生成的graphml文件可以通过yED打开查看，或者直接用yED Live在线平台查看。import chipsalliance.rocketchip.config.Parametersimport freechips.rocketchip.diplomacy._import freechips.rocketchip.util.HasRocketChipStageUtilsobject Elaborate extends App with HasRocketChipStageUtils { override def main(args: Array[String]): Unit = { implicit val config = Parameters.empty val top = LazyModule(new Top()) val verilog = chisel3.stage.ChiselStage.emitVerilog(top.module) writeOutputFile(\".\", \"Top.v\", verilog) writeOutputFile(\".\", \"Top.graphml\", top.graphML) }}测试方便起见，我们可以创建对应的Tester，在src/test/scala/Tester.scala，如下所示。之后我们只需要通过sbt test就可以直接进行仿真并生成波形，不过这种方式只适用于简单的测试，之后进一步的测试可能仍然需要自己写Verilator的测试代码。import chipsalliance.rocketchip.config.Parametersimport chiseltest._import freechips.rocketchip.diplomacy.LazyModuleimport org.scalatest.flatspec.AnyFlatSpecclass Tester extends AnyFlatSpec with ChiselScalatestTester { it should \"test\" in { val annotation = Seq( VerilatorBackendAnnotation, WriteVcdAnnotation ) test(LazyModule(new Top()(Parameters.empty)).module).withAnnotations(annotation) { tb =&gt; tb.clock.step(400) } }}" }, { "title": "Chisel笔记（一）：Diplomacy", "url": "/posts/CHISEL1/", "categories": "Chisel", "tags": "", "date": "2022-08-12 03:30:00 +0000", "snippet": "Diplomacy是一个参数协商框架，用于生成参数化的协议实现。在这个例子中，我们演示如何创建一个简单的参数化加法器与对应的测试模块，我们希望创建一个2-to-1的加法器、一个顶层测试模块、两个driver、以及一个monitor，如下所示加法器测试模块图我们同时还想要对这些模块的接口进行参数化，当进行参数协商时，我们的协议希望两个driver可以提供相同宽度的数据，然后使用两个宽度中较小的...", "content": "Diplomacy是一个参数协商框架，用于生成参数化的协议实现。在这个例子中，我们演示如何创建一个简单的参数化加法器与对应的测试模块，我们希望创建一个2-to-1的加法器、一个顶层测试模块、两个driver、以及一个monitor，如下所示加法器测试模块图我们同时还想要对这些模块的接口进行参数化，当进行参数协商时，我们的协议希望两个driver可以提供相同宽度的数据，然后使用两个宽度中较小的那个来对monitor进行初始化，注意这和Chisel默认的行为相反。import chipsalliance.rocketchip.config.{Config, Parameters}import chisel3._import chisel3.internal.sourceinfo.SourceInfoimport chisel3.stage.ChiselStageimport chisel3.util.random.FibonacciLFSRimport freechips.rocketchip.diplomacy.{SimpleNodeImp, RenderedEdge, ValName, SourceNode, NexusNode, SinkNode, LazyModule, LazyModuleImp}参数协商和传递我们希望数据宽度这一参数可以被不同模块共享，任何想要发送或接收参数信息的模块都需要有一个或多个节点。每一对节点之间可能存在一条或多条有向边，通过边进行参数协商，保证连接的节点之间参数一致。我们可以像如下所示来传递参数，注意该图必须是无环的，我们定义往sink的方向是向下的，往source的方向是向上的。加法器Diplomacy节点参数在这个例子中，我们希望所有电路都使用Scala Int作为数据宽度的类型，我们定义如下case class。case class UpwardParam(width: Int)case class DownwardParam(width: Int)case class EdgeParam(width: Int)节点实现在节点实现（即NodeImp中），我们描述参数如何在我们的图中流动，以及如何在节点之间协商参数。边参数（E）描述了需要在边上传递的数据类型，在这个例子中就是Int；捆绑参数（B）描述了模块之间硬件实现的参数化端口的数据类型，在这个例子中则为UInt。此处edge函数实际执行了节点之间的参数协商，比较了向上和向下传播的参数，并选择数据宽度较小的那个作为协商结果。// PARAMETER TYPES: D U E Bobject AdderNodeImp extends SimpleNodeImp[DownwardParam, UpwardParam, EdgeParam, UInt] { def edge(pd: DownwardParam, pu: UpwardParam, p: Parameters, sourceInfo: SourceInfo) = { if (pd.width &lt; pu.width) EdgeParam(pd.width) else EdgeParam(pu.width) } def bundle(e: EdgeParam) = UInt(e.width.W) def render(e: EdgeParam) = RenderedEdge(\"blue\", s\"width = ${e.width}\")}节点节点可以接收或生成参数，通过Diplomacy参数化的模块必须有一个或多个节点才能接收或发送参数。除了向上和向下两个方向之外，还有向内和向外两个方向属性。对一个节点而言，指向该节点的边是相对于该节点的向内边，反之则为向外边。向内与向外我们继续为driver创建节点，即SourceNode。由于SourceNode只沿向外边生成向下流动的参数，节点实现和之前一样。对AdderDriverNode而言，类型为Seq[DownwardParam]的widths表示初始化该节点（AdderDriver）的模块时输出的数据宽度，这里使用Seq是因为每个节点可能驱动多个输出，在这个例子中，每个节点会连接到加法器和monitor。/** node for [[AdderDriver]] (source) */class AdderDriverNode(widths: Seq[DownwardParam])(implicit valName: ValName) extends SourceNode(AdderNodeImp)(widths)Monitor节点也一样，但是为SinkNode，只从向内边接收上游传来的参数，该节点接收的参数是类型为UpwardParam的width。/** node for [[AdderMonitor]] (sink) */class AdderMonitorNode(width: UpwardParam)(implicit valName: ValName) extends SinkNode(AdderNodeImp)(Seq(width))加法器节点接收两个AdderDriverNode的输入，并把输出传递给monitor，该节点为NexusNode。dFn将向内边传来的向下的参数，映射到向外边的向下的参数，uFn将向外边的向上的参数，映射到向内边的向上的参数。/** node for [[Adder]] (nexus) */class AdderNode(dFn: Seq[DownwardParam] =&gt; DownwardParam, uFn: Seq[UpwardParam] =&gt; UpwardParam)(implicit valName: ValName) extends NexusNode(AdderNodeImp)(dFn, uFn)创建LazyModuleLazy的意思是指将表达式的evaluation推迟到需要的时候。在创建Diplomacy图之后，参数协商是lazy完成的，因此我们想要参数化的硬件也必须延迟生成，因此需要使用LazyModule。需要注意的是，定义Diplomacy图的组件（在这个例子里为节点）的创建不是lazy的，模块硬件需要写在LazyModuleImp。在这个例子中，我们希望driver将相同位宽的数据输入到加法器中，monitor的数据来自加法器的输出以及driver，所有这些数据位宽都应该相同。我们可以通过AdderNode的require来限制这些参数，将DownwardParam向下传递，以及将UpwardParam向上传递。/** adder DUT (nexus) */class Adder(implicit p: Parameters) extends LazyModule { val node = new AdderNode ( { case dps: Seq[DownwardParam] =&gt; require(dps.forall(dp =&gt; dp.width == dps.head.width), \"inward, downward adder widths must be equivalent\") dps.head }, { case ups: Seq[UpwardParam] =&gt; require(ups.forall(up =&gt; up.width == ups.head.width), \"outward, upward adder widths must be equivalent\") ups.head } ) lazy val module = new LazyModuleImp(this) { require(node.in.size &gt;= 2) node.out.head._1 := node.in.unzip._1.reduce(_ + _) } override lazy val desiredName = \"Adder\"}AdderDriver随机生成位宽为finalWidth的数据，并传递到numOutputs个source。/** driver (source) * drives one random number on multiple outputs */class AdderDriver(width: Int, numOutputs: Int)(implicit p: Parameters) extends LazyModule { val node = new AdderDriverNode(Seq.fill(numOutputs)(DownwardParam(width))) lazy val module = new LazyModuleImp(this) { // check that node parameters converge after negotiation val negotiatedWidths = node.edges.out.map(_.width) require(negotiatedWidths.forall(_ == negotiatedWidths.head), \"outputs must all have agreed on same width\") val finalWidth = negotiatedWidths.head // generate random addend (notice the use of the negotiated width) val randomAddend = FibonacciLFSR.maxPeriod(finalWidth) // drive signals node.out.foreach { case (addend, _) =&gt; addend := randomAddend } } override lazy val desiredName = \"AdderDriver\"}AdderMonitor打印加法器输出并检测错误，有两个AdderMonitorNode节点从AdderDriver接收加法的两个输入，以及一个AdderMonitorNode节点从加法器接收加法的输出。/** monitor (sink) */class AdderMonitor(width: Int, numOperands: Int)(implicit p: Parameters) extends LazyModule { val nodeSeq = Seq.fill(numOperands) { new AdderMonitorNode(UpwardParam(width)) } val nodeSum = new AdderMonitorNode(UpwardParam(width)) lazy val module = new LazyModuleImp(this) { val io = IO(new Bundle { val error = Output(Bool()) }) // print operation printf(nodeSeq.map(node =&gt; p\"${node.in.head._1}\").reduce(_ + p\" + \" + _) + p\" = ${nodeSum.in.head._1}\") // basic correctness checking io.error := nodeSum.in.head._1 =/= nodeSeq.map(_.in.head._1).reduce(_ + _) } override lazy val desiredName = \"AdderMonitor\"}创建顶层模块顶层模块负责初始化所有模块，注意这里driver和monitor初始化了不同的width，但是参数会通过Diplomacy框架进行协商。我们通过:=来创建Diplomacy图。/** top-level connector */class AdderTestHarness()(implicit p: Parameters) extends LazyModule { val numOperands = 2 val adder = LazyModule(new Adder) // 8 will be the downward-traveling widths from our drivers val drivers = Seq.fill(numOperands) { LazyModule(new AdderDriver(width = 8, numOutputs = 2)) } // 4 will be the upward-traveling width from our monitor val monitor = LazyModule(new AdderMonitor(width = 4, numOperands = numOperands)) // create edges via binding operators between nodes in order to define a complete graph drivers.foreach{ driver =&gt; adder.node := driver.node } drivers.zip(monitor.nodeSeq).foreach { case (driver, monitorNode) =&gt; monitorNode := driver.node } monitor.nodeSum := adder.node lazy val module = new LazyModuleImp(this) { when(monitor.module.io.error) { printf(\"something went wrong\") } } override lazy val desiredName = \"AdderTestHarness\"}生成Verilog以下是生成电路的Verilog代码。val verilog = (new ChiselStage).emitVerilog( LazyModule(new AdderTestHarness()(Parameters.empty)).module)也可以用以下代码替代。object Elaborate extends App { (new ChiselStage).execute(args, Seq(chisel3.stage.ChiselGeneratorAnnotation( () =&gt; LazyModule(new AdderTestHarness()(Parameters.empty)).module)) )}附录：环境配置build.sbt配置如下，需要注意的是不要直接使用dependsOn来添加对rocket-chip的依赖，而是需要通过git submodule来添加，并手动设置rocket-chip的路径。val chiselVersion = \"3.5.4\"scalaVersion := \"2.12.16\"lazy val commonSettings = Seq( scalacOptions ++= Seq( \"-language:reflectiveCalls\", \"-deprecation\", \"-unchecked\", \"-feature\", \"-Xsource:2.11\" ), libraryDependencies ++= Seq(\"org.scala-lang\" % \"scala-reflect\" % scalaVersion.value), libraryDependencies ++= Seq(\"org.json4s\" %% \"json4s-jackson\" % \"3.6.12\"), libraryDependencies ++= Seq(\"org.scalatest\" %% \"scalatest\" % \"3.2.12\" % \"test\"), addCompilerPlugin((\"org.scalamacros\" % \"paradise\" % \"2.1.1\").cross(CrossVersion.full)))lazy val chiselSettings = Seq( libraryDependencies ++= Seq( \"edu.berkeley.cs\" %% \"chisel3\" % chiselVersion, \"edu.berkeley.cs\" %% \"chiseltest\" % \"0.5.4\" ), addCompilerPlugin((\"edu.berkeley.cs\" % \"chisel3-plugin\" % chiselVersion).cross(CrossVersion.full)))lazy val `api-config-chipsalliance` = (project in file(\"rocket-chip/api-config-chipsalliance/build-rules/sbt\")) .settings(commonSettings)lazy val hardfloat = (project in file(\"rocket-chip/hardfloat\")) .settings(commonSettings, chiselSettings)lazy val rocketMacros = (project in file(\"rocket-chip/macros\")) .settings(commonSettings)lazy val rocketchip = (Project(\"rocket-chip\", file(\"rocket-chip/src\"))) .settings(commonSettings, chiselSettings) .settings( Compile / scalaSource := baseDirectory.value / \"main\" / \"scala\", Compile / resourceDirectory := baseDirectory.value / \"main\" / \"resources\" ) .dependsOn(`api-config-chipsalliance`) .dependsOn(hardfloat) .dependsOn(rocketMacros)lazy val example = (project in file(\".\")) .settings(commonSettings, chiselSettings) .dependsOn(rocketchip)" }, { "title": "TileLink笔记（五）：TL-C", "url": "/posts/TL5/", "categories": "TileLink", "tags": "", "date": "2022-08-11 04:02:00 +0000", "snippet": "TileLink缓存支持级（TL-C）给主端代理提供缓存共享数据块副本的能力，保证缓存一致性。本章描述TL-C中缓存的数据副本上允许进行哪些访存操作，以及用来传输数据块的缓存的消息类型，实现中所定义的一致性协议描述了副本和权限如何通过具体的TileLink代理网络传输以回复所接收的存储访问操作，但具体的一致性协议描述不在TileLink协议内容范围内。TL-C新添加了三种操作、三个通道、一个...", "content": "TileLink缓存支持级（TL-C）给主端代理提供缓存共享数据块副本的能力，保证缓存一致性。本章描述TL-C中缓存的数据副本上允许进行哪些访存操作，以及用来传输数据块的缓存的消息类型，实现中所定义的一致性协议描述了副本和权限如何通过具体的TileLink代理网络传输以回复所接收的存储访问操作，但具体的一致性协议描述不在TileLink协议内容范围内。TL-C新添加了三种操作、三个通道、一个五步的消息序列模板以及十种消息类型。新的操作transfer可以创建或者清除数据块的缓存副本。转换操作不会修改数据块的值，但是会改变他们的读写权限。转换操作可以与之前定义的TL-UL、TL-UH访存操作无缝协作，保证操作进行序列化的正确性。可缓存性是存储地址范围的一个属性，一个TileLink的实现需保证避免无法缓存地址范围的副本出现。使用TileLink实现缓存一致性所有基于Tilelink的一致性协议都由一系列操作组成，通过这些操作，可以完成读写数据块权限的在转变。在代理对已缓存的副本执行响应的操作之前，必须获得正确的权限。当代理希望在本地处理一个访问操作时，它必须首先使用转换操作来获得必要的权限。转换操作在网络上创建或删除副本，从而修改每个副本可以提供的权限。代理中数据块副本的基本权限可以包括：None、Read或Read+Write。缓存架构中副本 可用的权限取决于缓存层次结构中副本的当前存在情况，如下所述。对于任何给定的地址，在给定主端和拥有该地址范围的从端之间都只会存在一个具体的路径。在TileLink网络DAG中，所有这些路径都覆盖会形成一个树，根节点上仅有一个从端节点。对于每个地址，此树包含所有针对该地址的操作执行的路径。如果我们省略了所有不能缓存数据的代理，那么就会留下一个描述所有可能缓存该地址数据的缓存代理位置的树。在逻辑时间的任意时刻，这些代理的某些子集真正包含缓存数据的副本。这些代理形成 了一致性树 。包容性（Inclusive）的TileLink一致性协议要求树在响应内存访问操作时进行提权（Grow）和降权（Shrink）操作。图中的每个节点都属于树中的位置，分为以下四类： Nothing：当前没有缓存数据副本的节点，没有读写权限。 主干（Trunk）：在顶点（Tip）和根（Root）之间的路径上拥有缓存副本的节点，具有其副本的读权限，该副本可能包含脏数据。 无分支的顶点（Tip with no Branches）：具有缓存副本的节点，且可以用作内存访问序列化，对其拥有的可能包含脏数据的副本具有读写权限。 有分支的顶点（Tip with Branches）：具有缓存副本的节点，且可以用作内存写入序列化，对其拥有的可能包含过去写入的脏数据的副本具有读写权限。 分支（Branch）：该节点由Trunk结点分出，具有只读数据缓存副本。 同一个TileLink拓扑图中几种可能的一致性树上图例举了几个覆盖在单个TileLink网络上的一致性树。在A中，树的根节点有唯一的 副本，这使得它既是树的根节点，也是树的顶端节点。在B中，主端通过提升主干的特权 (Grow)，获得读写权限，直到其位于顶点处。在C中，另一个主端通过扩展一个分支获得了读权限，这意味着之前的顶点现在也是一个只读分支，且公共的父节点是主干节点。在D中，其他主端也升级为分支，进一步将尖端向根部移动，而最初的请求者已经主动修剪了其分支。权限与访存操作之间的关系上表分别描述哪种状态下的节点上可以执行哪些访问操作，并且这还定义这些状态在一致性树中的位置，另外协议具体定义的状态可以基于这些基本状态。操作三个新操作统称为transfer操作，原因是这些操作将数据块的副本传输到内存层次结构中的新位置，包括： Acquire：在请求主端中创建块（或其特定权限）的新副本。 Release：从请求的主端将块的副本（或其特定权限）释放回从端。 Probe：强制将块的副本（或其特定权限）从主端移到发起请求的从端。 Acquire操作要么以扩展树干的形式，要么以从现存的分支或者尖端添加新的分支的形式来拓展树。在新的分支生成前，旧的主干或分支可能需要递归的Probe方法进行修剪。为了响应缓存容量冲突，可通过Release操作主动裁剪分支。通道为了支持转换操作，TL-C在执行内存访问操作所需的两个基本通道上添加了三个新通道。A和D通道也被重新用于发送额外的消息，以实现转换操作。转换操作使用的五个通道分别是： 通道A：主端为了读取或写入缓存块副本而发起对权限的请求。 通道B：从端查询或修改主端对缓存数据块的权限，或将内存访问前递（forward）给主端。 通道C：主端响应通道B传输的消息，可能会释放带有任脏数据块的权限。也用于主动回写脏的缓存数据。 通道D：从端向原始请求者提供数据或权限，授予对缓存块的访问权。也用于确认脏数据的主动写回。 通道E：主端提供此次事务完成的最终确认消息，从端可用来事务序列化。 消息TL-C转换操作消息总览TL-C规定了五个通道内的十种消息，如上表所示。注意某种请求消息可能会对应多种回复消息，取决于从端如何返回数据副本。权限转换Transfer是对权限进行操作，因此组成其的消息必须指定预期的结果：升级到更高权限，降级到更低权限，或者一个保持权限不变的无操作，这些变化是根据它们对特定地址的一致性树形状的影响来指定的。权限转换的分类与编码上表展示了基于TileLink的一致性协议所需的权限转换，被分成四个子集： Prune：权限降级，缩小一致性树。相比过去，完成操作之后具有较低的权限。 Grow：权限升级，增大一致性树。相比过去，完成操作之后具有较高的权限。 Report：包含权限保持不变，但报告当前权限状态。 Cap：包含权限更改，但不指定原始权限是什么，而只指定它们应该成为什么。 消息流与波形Transfer操作引入新的事务流，这些事务流可以组成完整的缓存一致性协议事务。下图描述了三个新消息流。TileLink操作的事务流包含了所有三种transfer操作的事务流上图展示了一个消息流，包含了所有三种transfer操作，具体如下： 缓存主代理向从代理发送Acquire。 为保证给响应消息留足够空间，主代理主动发送Release，即写回操作。 从代理访问存储结构，以完成写操作。 从代理通过ReleaseAck确认写回操作已完成。 从代理向其他主代理发送必需的Probe。 从代理等待所有被Probe的主代理返回ProbeAck。 若有需要，从代理还需访问存储结构。 从代理向原请求设备发送Grant。 原主代理以GrantAck说明此次事务成功完成。 虽然这三个流程构成了所有涉及缓存块传输的TileLink事务的基础，但是当它们临时地重叠或分层地组合时，会出现一些边界情况。现在我们将讨论TileLink如何管理并发性，并分散到各主从代理之间。TileLink假设消息并不完全是点到点的有序的传递，来自高优先级通道的消息必须能够在网络中先于较低优先级的消息处理，从端充当连接到它的所有主端的一个同步节点。由于每个事务都必须通过发送给从端的一条Acquire消息作为初始消息，因此从端可以轻松对事务进行排序。一个非常安全的实现方式是一次只接受一个事务，但是这种方式对性能影响巨大，而且事实证明我们可以在继续提供正确的序列化的同时增加并发性。尽管事务有着天然的分布式属性，对代理的行为施加一些限制，仍能够保证事务间的有序性。下图描述了每种操作并发的一些限制。操作流对TileLink代理上的并发限制在发射或阻塞请求消息方面最容易理解。所有请求消息都会引发响应消息，并且响应消息保证最终产生向前推进的效果，但是在某些情况下，在收到还未处理的响应消息之前，不应该发出针对同一块的递归请求消息。我们按照请求消息类型来对这些情况分类： Acquire：如果在块上有一个pending的Grant块，主端就不应发射一个Acquire。一旦Acquire被发出，主端不应该在该块发出进一步的Acquire，直到它收到一个Grant。 Grant：在块上有一个pending的ProbeAck时，从端不应该发射Grant。一旦发出了一个Grant时，从端不应该在该块上发射Probe，直到它收到一个GrantAck。 Release：在块上有一个pending的Grant时，主端不应该发射一个Release。一旦发出了一个Release后，主端不能发射ProbeAck，直到它收到来自从端确认写回操作完成的ReleaseAck。 Probe：在块上有一个pending的GrantAck时，从端不应该发射一个Probe。一旦发射了一个Probe，从端不能进一步发射Probe，直到它收到一个ProbeAck。 从代理端通过GrantAck来序列化相互交织的消息流的Grant和Probe上图展示了一个消息流，包含了Grant和Probe，具体如下： 主代理A先发送Acquire，但由于网络延迟，后到从代理。 主代理B后发送Acquire，但先到达从代理，被序列化在A的前面。 从代理向A发送Probe，即使A还在等待Grant，也必须先处理Probe。 从代理接收到A的ProbeAck后，向B发送Grant。 从代理接收到A的Acquire，但由于正等待B的GrantAck，所以现在还不能处理这个请求。 一旦接收到B的GrantAck，A的事务就可以正常处理了。 从代理向B发送Probe，但这个操作被在上一个Grant之后。 从代理向A发送合适类型的Grant(包括数据副本)，说明A在Acquire后被Probe过。 从代理端序列化相互交织的消息流的Release和Probe上图展示了一个消息流，包含了Release和Probe，具体如下： 主代理A向从代理发送Acquire。 与此同时，主代理B通过Release主动剔除相同的数据缓存块。 从代理向B发送Probe。 从代理等待每个发送出的Probe，但可以处理主动发起的Release。从代理发送ReleaseAck确认主动写回的操作完成。 B在接收到写回确认前不处理Probe。 在从代理接收到B的ProbeAck后，A的事务就可以正常执行了。 TL-C消息权限转换新增的三个通道有六个新消息，另外新增了一个通道A消息、三个通道D消息。新的通道是B、C和E，新的消息类型是Acquire、Probe、ProbeAck[Data]、Release[Data]、ReleaseAck、Grant[Data]和GrantAck。AcquireBlockAcquireBlock消息是主代理计划在本地缓存数据块的副本时，发起的请求消息类型，主代理还可以使用这种消息类型来升级他们已缓存块上的权限（例如，获得只读副本的写权限），与Get消息一样，Acquire消息本身不包含数据，下表说明了通道A内该消息的信号编码。AcquireBlock消息的编码AcquirePermAcquirePerm消息是主代理计划升级缓存数据块的权限，且无需提供数据副本时，发起的请求消息类型，下表说明了通道A内该消息的信号编码。AcquirePerm消息的编码ProbeBlockProbeBlock消息是从代理用来查询或修改由特定主代理存储的数据块的缓存副本的权限的请求消息，从代理响应另一个主代理的Acquire或主动发起，可以取消主代理对一块缓存块的权限，下表说明了通道B内该消息的信号编码。ProbeBlock消息的编码ProbePermProbePerm消息是从代理用来查询或修改由特定主代理存储的数据块的缓存副本的权限的请求消息，从代理响应另一个主代理的Acquire或主动发起，可以取消主代理对一块缓存块的权限，不过和ProbeBlock不同的是，ProbePerm要求不需要提供数据副本就可以发起请求，下表说明了通道B内该消息的信号编码。ProbePerm消息的编码ProbeAckProbeAck消息是主代理用来回复Probe的消息，下表说明了通道C内该消息的信号编码。ProbeAck消息的编码ProbeAckDataProbeAckData消息是主代理使用的响应消息，用于确认接收到Probe，并写回发送请求的从代理所需的脏数据，下表说明了通道C内该消息的信号编码。ProbeAckData消息的编码GrantGrant消息是一个响应也是一个请求消息，从代理使用它来确认接收到一个Acquire，并提供访问缓存块的权限给原始发送请求的主代理，下表说明了通道D内该消息的信号编码。Grant消息的编码GrantDataGrantData消息既是响应也是请求消息，从代理使用它向原始请求主代理提供确认消息以及数据块副本，下表说明了通道D内该消息的信号编码。GrantData消息的编码GrantAckGrantAck响应消息被主代理用来提供事务完成的最终确认消息，同时也被从代理用来确 保操作的全局序列化，下表说明了通道E内该消息的信号编码。GrantAck消息的编码ReleaseRelease消息是主代理用来主动降低其对一个缓存数据块的权限的请求消息，下表说明了通道C内该消息的信号编码。Release消息的编码ReleaseDataReleaseData消息是主代理发起的请求消息，用于主动降低对一块缓存数据块的权限，并将脏数据写回从代理，下表说明了通道C内该消息的信号编码。ReleaseData消息的编码ReleaseAckReleaseAck消息是一个从代理发起的响应消息，用来响应Release[Data]，用于确保从代理的操作的全局序列化，下表说明了通道D内该消息的信号编码。ReleaseAck消息的编码TL-UL和TL-UH在通道A与通道D上的消息TL-C规定了TL-UL和TL-UH现有消息的权限转换： Get操作隐式地将权限Cap为None（Invalid）。 PutFullData、PutPartialData、ArthmeticData、LogicalData隐式地将权限Cap为非Read+Write（主干或顶点），即None（Invalid）或Read（分支）。 TL-UL和TL-UH在通道B与通道C上的消息除了三个新操作（Acquire、Probe、Release）之外，TL-C重新定义了所有在通道B和C上的TL-UH的操作，这允许那些通道被用来转发Access和Hint操作给远端的缓存数据所有者。换句话说，其实现可以选择使用基于更新的协议，而不是基于失效的协议。具体的消息类型和TL-UH的消息类似，可以参考原文，此处篇幅有限就不再详细阐述了。" }, { "title": "TileLink笔记（四）：TL-UH", "url": "/posts/TL4/", "categories": "TileLink", "tags": "", "date": "2022-08-10 15:55:00 +0000", "snippet": "TileLink无缓冲重量级（TL-UH）用于最后一级缓存之外的总线，这种应用中不需要使用权限转换的操作。TL-UH建立在TL-UL的基础上，并提供一部分额外的操作，包括： 原子（Atomic）操作：在原子性地读取现存的数据值的同时，同步地写入一个新的值，此新值为某些逻辑和算法操作的结果。 预处理（Hint）操作：提供了与某些性能优化相关的可选的提示性消息。 ...", "content": "TileLink无缓冲重量级（TL-UH）用于最后一级缓存之外的总线，这种应用中不需要使用权限转换的操作。TL-UH建立在TL-UL的基础上，并提供一部分额外的操作，包括： 原子（Atomic）操作：在原子性地读取现存的数据值的同时，同步地写入一个新的值，此新值为某些逻辑和算法操作的结果。 预处理（Hint）操作：提供了与某些性能优化相关的可选的提示性消息。 簇发（Burst）消息：允许带有比数据总线宽度更大的数据的消息在多个周期内作为数据包传输，应用于在Get、Put和原子操作中多种包含数据的消息。 下表列举了TL-UH支持的消息。TL-UH消息总览消息流与波形原子和预处理操作的信号波形图原子内存访问的消息流预处理操作的消息流消息我们定义TL-UH新增的四个消息类型的信号编码。算术数据（ArithmeticData）一个算术数据消息是一个代理为对一数据块进行算术操作，先读取然后改写，而发起的 访问一块特定的数据块的请求消息，下表说明了通道A内该消息的信号编码。ArithmeticData消息的编码ArithmeticData的param域逻辑数据（LogicalData）一个逻辑数据消息是一个代理为对一数据块进行位逻辑操作，先读取然后改写，而发起 的访问一块特定的数据块的请求消息，下表说明了通道A内该消息的信号编码。LogicalData消息的编码LogicalData的param域预处理（Intent）一个Intent消息是一个代理为了表示未来的可能要访问一块特定数据块的目的而发出的请求消息，下表说明了通道A内该消息的信号编码。Intent消息的编码Intent的param域预处理确认（HintAck）HintAck是用于一个Hint操作的确认消息，下表说明了通道D内该消息的信号编码。HintAck消息的编码" }, { "title": "TileLink笔记（三）：TL-UL", "url": "/posts/TL3/", "categories": "TileLink", "tags": "", "date": "2022-08-10 02:37:00 +0000", "snippet": "TileLink无缓存轻量级（TL-UL）是最简单的TileLink协议兼容级别，可用于连接低性能的外设以减小总线的面积消耗。该兼容级别的代理都支持两种存储访问操作： 读（Get）操作：从底层内存中读取一定量的数据。 写（Put）操作：向底层内存中写入一定数目的数据，写操作支持基于字节t通路掩码的部分写功能。 在TL-UL中，每条消息都必须放在一拍中，不支持簇发操作...", "content": "TileLink无缓存轻量级（TL-UL）是最简单的TileLink协议兼容级别，可用于连接低性能的外设以减小总线的面积消耗。该兼容级别的代理都支持两种存储访问操作： 读（Get）操作：从底层内存中读取一定量的数据。 写（Put）操作：向底层内存中写入一定数目的数据，写操作支持基于字节t通路掩码的部分写功能。 在TL-UL中，每条消息都必须放在一拍中，不支持簇发操作，TL-UL一共定义了与存储访问操作相关的三种请求消息和两种响应消息类型，下表列举了这些消息。TL-UL消息总览消息流与波形Get和Put操作的波形Get操作的消息流Put操作的消息流穿越多个代理层级的数据块读取消息流消息我们定义TL-UL所包含的五个消息类型的信号编码。读（Get）Get消息是代理发出的请求，用于访问一块特定的数据存储块来读取数据，下表说明了通道A内该消息的信号编码。Get消息的编码完整写（PutFullData）PutFullData是代理请求访问并写入一整块数据时发出的消息，下表说明了通道A内该消息的信号编码。PutFullData消息的编码部分写（PutPartialData）PutPartialData是代理请求访问并写入一块数据时发出的消息，下表说明了通道A内该消息的信号编码。PutPartialData消息的编码无数据确认（AccessAck）AccessAck是一个送往原请求代理的无数据确认消息，下表说明了此消息在通道D中的编码。AccessAck消息的编码带数据确认（AccessAckData）AccessAckData是一个向原请求代理返回数据的确认消息，下表说明了此消息在通道D中的编码。AccessAckData消息的编码" }, { "title": "TileLink笔记（二）：序列化、死锁避免、操作与消息", "url": "/posts/TL2/", "categories": "TileLink", "tags": "", "date": "2022-08-09 21:58:00 +0000", "snippet": "在上一节中我们简单了解了TileLink总线协议，包括总体架构与信号描述。具体来说，TileLink有三种协议扩展，即TL-UL、TL-UH和TL-C，有五个通道，分别为A、B、C、D和E，其中B、C和E为TL-C独有。在这一节中我们继续讨论序列化、死锁避免、操作与消息。序列化TileLink中的五个通道实现为五个物理隔离的单向并行总线。许多TileLink消息包含有效的数据负载，而根据消息...", "content": "在上一节中我们简单了解了TileLink总线协议，包括总体架构与信号描述。具体来说，TileLink有三种协议扩展，即TL-UL、TL-UH和TL-C，有五个通道，分别为A、B、C、D和E，其中B、C和E为TL-C独有。在这一节中我们继续讨论序列化、死锁避免、操作与消息。序列化TileLink中的五个通道实现为五个物理隔离的单向并行总线。许多TileLink消息包含有效的数据负载，而根据消息和数据总线的大小，可能需要跨多个时钟周期发送，多拍的信息则通常被称为突发传输或簇发（burst），不过没有数据载荷的TileLink消息总是在单拍中完成。流量控制规则为了实现正确的ready和valid信号握手，需要遵守以下规则： 当ready信号为低时，接收者不能处理数据并且发送者认为该拍内的数据未被处理。 当valid信号为低时，接收者应当认为该数据信号并不是一个正确的TileLink数据。 valid必须与ready信号独立，如果一个发送者希望发送一拍数据，则必须使能一个valid信号，同时该valid信号是与接收者的ready信号独立的。 从ready到valid信号或者任何数据或控制信号之间不能存在任何组合逻辑路径。 对于能够携带簇发的TileLink的通道，有着额外的规则约束。一个簇发一旦第一拍数据 被接收，直到最后一拍数据传输结束都认为处于传输过程中。当一个数据包处于传输过程中 时，如果valid信号为高，发送者必须满足： 只有同消息的簇发的一拍数据。 控制信号与第一拍相同。 数据为与之前拍数据的地址再加上按数据总线的字节宽度得到的地址所对应的数据。 在一个64-bit的A通道中，6个消息的Ready-Valid信号实例在上面的例子中，F消息的信号长度为2^5=32字节，需要4拍来传输；J消息操作码为4，表示一个Get消息，不携带任何数据，单拍即可完成。在一个64-bit的D通道中，6个消息的Ready-Valid信号实例请求-响应消息排序我们现在定义在一个具有多拍数据包的情况下，发送响应消息时的规则。在如下情况下，响应消息的第一拍可以出现在响应通道上： 在请求消息被接收了的同一个周期出现，但不会在这周期之前出现。 在请求消息的第一拍握手成功之后的任意时间 跟随在包的响应消息的第一拍后的多拍信号可以在任意延迟后出现，但与此同时，不能有来自其他消息的任一拍数据交错地插入此次传输中。簇发响应Get (4) 和 AccessAckData (1) 在一个8字节宽度总线上的最小与最大延迟上图描述了两个Get操作。Get操作的请求消息（操作码4）从通道A发出，两个消息都访问32字节，在8字节的总线上分4拍取回数据，4拍的AccessAckData（操作码1）回复消息到达通道D。第一个回复消息在一个任意的延迟之后到达，主端接口必须要能够无限期地等待回复消息，在TileLink互联网络中不存在超时设定；第二个Get操作在请求消息被接收的同一个周期内即被回复，这种重叠情况在Get第一拍的信号被接收之后是允许的，但回复消息不能再提前出现，因为第二个Get首次出现时，a_ready为低，请求消息被拒绝了，d_valid信号也必须为低。簇发请求PutFullData (0) 和 AccessAck (0) 在一个8字节宽度总线上的最小与最大延迟上图描述了两个Put操作。PutFullData请求消息（操作码0）在通道A发出，对应的AccessAck回复消息（操作码0）则在通道D返回。簇发请求与响应ArithmeticData (2) 和 AccessAckData (1) 在一个8字节宽度总线上的延迟同时含有数据载荷的请求和响应消息适用和之前相同的规则。上图描述了由一个分别都带有数据载荷的请求和响应消息所组成的原子性操作，对于大小为4，16字节（2拍）的操作，要么在请求和响应消息之间存在较长的延迟，要么两者的各拍信号重叠。字节通路带有数据域的TileLink通道总是自然地以小端对齐方式运载数据载荷，一个字节通路传输的数据的地址的最低一部分总是相同的，如下图所示。在一个16字节宽度总线上的数据载荷分布在带有掩码（mask）域的通道A和B上，所有的无效字节通路的掩码位必须为低。除了PutPartialData以外的消息，所有的有效字节通路的掩码的信号必须为高。掩码信号也被其他不带数据载荷的消息使用，当操作需要使用的大小小于数据总线时，掩码应该要按带有数据载荷的信号时一样生成。对于比数据总线大小更大的操作，掩码的所有位数都应该拉高，尽管消息仍为一拍，如下图所示。字节通路掩码的实例死锁避免TileLink被设计为可以完全避免死锁，为了保证一个TileLink拓扑网络不会产生死锁，我们提出一些一致性系统必须遵守的规则，如下所示：TileLink网络需要遵守的规则术语定义 接受（accept）一拍：若发送者在某个通道上把valid信号拉高，接收者若把ready信号拉高则接受一拍。 拒绝（reject）一拍：若发送者在某个通道上把valid信号拉高，接收者若把ready信号拉低则接受一拍。 撤回（retract）一拍：若接收者拒绝一拍，发送者可以拉低valid或在下一拍修改控制或数据信号来撤回一拍。 呈现（present）一拍：若valid拉高且所有控制和数据信号在ready也拉高之前都保持不变，称之为呈现一拍。 进行中（in progress）：一个消息正在进行，表示从被接受的第一拍到最后一拍被接受之间的这段时间。 传输中（in flight）：一个消息正在进行，表示从被呈现的第一拍到最后一拍被接受之间的这段时间。 请求消息：需要一个对应的响应消息，所有的TileLink操作都从某个通道的请求消息开始。 响应消息：跟随在请求消息之后的响应。 已收到的（received）消息：从簇发传输的第一拍被接受开始，消息被收到。 已回答的（answered）消息：收到的请求被响应后，消息被回答。 代理（agent）：TileLink系统中的参与者，有一或多个TileLink链路。 优先级：一个消息的优先级与对应通道的优先级对应，响应消息的优先级总是比对应请求的优先级高，转发消息的优先级也更高。 最终事件：一个事件在任意长但并非无穷长之后最终发生。 向前推进：当所有呈现的拍最终被接受，且所有收到的请求最终被回答时，系统可以向前推进。 不活动的（quiescent）：当没有添加新的消息时，系统处于不活动的状态，注意尽管TileLink保证不出现死锁，但仍然可能产生活锁或饥饿状态。为了限制这种情况，代理只被允许发送n+rf个消息，包括n为该代理允许发送的新消息、r为该代理接收的消息以及f为该代理为每个收到的消息允许发送的后续消息。 符合或不符合协议要求的代理示例周期性转发的主设备（不符合）一个代理有一个主接口和一个从接口，在A通道接受请求消息，但在转发消息时只在偶数周期拉高valid信号。这个代理不符合协议要求，原因是该代理不能完整呈现转发的消息，不能保证最终回答一个收到的请求。等待刷新（符合）一个DDR控制器周期性地无法处理请求，在这段时间内无条件地拉低所有ready信号，但注意到该代理仍然最终可以接受呈现的拍，因此还是符合协议要求的。等待消息被接收（不符合）一个代理有两个用于发送消息的链路，在链路1上发起簇发传输A，并且几拍中的第一拍被接受，表明消息已被接收，但仍在传输中，然后在链路2上发起簇发传输B，此时A的传输还未结束。在恢复A之前等待B被接受是不符合协议要求的。非常慢的仲裁器（符合）考虑一个具有三个链路的TL-UH代理，仲裁来自两条链路的A通道请求，并转发到第三条链路的A通道，但该仲裁器的吞吐量非常低。代理空闲时将从任一输入的A通道中选择一个valid请求，一旦选择了一个请求，就会在另一个A通道上拉低ready信号，并在所选链路和输出链路之间连接ready和valid（通道A和D）。一旦请求的所有拍都已被输出通道接受，代理在两个输入A通道上拉低ready信号，并等待响应消息的最后一拍被所选的通道接受。该代理符合规则2，因为可以假设输入的A通道都遵守规则2，因此其输出的A通道请求将呈现接收到的请求的所有节拍，D通道也同理。假设代理正在处理输入的一个消息，在转发链路上分别有一个未回答或进行中的消息。由于该链路的优先级高于输入的链路，因此适用规则3，并且代理可以无限期地拒绝后续提交的A通道请求。假设代理空闲并且没有输入呈现消息，这种情况满足规则3i，因为没有消息可以接受，同时也满足规则3ii，因为只有在收到的请求被回答时，仲裁器才会再次空闲。代理图每个TileLink网络都由TileLink链路和这些链路连接的代理组成，该网络可以表示为代理图，其中每个代理都是一个顶点，每个链路都是从主节点指向从节点的有向边，如下图所示。一个小型RISC-V系统的代理图示例操作与消息带有主端接口的TileLink代理通过执行各类操作与共享存储系统进行交互，操作可以完成我们希望的的对地址范围的修改，可能是数据值，可能是这段地址内数据的权限，也可能是在存储层次中的位置，操作通过在通道内传输的具体消息的交换来实现。为了支持一个操作的实现，我们需要支持组成该操作的所有消息。我们详细描述每个操作的具体的消息交换过程，操作分为三个兼容级别，即TL-UL、TL-UH和TL-C。操作分类TileLink 操作能被分为下列三组: Accesses (A) 在具体的地址读或写数据。 Hints (H) 只是提供一些信息，没有实际的影响。 Transfers (T) 在拓扑网络中改变权限或移动缓存拷贝。 不是每个TileLink代理都需要支持所有操作。取决于其的TileLink兼容级别，一个代理需要支持的操作如下表所列。TileLink操作总览消息分类所有操作（蓝色）和对应的消息（紫色），虚线箭头表示请求和响应的配对操作通过在五个TileLink通道内交换消息来执行。某些消息携带数据载荷，而有些则没有。如果一个TileLink消息携带数据载荷，那么其消息名以Data结尾。不是每一个通道都支持所有类型的消息。某些数据的到达必然会导致最终会有一个回复消息发送到请求发出者。带有回复的操作和消息分类如上图所示。下面两张表分别为通过兼容级别和操作对TileLink用到的消息进行分组，以及以通道和操作码来对消息进行排序。通过操作和兼容级别进行分组的TileLink消息总览通过通道与操作码进行排序的TileLink消息注意到存在不同消息类型有着相同的操作码，不同的通道对于操作码有着不同的命名空间。在任何指定的通道内，每一个可能的消息类型有着独有的操作码。不管是在哪些通道内被进行交换的，相同类型的消息有着相同的操作码。操作码空间通过消息内容的有效解码进行分配。寻址TileLink通道内的所有地址都是物理地址。从TileLink有向无环图中的节点出发，每一个有效地址必须导向一条唯一通往一个具体的从端的路径。在TileLink中，地址决定了哪些操作是可以执行的，哪些效果可以产生，哪些排序约束可以施加。能被添加为一个地址空间的属性包括：兼容级别、存储一致性模型、可缓存性、FIFO排序要求、可执行性、特权等级、以及任何服务质量保证。两个主代理M0和M1都可以访问从代理S0，但是只有M1可以通过缓存C访问从代理S1源与目的地标识符TileLink传输路径域总览TileLink中并不是所有的消息都是根据地址来路由。尤其是，回复消息必须返回到正确的请求者。为支持此功能，TileLink通道包括了一个或更多的与本地链路相关的事务标识符域。这些域与address域一起用于路由消息，并且保证每一个发送中的消息能被特定操作唯一标识，如上表所示。操作排序在一个TileLink拓扑网络中，在任意给定时间内会有多个操作在进程中。这些操作可能以任意的顺序完成。为了确保主端能在一个操作完成后，再执行其他操作，TileLink要求从端在操作结束时及时发送一个回复消息。因此，如果处理器要确保两个写操作X和Y的顺序被其他所有代理获取时都是一致的，那么处理器发送X的PutFullData后，必须等待AccessAck回复，在此之后，才发送Y的PutFullData。TileLink的从端，包括缓存，不能在Put操作确认前就将数据写回。唯一的约束是，一旦确认消息发出后，整个拓扑网络不能观察到过去的状态，这是因为在确认消息发出后，所有的被缓存的数据的拷贝都必须是已更新的。例如，对于一个Put操作，其他缓存要么被Probe现有的数据拷贝，要么通过PutFullData将消息前递给其他缓存，并且在确认原始的请求消息前，收集对应的回复消息。发射回复消息的代理需要保证它们接收的操作是一个有效的序列。例如，假设一个代理接收了两个Put，X和Y，并且都没有被确认，必须选择一定的顺序，例如说是X在Y之前。如果选择了这样的顺序，必须保证只有三个状态，X与Y前的状态，X之后且Y之前的状态和X与Y之后的状态。代理不一定要以这样的顺序发射回复消息。然而，在代理已发射了一条回复之后，例如Y，如果此时接受了新的操作Z，那么Z必须排在Y之后。这些规则确保每个代理看到的全局操作排序与主端的确认信号引导的局部排序是一致的。处理器可以等待发出的确认消息返回后再发起其他请求，来实现fence指令。这样的能力使得多处理器在TileLink的共享存储系统中，安全地同步执行操作。" }, { "title": "TileLink笔记（一）：架构与信号描述", "url": "/posts/TL1/", "categories": "TileLink", "tags": "", "date": "2022-08-09 05:36:00 +0000", "snippet": "笔记基于1.7.1-draft版本，但TileLink最新版本为1.8.1，最新版本的spec可以在这里下载。笔记中的中文译文引自刘鹏等人翻译的《SiFive TileLink规格书1.7.1草案预发布版本（中文）》。TileLink是一个芯片级互连标准，允许多个主设备，以支持一致性的存储器映射方式访问存储器和其他从设备。TileLink的设计目标是为片上系统提供一个具有低延迟和高吞吐率传输...", "content": "笔记基于1.7.1-draft版本，但TileLink最新版本为1.8.1，最新版本的spec可以在这里下载。笔记中的中文译文引自刘鹏等人翻译的《SiFive TileLink规格书1.7.1草案预发布版本（中文）》。TileLink是一个芯片级互连标准，允许多个主设备，以支持一致性的存储器映射方式访问存储器和其他从设备。TileLink的设计目标是为片上系统提供一个具有低延迟和高吞吐率传输的高速、可扩展的片上互连方式，来连接通用多处理器、协处理器、加速器、DMA以及各类简单或复杂的设备。总结来说，TileLink是： 免费开放的紧耦合、低延迟的 SoC 总线 为 RISC-V 设计，也支持其他 ISAs 提供物理寻址、共享内存的系统 可用于建立可扩展的、层次化结构的和点对点的网络 为任意数量的缓存或非缓存主设备提供一致性的访问 支持从单一简单外设到高吞吐量的复杂多外设的所有通讯需求 同时还具备以下重要的特性： 缓存一致性的内存共享系统，支持兼容MOESI的一致性协议 对任何遵守该协议的SoC系统来说，可验证确保无死锁 使用乱序的并发操作以提高吞吐率 使用完全解耦的通讯接口，有利于插入寄存器来优化时序 总线宽度的透明自适应和突发传输序列的自动分割 针对功耗优化的信号译码 TileLink定义了三个从简单到复杂的的协议扩展级别，如下图所示。TileLink协议扩展级别架构Tilelink 协议适用于在代理（agent）互联拓扑图中，完成消息（message）的传递。共享地址空间的代理经由点对点的通道（channel）收发消息，来完成操作（operation），该通道被称为链路（link）。 操作（operation）：在特定地址范围内，改动存储数据的内容、权限或是其在多级缓存 中的存储位置。 代理（agent）：在协议中，为完成操作负责接收、发送消息的有效参与者。 通道（channel）：一个用于在主（master）接口和从（slave）接口之间传递相同优先级消息的单向通信连接。 消息（message）：经由通道传输的控制和数据信息。 链路（link）：两个代理之间完成操作所需的通道组合。 网络拓扑下图展示了最基本的TileLink网络操作，两个模块通过链路连接。最基本的TileLink操作TileLink支持多种网络拓扑。具体来说，如果将代理抽象为图论中的节点，而将链路抽象为从主接口指向从接口的有向边，那么任何有能被描述为有向无环图的拓扑结构都可以被支持。下图展示了一个典型的拓扑结构。该拓扑中的交换器（Crossbar）和缓存（Cache）模块都包含一个兼有主接口和从接口的代理节点。一个更复杂的TileLink网络拓扑包含两个代理的交换器，下侧代理有多个主和从接口，上侧代理只有一个从接口，用于配置交换器通道优先级TileLink协议定义了五个逻辑上相互独立的通道，下图标明了五个通道的方向。在任意一对代理之间构成一条TileLink链路的五个通道任何访存操作都需要两个最基本的通道： 通道A：传送一个请求，访问指定的地址范围或对数据进行缓存操作。 通道D：向最初的请求者传送一个数据回复响应或是应答消息。 最高协议兼容层TL-C额外包含另外三个通道，具备管理数据缓存块权限的能力： 通道B：传输一个请求，对主代理已缓存的某个地址上的数据进行访问或是写回操作。 通道C：响应通道B的请求，传送一个数据或是应答消息。 通道E：传输来自最初请求者的缓存块传输的最终应答，用于序列化。 各个通道传递消息的优先级顺序是 A « B « C « D « E，设置优先级保证了消息在TileLink网络的传输过程中不会进入路由环路或是资源死锁。信号描述本节用表格的形式介绍了TileLink五个通道使用到的所有信号，汇总在第一张表。结合每个通道的方向，第二张表的信号类型决定了信号的方向。这些信号的宽度由第三张表描述的值进行参数化。TileLink信号需要注意的是，第二张表中的F类型信号在最新版的TileLink规范中已被移除。通道A（强制）通道A的传输方向是从主接口到从接口，携带请求消息发送到一个特定的地址。通道A信号通道B（TL-C独有）通道B的传输方向是从接口到主接口，用于向保存一个特定缓存块的主代理发送请求消息。通道B信号通道C（TL-C独有）通道C的传输方向是主接口到从接口，携带对通道B请求作为响应的消息发送给一个特定缓存数据块，也被用于自发地（voluntarily）写回脏缓存数据。通道C信号通道D（强制）通道D的传输方向是从接口到主接口，携带对通道A发送到特定地址请求作出响应的消息，还携带了对于通道C自发写回的应答。通道D信号通道E（TL-C独有）通道E的传输方向是主接口到从接口，携带是否收到通道D响应消息的应答信号，用作操作序列化。通道E信号" }, { "title": "《内存一致性与缓存一致性》笔记（九）：异构系统的内存一致性与缓存一致性", "url": "/posts/MCCC9/", "categories": "读书笔记, 内存一致性与缓存一致性", "tags": "", "date": "2022-08-07 02:23:00 +0000", "snippet": "在异构架构中，我们可以在CPU和加速器之间暴露一个全局的共享内存接口，我们假设CPU、GPU等设备共享相同的物理内存（如手机上的SoC这种情况），如下图所示。在这种系统中，共享内存会引发一些新的问题，如什么是内存一致性模型？内存一致性模型如何实现？加速器和处理器的私有缓存如何保持一致？我们接下来首先讨论加速器内的内存一致性和缓存一致性，重点是GPU，然后讨论跨加速器的内存一致性和缓存一致性。...", "content": "在异构架构中，我们可以在CPU和加速器之间暴露一个全局的共享内存接口，我们假设CPU、GPU等设备共享相同的物理内存（如手机上的SoC这种情况），如下图所示。在这种系统中，共享内存会引发一些新的问题，如什么是内存一致性模型？内存一致性模型如何实现？加速器和处理器的私有缓存如何保持一致？我们接下来首先讨论加速器内的内存一致性和缓存一致性，重点是GPU，然后讨论跨加速器的内存一致性和缓存一致性。异构SoC系统模型GPU的内存一致性和缓存一致性早期GPU的架构与编程模型早期GPU主要为并行图形工作负载设计，特点是数据并行度高，但数据共享、同步和通信程度低。GPU架构GPU通常有几十个内核，叫做流式多处理器（Streaming Multiprocessors，SM），每个SM都是高度多线程的，能够运行上千条线程，映射到SM上的线程共享L1缓存和本地scratchpad内存，所有的SM都共享一个L2缓存。GPU通常将线程分组执行，称为warps，一个warp中的所有线程共享PC和堆栈，可以使用掩码来执行独立的线程，表明哪些线程在执行，哪些线程没有在执行，这种并行方式就是我们常听说的SIMT。但是最近，GPU开始允许一个warp中的线程有独立的PC和堆栈，允许线程独立调度，我们接下来也假设线程是可以独立调度的。由于图形工作负载并不经常共享数据或进行同步，早期GPU选择不在L1缓存实现硬件的缓存一致性。GPU编程模型和CPU类似，GPU也有一套虚拟ISA，同时也有一套更高级别的语言框架，如CUDA和OpenCL，框架中的高级语言被编译成虚拟的ISA，并被翻译为本地的二进制文件。GPU虚拟ISA和高级语言框架通过被称为作用域（scope）的线程结构，向程序员暴露GPU架构的层次结构。和CPU相比，GPU线程被分组为叫做Cooperative Thread Arrays（CTA）的集群。CTA作用域指来自同一个CTA的线程集合，保证映射到相同的SM，并共享同一个L1huancun。GPU作用域指的是来自同一个GPU的线程集合，可以来自相同或不同的CTA，并共享L2缓存。最后，系统作用域指整个系统所有线程的集合，共享LLC缓存或统一的共享内存。这种方式可以在没有硬件缓存一致性的情况下，用软件的方式实现数据同步和通信。GPU内存一致性GPU支持宽松一致性模型，通过FENCE指令进行同步，但是由于没有硬件缓存一致性，GPU的FENCE只针对属于同一CTA的其他线程。GPU的store指令也不保证原子性。因此，早期GPU明确禁止在CTA组之间进行数据同步，但实践中可以通过绕过L1，在L2上进行同步。这种方案也有一些问题，绕过L1会导致性能下降，且程序员需要仔细地编写程序。 测验问题8：GPU不支持硬件缓存一致性，因此无法实现内存一致性模型。 答：错误。早期的GPU不支持硬件缓存一致性，但是支持作用域内的宽松内存一致性模型。GPGPU的内存一致性和缓存一致性GPGPU最好满足以下特点： 严格而直观的内存一致性模型，允许在所有线程之间进行同步 一个能够实现内存一致性模型的缓存一致性协议，允许高效的数据共享和同步，且保持常规GPU架构的简单性，因为GPU的主要任务仍然是图形工作负载 我们可以使用类似多核CPU的方法来实现缓存一致性，使用一种与内存模型无关的协议。然而这种方法并不适用于GPU，主要有两个原因。首先，类似于CPU的缓存一致性协议在GPU环境下会产生很高的通信开销，因为L1缓存的总容量通常与L2相当，甚至更大，会产生很大的面积开销，设计也非常复杂；其次，由于GPU保持着数千个活跃的硬件线程，因此需要跟踪相应数量的一致性事务，需要大量的硬件开销。具体的技术细节可以参考原书（埋坑，GPU的部分确实现在看来有些难理解，等之后学习了GPU的架构之后再回来填坑）。其他异构系统我们开始进一步讨论如何在多个设备的系统中暴露一个全局的共享内存接口，难点在于每个设备都可能通过不同的一致性协议来实现不同的内存模型，当多个内存模型不同的设备被集成在一起时，异构系统的内存模型是什么样的？如何对这种系统进行编程？如何整合多个设备的缓存一致性协议？我们接下来简单讨论这些问题，并概述设计空间。异构系统的内存模型如果两个设备A和B连接在一起并共享内存，这种情况下的内存模型是什么样的？一个符合直觉的答案是选择其中较弱的一个作为整体的内存模型，但如果两个内存模型无法比较，这种答案就不可行了。即使两者可以比较，可能也会有更好的答案。四个设备组成的异构系统在如上所示的异构系统中，C1和C2内存模型为SC，C3和C4内存模型为TSO。考虑下面的例子，在第一个例子中，有可能r1和r2同时读到0，但在第二个例子中插入了FENCE后，r1和r2就不能同时读到0了，但需要注意的是在SC的线程上仍然不需要插入FENCE，这种系统实际上产生了一个不同于SC和TSO的复合内存模型。Dekker算法的例子在这种系统中变成也是比较困难的，很难直接精确定义复合内存模型，更好的办法可能是使用HSA或OpenCL等进行编程。异构系统的缓存一致性协议通过分层缓存一致性来集成多个设备考虑两个多核设备A和B，每个设备都有自己的内存模型，并通过不同的缓存一致性协议来实现内存模型，如上图所示。我们应该如何将两个设备集成到一个共享内存异构系统中，并将两个缓存一致性协议正确连接在一起？答案取决于每个设备的内存操作是否满足自己的内存排序规则。分层缓存一致性在分层缓存一致性系统中，本地缓存一致性控制器收到请求后，首先试图在本地满足该请求，如果无法满足就转发到全局一致性控制器，再转发到另一个设备的本地缓存一致性控制器。在这种设计中，全局控制器必须有足够丰富的接口来满足各个设备本地控制器发起的请求，且每个本地控制器必须用shim来扩展，作为两个控制器之间的接口转换。相关的一些例子可以参考原文。减少异构系统缓存一致性的带宽需求在异构系统中，内核之间的缓存一致性通信带宽可能成为性能瓶颈。一种解决思路是采用粗粒度的缓存一致性，比如在GPU本地以page大小的粒度进行跟踪，如果缓存未命中且已知该位置对应的page对GPU来说是私有的或只读的，就不需要访问全局的目录，而是可以从高带宽的总线从内存直接访问。CPU-GPU系统中缓存一致性的一个低复杂度解决方案我们可以采用选择性的GPU缓存，任何被映射到CPU内存的数据都不会缓存在GPU中，任何来自GPU内存的数据，如果目前被缓存在CPU中，也不会被缓存在GPU中。这个简单的策略可以很好地实现缓存一致性。为了实现这种方法，GPU维护一个粗粒度的目录，维护当前由CPU缓存的数据，当CPU访问GPU内存中的一个缓存块时，这个缓存块所在的区域就被插入到目录中，目录中所有的位置都不会被缓存到GPU中。不过这种方法也有坏处，即任何被CPU缓存的数据都必须从CPU中取回，为了解决这个问题需要进行进一步的优化，如GPU对CPU的请求进行合并、在GPU上加入一个特殊的CPU侧的缓存等。" }, { "title": "《内存一致性与缓存一致性》笔记（八）：目录一致性协议", "url": "/posts/MCCC8/", "categories": "读书笔记, 内存一致性与缓存一致性", "tags": "", "date": "2022-07-22 20:10:00 +0000", "snippet": "我们在上一节中注意到，总线嗅探协议的扩展性较差。我们接下来介绍目录协议，不需要可以保证全局顺序的广播网络。和之前一样，我们先从MSI开始介绍简单的目录协议，然后增加功能并进行优化，进一步讨论更复杂的协议，介绍如何表示目录状态以及如何实现目录协议。目录协议的关键是建立一个目录，维护每个缓存块的一致性状态，跟踪哪些缓存拥有缓存块以及处于什么状态。需要发出一致性请求的缓存控制器直接发送给目录（单播...", "content": "我们在上一节中注意到，总线嗅探协议的扩展性较差。我们接下来介绍目录协议，不需要可以保证全局顺序的广播网络。和之前一样，我们先从MSI开始介绍简单的目录协议，然后增加功能并进行优化，进一步讨论更复杂的协议，介绍如何表示目录状态以及如何实现目录协议。目录协议的关键是建立一个目录，维护每个缓存块的一致性状态，跟踪哪些缓存拥有缓存块以及处于什么状态。需要发出一致性请求的缓存控制器直接发送给目录（单播），目录确定接下来要采取的行动，例如目录状态可能表示所请求的块在C2的缓存，因此请求应该被转发给C2（比如可以使用一个新的Fwd-GetS请求），C2的缓存控制器收到这个转发的请求时，向请求者发送响应。这一过程通常涉及两个步骤（一个单播请求和一个单播响应）或三个步骤（一个单播请求，K个转发请求和K个响应），有的协议甚至涉及第四步，相比之下总线嗅探协议只会有两个步骤（即广播请求和单播响应）。目录协议同样需要确定事务顺序，在大多数目录协议中，一致性事务是在目录中排序的。如果有两个请求先后到达，第二个请求有可能在第一个请求之后立即被处理，或者停顿，或者被否定确认（即NACK），最后一种情况需要妥善处理活锁的问题。然而，相比总线嗅探协议，目录协议在目录中对事务进行排序，没有一个全局的顺序。我们可以发现目录协议实现了更好的可扩展性，但代价是一个层次的间接性，会增加一些事务的延迟。最简单的目录一致性协议系统和之前一样，我们从最简单的模型开始讨论。下图展示了我们现在讨论的系统模型，注意和之前的模型不同，这里互联网络的拓扑结构是不确定的，可以是任何形式的拓扑结构，不过需要执行点对点的排序，也就是说如果A对B发送了两条消息，这两条消息是需要顺序的，这可以降低协议设计的复杂性。同时，我们在LLC的旁边增加了一个目录，并把内存控制器改名叫目录控制器。我们现在假设一个最简单的模型，对内存中的每个块都有相应的目录条目，之后会讨论更实用的目录组织方式。协议规范我们仍然从MSI协议开始。目录状态包括一致性状态、缓存块所有者（M状态）以及共享者（S状态），一个简单的目录条目如下图所示。N节点系统中的目录条目下图中展示了缓存控制器发出一致性请求的过程，我们仍然使用以缓存为中心的符号来表示目录的状态。注意缓存控制器不能自己驱逐一个共享块，而是需要发出一个PutS请求。目录协议中的状态变化避免死锁在这种协议中，一个消息的接收可以导致一致性控制器发送另一个消息，且消息需要占用一些资源，我们需要注意避免可能产生的死锁。下图说明了一种死锁产生的情况以及可能的解决方案，包括为每一类消息使用单独的网络，可以是物理上分离的或者逻辑上分离的。目录协议中的死锁本节中的目录协议使用三个网络来避免死锁，分别为请求消息（GetS、GetM、PutM）、转发请求消息（Fwd-GetS、Fwd-GetM、Invalidation）和响应消息（Data、Inv-Ack）的网络。协议细节下表介绍了详细的协议细节，包括过渡状态。缓存控制器视角下的协议细节内存控制器视角下的协议细节具体的状态变化的细节可以参考原书。这个协议相对来说比较简单，牺牲了一些性能。最重要的简化是协议在某些情况下会停顿，可以通过添加更多的过渡状态来提升性能，其次第二个简化是目录发送数据（和AckCount）来响应缓存将一个缓存块的状态从S升级到M的请求，但其实缓存中已经有数据了，不再需要发送数据，我们可以定义一种新的消息类型。添加Exclusive状态处于E状态的缓存是该缓存块的所有者，负责对请求做出响应，不过需要注意E状态缓存块的驱逐需要额外的PutE请求，把所有者转交给目录。当然E状态缓存块也可以不是所有者，但是会增加协议的复杂性。协议规范下图展示了MESI协议中的状态变化，粗线突出了与MSI协议的区别。MESI协议中的状态变化协议细节下表介绍了详细的协议细节，包括过渡状态。缓存控制器视角下的协议细节内存控制器视角下的协议细节表示目录状态之前我们总是假设存在一个完整的目录，维护了每个缓存块的状态，但是这违背了可扩展性，需要占用大量的存储空间，因而我们需要有可扩展的方案来维护目录状态。在C个节点的系统中维护目录状态粗略（Coarse）的目录减少目录占用空间的一个办法是保守地维护一个粗略的共享者列表，比如用一位来表示K个缓存，表明这K个缓存中有一个或多个缓存可能有状态为S的缓存块，一个GetM请求会往这K个缓存都发送Invalidation消息。好处在于减少了一个目录条目的空间，代价则是可能会增加网络带宽，发送一些无效的消息。有限指针式（Limited Pointer）目录研究表明许多缓存块的共享者通常为0或者只有一个共享者。在这种设计中，我们通过i（i小于C）个条目来进行优化，并利用一些额外的机制来处理i+1个或更多共享者的情况。目录组织形式逻辑上来说，目录会包含每个缓存块的条目。许多传统的基于目录的系统，目录控制器与内存控制器集成在一起，通过增加内存来放置目录，直接实现了这种逻辑抽象。然而在现代的多核处理器和LLC中，原来设计会导致延迟和功耗很大，且当大多数缓存块都没有被缓存时，目录状态占用的空间也很大。我们可以采用缓存的思想，实现一个目录缓存。目录缓存对一致性协议的功能没有影响，只是为了降低平均目录访问延迟。关键的设计问题是处理目录缓存缺失的情况下，也就是当一致性请求到达目录但目录条目不在缓存中的情况。下表总结了一些设计方案。目录缓存的设计DRAM支持的目录缓存最简单的做法是把完整的目录保存在DRAM中，并使用单独的目录缓存结构来减少延迟，如果缓存未命中则访问DRAM中的目录。这种设计有一些缺点，首先需要大量DRAM来保存目录（包括没有被缓存的块），其次目录缓存和LLC解耦，可能LLC命中但是目录缓存未命中，最后目录缓存替换必须将目录条目写回DRAM，延迟和功率都高。包容性（Inclusive）目录缓存我们实际上只需要缓存LLC上缓存块的目录状态即可，如果一个目录缓存包括了片上缓存所有块的超级，我们将其称为包容性目录缓存，因而也不需要在DRAM中存储完整的目录了。接下来讨论两种包容性目录缓存设计。嵌入在包容性LLC中的包容性目录缓存最简单的目录缓存设计依赖于LLC，与上层缓存保持包容，也就是说如果一个块在上层缓存中，那么也必须出现在下层缓存中。在这种设计中，只需要在每个缓存块中额外增加几位表示目录状态即可。如果一个一致性请求发送到LLC上的目录控制器，而LLC未命中，那么目录控制器就可以确定该缓存块没有被缓存，在所有L1缓存中都处于I状态。不过，这种设计依然有几个重要的缺点。首先，在替换LLC中的缓存块时，通常需要发送特殊的召回请求，使L1缓存中的缓存块失效；更重要的是，LLC的包容性使其需要存储上层缓存中数据的冗余副本，在多核处理器中非常占用空间。独立式包容性目录缓存目录缓存也可以不依赖于LLC，而是一个独立的结构，只是在逻辑上于目录控制器关联，而非嵌入到LLC内。为了使目录缓存保持包容性，必须包含所有L1缓存中的目录条目。这种设计更加灵活，不需要包含LLC的内容，但相对来说实现比较复杂。最明显的是，这种设计需要一个组相联度较高的目录缓存，例如考虑一个有C个内核的处理器，每个内核都有一个K路组相联的L1缓存，那么目录缓存必须是C*K路组相联的。下图展示了K=2时的情况。独立式包容性目录缓存结构性能和可扩展性优化分布式目录分布式目录多处理器系统模型上图说明了有N个节点的多处理器系统模型，内存地址对节点的分配通常是静态的。这种设计可以提供更高的带宽，且对一致性协议没有影响。在现代大型LLC和目录缓存的处理器中，分配目录的方法在逻辑上仍然采用这种方式，分配LLC和目录缓存。不停顿的目录协议我们之前在讨论一致性协议时，有很多情况都会导致停顿，但我们可以修改一些协议细节，使一致性控制器跟踪正在处理的额外消息，添加一些额外状态来减少停顿。原文中给出了一种之前MSI协议细节的变体，限于篇幅这里就不贴出了。没有点对点排序的互联网络我们之前假设了互联网络中点到点的消息是顺序的，如果没有这个假设，可能会出现竞争，例如下面的例子中展示了如果没有点对点排序可能会出现死锁。没有点对点排序可能会导致死锁" }, { "title": "《内存一致性与缓存一致性》笔记（七）：总线嗅探一致性协议", "url": "/posts/MCCC7/", "categories": "读书笔记, 内存一致性与缓存一致性", "tags": "", "date": "2022-07-21 21:20:00 +0000", "snippet": "在这一节中我们进一步深入讨论总线嗅探的缓存一致性协议，首先从MSI开始，然后进一步讨论更复杂的协议。总线嗅探基于这样一个想法，所有的一致性控制器都以相同的顺序观察到总线上的一致性请求来保持一致性，因此我们对一致性请求的顺序到达有一定要求。我们考虑下面图中的例子，如果一致性请求的顺序没有保证，可能会出现缓存不一致的情况。总线嗅探一致性协议的简单例子传统的嗅探协议在所有缓存块中都保持请求的全局顺...", "content": "在这一节中我们进一步深入讨论总线嗅探的缓存一致性协议，首先从MSI开始，然后进一步讨论更复杂的协议。总线嗅探基于这样一个想法，所有的一致性控制器都以相同的顺序观察到总线上的一致性请求来保持一致性，因此我们对一致性请求的顺序到达有一定要求。我们考虑下面图中的例子，如果一致性请求的顺序没有保证，可能会出现缓存不一致的情况。总线嗅探一致性协议的简单例子传统的嗅探协议在所有缓存块中都保持请求的全局顺序，因而可以很容易实现需要全局顺序的内存模型，如SC和TSO，但也可以看下面的例子，C1和C2观察到的GetM和GetS请求顺序是不同的，违反了SC和TSO内存模型。每个缓存块都保持总线上的一致性请求顺序，但不保持全局顺序，违反了SC和TSO内存模型，注意方括号表示该内存地址中存放的值最简单的总线嗅探一致性协议我们讨论一个最简单的、没有优化的协议，并描述在两个不同的系统模型上的实现，前者说明了实现的基本方法，后者更复杂一些，表明即使是相对简单的性能改进的方法也会影响协议实现的复杂性。协议规范有三种稳定状态，即M、S和I，也叫MSI协议。下面两张图展示了缓存和内存控制器的稳定状态之间转换的状态机。MSI协议状态机简单的总线嗅探系统模型：原子请求与原子事务简单的总线嗅探系统这个系统实现了两个原子属性：原子请求和原子事务，前者表示一致性请求在发出的同一个周期内就被排序，后者表示一致性事务也是原子性的，对同一个块的后续的请求会在第一个事务完成之后才会出现在总线上。下表列出了该系统模型的协议细节，和之前的状态机不同的是，这里我们加入了两个过渡状态。缓存控制器视角下的协议细节内存控制器视角下的协议细节 测验问题6：在MSI总线嗅探协议中，一个缓存行只能处于三种一致性状态中的一种。 答：错误。即使对上面讨论的最简单的系统模型，也包含了过渡状态，不止三个状态。这个系统的原子性属性以两种方式简化了缓存缺失的处理，首先原子请求属性确保了当控制器想提升一个区块的状态时，可以直接发出一个请求，而不需要考虑另一个控制器的请求会排在前面，因此可以直接进入过渡状态，同时原子事务属性保证了当前事务完成之前不会发生对同一个块的后续请求。同时也可以简化缓存块驱逐的处理方式，例如原子请求属性简化了缓存控制器的设计，例如我们不需要考虑PutM请求发出之后但是再被总线接收之前，其他内核发出GetM请求的情况，原子事务属性也简化了内存控制器的设计。下图展示了一个例子来说明该系统模型。简单总线嗅探系统模型的例子复杂一些的总线嗅探系统模型：非原子请求与原子事务在这种模型中，我们允许非原子请求，通常是由于我们在缓存控制器和总线之间插入了一个消息队列，但仍然保留了原子事务属性。下表介绍了详细的协议细节，包括过渡状态，注意这里过渡状态的数量更多。缓存控制器视角下的协议细节内存控制器视角下的协议细节特别需要注意的是M到I的状态变化，我们额外加入了一种NoData消息，表示向内存控制器表明这个消息来自一个非所有者，意思就是该内核已经不是缓存块的所有者了，并使内存控制器退出其过渡状态，内存控制器也添加了一个额外的状态M_D，方便表示内存控制器收到NoData后应该回到哪个稳定状态。下图展示了一个例子来说明该系统模型。复杂一些的总线嗅探系统模型的例子添加Exclusive状态一个非常常用的优化是增加E状态，如果一个缓存有一个处于E状态的块，那么这个块是有效的、只读的、干净的、独占的、拥有的，缓存控制器可以自己把E状态变为M状态，不需要发出一致性请求。这种协议在首先读取一个块然后再写入的情况下有优势，在这种情况中，E状态可以减少一半的一致性事务。进入E状态我们必须首先理解GetS的发出者如何确定没有其他内核也share了该缓存块，保证进入状态E是安全的。一种方法是在总线上增加一个或门连接的sharer信号，如果GetS请求发现这个信号为1，那么就将缓存块状态改为S，否则进入E状态；另一种方法是在LLC中保持额外的状态，在LLC中区分I和S状态，但是准确维护S状态也是一个挑战，LLC必须检测最后一个sharer什么时候放弃所有权，要求缓存控制器在驱逐S状态区块时发出PutS消息，且内存控制器需要维护sharer的数量，一个更简单但不太完整的替代方案是允许LLC保守地跟踪sharer，放弃了一些使用E状态的机会，但不需要额外的PutS事务。在接下来的讨论中我们选择了最容易实现的方案，在LLC上保持一个保守的S状态，避免出现在总线上实现或门信号相关的工程问题，并且不需要PutS事务。协议规范与细节下面两张图展示了缓存和内存控制器的稳定状态之间转换的状态机。MESI协议状态机下表说明了MESI协议的实现细节，包括了过渡状态。注意我们仍然假设了原子事务属性，但不要求原子请求属性。缓存控制器视角下的协议细节内存控制器视角下的协议细节下面展示了一个例子来说明MESI协议。MESI协议系统模型的例子非原子总线我们之前讨论的实现都是基于原子事务属性的，也就是说所有事务都由不可分割的请求与响应组成，这种原子性大大简化了协议实现的设计，但牺牲了性能。下图直观地展示了总线是否保证原子性会影响性能，注意到第三种情况中响应不一定是顺序的，这种情况响应的数据中需要带有请求或请求者的身份信息。总线的原子性乱序事务总线上的MSI协议乱序事务总线的系统模型我们现在在上图中的系统模型中实现MSI协议，下面两张表说明了协议的实现细节。注意事务的排序是基于请求被放入总线的时间，而非到达请求者的时间。我们需要实现一些原子总线上不可能出现的状态变化，例如一个缓存块可以在状态IS_D中收到Other-GetS请求，此时不需要做任何事情。但可能还有一些其他更复杂的状态转换，例如当缓存块处于状态IM_D且观察到一个Other-GetS请求时，该缓存块实际上处于M状态，是所有者但没有数据，必须对GetS请求做出响应，最简单的解决方案就是暂时不处理Other-GetS，直到该缓存从内存中得到数据。缓存控制器视角下的协议细节内存控制器视角下的协议细节对于其他复杂的状态转换，我们也采用了停顿的策略，但可能会引起一些问题：首先是牺牲了性能；其次可能会引起死锁，我们需要避免出现链式的停顿，在这一节讨论的协议中不会出现；最后一个问题是请求者可能会在处理自己的请求之前就收到了请求的响应，可以参考下面的例子。响应在请求之前到达的例子该实现可以进一步优化，具体的细节可以参考原书。总线互联网络的优化我们进一步讨论另外两种可能的系统模型，进一步提升性能。第一种模型中，我们可以使用独立的数据响应非总线网络，一致性请求的响应没有必要进行排序，也没有必要广播，可以使用独立的网络进行传递，更容易实现、吞吐量高、且延迟低。第二种模型中，我们使用一个用于一致性请求的逻辑总线，有两种方法。首先是其他支持物理上的全局顺序的拓扑结构，如树形拓扑，一致性控制器位于叶节点，所有请求单播到树的根部，然后继续向下广播；其次也可以使用逻辑全局顺序，关键是在逻辑时间上对请求进行排序，需要确保每个请求都有一个不同的逻辑时间、一致性控制器按照逻辑时间顺序处理请求、且没有位于逻辑时间T的请求会在控制器已经过了逻辑时间T之后到达。 测验问题7：总线嗅探缓存一致性协议要求内核必须在总线上通信。 答：错误。总线嗅探需要一个可以保持全局顺序的广播网络，但是不需要物理总线也可以实现。" }, { "title": "CONNECT AXI Chisel Wrapper Documentation (Draft)", "url": "/posts/CONNECT-AXI-CHISEL-DOCS/", "categories": "CALCM, CONNECT", "tags": "", "date": "2022-07-19 21:52:00 +0000", "snippet": "IntroductionThe AXI4 wrapper is built over the CONNECT NoC, which is a network generator targeting FPGA. The wrapper supports the following protocols. AXI4 AXI4-Lite AXI4-Stream ...", "content": "IntroductionThe AXI4 wrapper is built over the CONNECT NoC, which is a network generator targeting FPGA. The wrapper supports the following protocols. AXI4 AXI4-Lite AXI4-Stream Simple packet with valid-ready interface Getting StartedBuild The Chisel WrapperClone the wrapper repository and build. PROTOCOL can be AXI4, AXI4-Lite, AXI4-Stream or Simple. Note that this repository already contains a sample network with 4 send ports and 4 receive ports.$ git clone https://github.com/shili2017/CONNECT-AXI.git$ cd CONNECT-AXI$ git submodule update --init --recursive$ make PROTOCOL=&lt;PROTOCOL&gt;Before using the network, routing tables are required to be created manually, but can also be generated by CONNECT. For example, for the given sample network, create the following four files. double_ring_4RTs_3VCs_4BD_34DW_SepIFRoundRobinAlloc_routing_0.hex 0 1 1 2 double_ring_4RTs_3VCs_4BD_34DW_SepIFRoundRobinAlloc_routing_1.hex 2 0 1 1 double_ring_4RTs_3VCs_4BD_34DW_SepIFRoundRobinAlloc_routing_2.hex 2 2 0 1 double_ring_4RTs_3VCs_4BD_34DW_SepIFRoundRobinAlloc_routing_3.hex 1 1 1 0 Then run the regression test for all the protocols.$ make testBuild The Network (Optional)Clone the CONNECT repository and build a network.$ git clone https://github.com/crossroadsfpga/connect.git$ cd connect$ python gen_network.py -t double_ring -w 38 -n 4 -v 2 -d 4 --gen_rtlThe network will be generated and put in the build directory. To change (a) flit width or (b) number of virtual channels or (c) flit buffer depth, re-generate the network.$ python gen_network.py -t double_ring -w &lt;flit_width&gt; -n 4 -v &lt;num_vcs&gt; -d &lt;flit_buffer_depth&gt; --gen_rtlTo put the network in the chisel wrapper, remove all redundant files and test bench files, and$ cat *.v &gt; network.vThen copy the network into src/main/resources/vsrc and copy the routing tables with .hex suffix to the working directory. Also emember to change the corresponding parameters in the chisel wrapper.ConfigsTo customize the wrapper, modify the configurations in Config.scala.CONNECT Network ConfigThese parameters need to be consistent with the generated CONNECT NoC. AXI4[-Lite] protocol requires at least 3 virtual channels to avoid deadlocks, but there’s no limitations for other protocols.class ConnectConfig extends Config((site, here, up) =&gt; { case NUM_USER_SEND_PORTS =&gt; 4 case NUM_USER_RECV_PORTS =&gt; 4 case NUM_VCS =&gt; 3 case REAL_FLIT_DATA_WIDTH =&gt; 34 case FLIT_BUFFER_DEPTH =&gt; 4 // ... })Protocol &amp; Library ConfigThese parameters can be customized by users.class AXI4WrapperConfig extends Config((site, here, up) =&gt; { case PROTOCOL =&gt; \"AXI4\" case NUM_MASTER_DEVICES =&gt; site(NUM_USER_SEND_PORTS) / 2 case NUM_SLAVE_DEVICES =&gt; site(NUM_USER_RECV_PORTS) / 2 case WRITE_INTERLEAVE =&gt; true case AXI4_MAX_BURST_LEN =&gt; 16 })// ...class LibraryConfig extends Config((site, here, up) =&gt; { case USE_FIFO_IP =&gt; true case ALTERA_MF_V =&gt; \"/afs/ece.cmu.edu/support/altera/release/pro-19.3.0.222/quartus/eda/sim_lib/altera_mf.v\" })Requiments on NetworkThe Noc in the wrapper can be replaced by another in the future, and the wrapper requires the network to Support virtual channels for AXI4 and AXI4-Lite (at least 3 VCs) Maintain in-order transfer between sources and sinks Be credit-based for flow control Is possible to convert to a valid-ready flit interface However, the network can still Arbitrarily interleave flits as long as they are in-order Be in any kind of topology Support any width of flit data width Programmer’s ManualThe overall design is shown in the following diagrams.AXI4 wrapper block diagram (master device side)AXI4-Stream wrapper block diagramAXI4[-Stream] BridgeAn AXI4 bridge is used to convert the protocol channels into coressponding packets. For example, for an AXI4 master bridge, it converts 5 AXI4[-Lite] channels into packets in 3 virtual channels (aw/ar channel to a_packet in VC 2, w channel to w_packet in VC 1, and b/r channel to br_packet in VC 0). For an AXI4-Stream master bridge, it converts the t channel into t_packet and put into VC 0.AXI4 bridge is further split into 2 stages. The first stage converts 5 channels into 5 packet outputs, and the second stage contains an arbiter, as shown below. It supports reading and writing at the same time, which are handled by two seprate state machines. The arbiter is set to be round-robin at default.Two stages of AXI4 master bridgeThe original AXI4 protocol doesn’t support transfer-level write interleaving. To support write packet interleaving, set WRITE_INTERLEAVE to true and set a proper AXI4_MAX_BURST_LEN. A write buffer in the slave bridge between stage 1 and stage 2 will be created to handle incoming interleaving write packets and transfer the packets according to current destination.Write buffer between stage 1 and 2 to store interleaving write packetsParameters of AXI4[-Stream] protocol can be adjusted in AXI4.scala. Each AXI4[-Stream] request and response is encoded in a single AXI4[-Stream] packet, or the packet can be decoded to be an AXI4[-Stream] request or response, related functions in AXI4Packet.scala. For example, the following code snippet shows the encoding and decoding of AXI4 or AXI4-Lite write channel packet.// Encodeobject AXI4ChannelW2PacketData { def apply[C &lt;: AXI4LiteChannelW](w: C): UInt = { if (w.getClass == classOf[AXI4ChannelW]) { val w_ = w.asInstanceOf[AXI4ChannelW] Cat( w_.strb, w_.data, w_.last.asUInt, AXI4ChannelID.W ) } else { Cat( w.strb, w.data, AXI4ChannelID.W ) } }}// Decodeobject Packet2AXI4ChannelW { def apply(packet: UInt)(implicit p: Parameters): AXI4LiteChannelW = { assert(packet.getWidth == p(PACKET_WIDTH)) if (p(PROTOCOL) == \"AXI4\") { val w = Wire(new AXI4ChannelW) w.strb := packet( 3 + AXI4Parameters.AXI4DataWidth + AXI4Parameters.AXI4DataWidth / 8, 4 + AXI4Parameters.AXI4DataWidth ) w.data := packet(3 + AXI4Parameters.AXI4DataWidth, 4) w.last := packet(3).asBool w } else { val w = Wire(new AXI4LiteChannelW) w.strb := packet( 2 + AXI4Parameters.AXI4DataWidth + AXI4Parameters.AXI4DataWidth / 8, 3 + AXI4Parameters.AXI4DataWidth ) w.data := packet(2 + AXI4Parameters.AXI4DataWidth, 3) w } }}Serializer &amp; DeserializerUsually the size of AXI4 protocol packets or customized user packets is larger than flit data width. In such circumstance, we need a serializer and deserializer to convert between large packets and small flits. A serializer cuts a packet into small flits with the user-defined size and send to the network, while a deserializer receives flits from the network and reassembles to packets and send to the user device.In the case that multiple master devices send data to a single slave device, flits that arrives at the deserializer may interleave when virtual link is disabled in CONNECT network or in other NoCs. As the flits are always transferred in-order, and we know the number of send ports, we simply allocate a table with one row per source ID and fill the table with successive flits until the tail flit arrives, at which the flits are reassembled into a packet. If packets from multiple sources are ready, a priority encoder is implemented to decide which packet to send to output.If Intel FIFO IP exists, user may set USE_FIFO_IP to true and set the corresponding library path. A dcfifo will be created in the serializer and deserializer, acting as the boundary of slow clock domain for FPGA logics and fast clock domain for hard NoC.Flow Control LayerCredit-based flow control is supported in CONNECT and the wrapper. To support flow control, a synchronous FIFO, e.g., SCFIFO or chisel queue, is included as a buffer in each virtual channel between the user device and the network. On the sender side, we need to maintain a credit counter. Each time we send a flit, we need to decrement the counter, and each time we receive a credit from the network, we increment the counter. On the receiver side, each time we dequeue a flit from the FIFO to the user device, we send a credit to the network. After the FIFO, we implement a N-to-1 hub on the sender side and a 1-to-N hub on the receiver side. Finally a send/recv port interface converter is implemented to convert valid-ready interface into BSV-style interface.Flow control layer on the sender side with 3 virtual channels" }, { "title": "CONNECT Note (7) - So... why Chisel?", "url": "/posts/CONNECT7/", "categories": "CALCM, CONNECT", "tags": "", "date": "2022-07-12 07:43:00 +0000", "snippet": "We define our AXI4 protocol interface as follows.trait AXI4Id extends Bundle with AXI4Parameters { val id = UInt(AXI4IdWidth.W)}class AXI4LiteChannelA extends Bundle with AXI4Parameters { val add...", "content": "We define our AXI4 protocol interface as follows.trait AXI4Id extends Bundle with AXI4Parameters { val id = UInt(AXI4IdWidth.W)}class AXI4LiteChannelA extends Bundle with AXI4Parameters { val addr = UInt(AXI4AddrWidth.W) val prot = UInt(3.W)}class AXI4ChannelA extends AXI4LiteChannelA with AXI4Id { val len = UInt(8.W) val size = UInt(3.W) val burst = UInt(2.W) val lock = Bool() val cache = UInt(4.W) val qos = UInt(4.W) val region = UInt(4.W)}// ...class AXI4LiteIO extends Bundle { val aw = Decoupled(new AXI4LiteChannelA) val w = Decoupled(new AXI4LiteChannelW) val b = Flipped(Decoupled(new AXI4LiteChannelB)) val ar = Decoupled(new AXI4LiteChannelA) val r = Flipped(Decoupled(new AXI4LiteChannelR))}class AXI4IO extends AXI4LiteIO { override val aw = Decoupled(new AXI4ChannelA) override val w = Decoupled(new AXI4ChannelW) override val b = Flipped(Decoupled(new AXI4ChannelB)) override val ar = Decoupled(new AXI4ChannelA) override val r = Flipped(Decoupled(new AXI4ChannelR))}OOP features including templates, inheritance and polymorphism make things much easier for RTL programmers. For example, in terms of AXI4 bridge design, almost every line of code can be reused for AXI4-Lite protocol. The following code snippet shows one of the FSMs in the bridge.// FSM to handle AXI master device readswitch(r_state) { is(r_addr) { when(io.axi.ar.fire) { r_state := r_data } } is(r_data) { if (io.axi.r.bits.getClass == classOf[AXI4ChannelR]) { when(io.axi.r.fire &amp;&amp; io.axi.r.bits.asInstanceOf[AXI4ChannelR].last) { r_state := r_addr } } else { when(io.axi.r.fire) { r_state := r_addr } } }}We also need to redefine the method to pack and unpack the data as follows.// Pack the data in aw/ar channelobject AXI4ChannelA2PacketData { def apply[C &lt;: AXI4LiteChannelA](a: C, is_w: Bool): UInt = { if (a.getClass == classOf[AXI4ChannelA]) { val a_ = a.asInstanceOf[AXI4ChannelA] Cat( a_.id, a_.addr, a_.region, a_.qos, a_.prot, a_.cache, a_.lock.asUInt, a_.burst, a_.size, a_.len, Mux(is_w, AXI4ChannelID.AW, AXI4ChannelID.AR) ) } else { Cat( a.addr, a.prot, Mux(is_w, AXI4ChannelID.AW, AXI4ChannelID.AR) ) } }}// Unpack the data in aw/ar channelobject Packet2AXI4ChannelA { def apply[B &lt;: AXI4LiteIO](bus_io: B)(packet: UInt): AXI4LiteChannelA = { assert(packet.getWidth == AXI4PacketWidth(bus_io)) if (bus_io.getClass == classOf[AXI4IO]) { val a = Wire(new AXI4ChannelA) a.id := packet( 31 + AXI4Parameters.AXI4AddrWidth + AXI4Parameters.AXI4IdWidth, 32 + AXI4Parameters.AXI4AddrWidth ) a.addr := packet(31 + AXI4Parameters.AXI4AddrWidth, 32) a.region := packet(31, 28) a.qos := packet(27, 24) a.prot := packet(23, 21) a.cache := packet(20, 17) a.lock := packet(16).asBool a.burst := packet(15, 14) a.size := packet(13, 11) a.len := packet(10, 3) a } else { val a = Wire(new AXI4LiteChannelA) a.addr := packet(5 + AXI4Parameters.AXI4AddrWidth, 6) a.prot := packet(5, 3) a } }}" }, { "title": "《内存一致性与缓存一致性》笔记（六）：缓存一致性协议", "url": "/posts/MCCC6/", "categories": "读书笔记, 内存一致性与缓存一致性", "tags": "", "date": "2022-07-03 22:44:00 +0000", "snippet": "我们从这一节开始讨论缓存一致性协议，包括协议的工作原理、如何定义缓存一致性协议、协议的设计空间等。我们先前已经讨论过缓存一致性协议的目标，具体包括： 单写多读（single-writer-multiple-reader, SWMR）不变性：对任何内存地址A，在任何时间点上只存在一个可以向A写入的内核，但可以存在多个从A读取的内核。 数据-值（data-value）不变...", "content": "我们从这一节开始讨论缓存一致性协议，包括协议的工作原理、如何定义缓存一致性协议、协议的设计空间等。我们先前已经讨论过缓存一致性协议的目标，具体包括： 单写多读（single-writer-multiple-reader, SWMR）不变性：对任何内存地址A，在任何时间点上只存在一个可以向A写入的内核，但可以存在多个从A读取的内核。 数据-值（data-value）不变性：一个内存位置在一个时间片开始时的值与该内存位置在其上一个时间片结束时的值相同。 为了实现这些性质，我们把每个存储结构（包括私有缓存、LLC、内存）与一致性控制器连接起来，构成一个分布式系统，控制器之间交换信息。下面两张图展示了缓存控制器与LLC/内存控制器的功能。缓存控制器LLC/内存控制器定义缓存一致性协议我们通过规定控制器的行为来定义一致性协议，通常用表格的形式来说明，如下图所示。表格形式的一致性定义最简单的一致性协议我们用图和表格来展示这种只有两个状态的一致性协议，包括I状态和V状态。缓存控制器中不同状态之间的转换缓存控制器定义内存控制器定义协议设计空间协议设计者需要定义每种控制器中的状态、事务、事件以及状态转换。稳定状态和事务的选择通常和一致性协议的其他部分无关，但事件、状态转换以及过渡状态高度依赖于一致性协议本身。状态在单处理器系统中，一个缓存行可以只有一个性质，即validity，如果采用写回策略还有一个性质，即dirtiness，但是在多处理器系统中可能会有其他状态，如exclusivity和ownership。我们现在给出如下定义： Validity：指缓存行包含了这个block的最新值，可以读取，但只有同时为exclusive的时候才可以写入。 Dirtiness：指缓存行包含了最新的值且与LLC/内存中的值不同，且缓存控制器最终用这个新的值更新LLC/内存，反义词为clean。 Exclusivity：指缓存行是系统中唯一的私有缓存副本。 Ownership：如果一个缓存控制器或内存控制器负责响应一个block的一致性请求，那么这个控制器就是该block的所有者。 稳定状态许多一致性协议使用MOESI模型中的一个子集，最基本的三个状态是MSI，下图说明了MOESI中每种状态具有哪些特性。MOESI状态过渡状态在复杂的协议中，我们可能会有几十个过渡状态，我们用XY_Z的形式来表示这些状态，如IV_D表示在Invalid状态中，准备进入Valid状态，且正在等待DataResp。LLC/内存中的状态有两种命名这些状态的方法： 以缓存为中心：一个缓存行在LLC和内存中的状态是该行在缓存中的状态的集合，例如，如果该行在所有缓存中状态都是I，那么在LLC/内存状态也是I，如果在一个或多个缓存中的状态是S，那么LLC/内存状态就是S，如果在一个缓存中状态为M，那么LLC/内存状态就是M。 以内存为中心：一个缓存行在LLC/内存中的状态对应于内存控制器对该行的权限（而不是缓存的权限），例如，如果该行在所有缓存中状态都是I，那么在LLC/内存控制器中的状态就是O（而不是以缓存为中心的方法中的I），如果在一个或多个缓存中的状态是S，那么LLC/内存的状态也是O，如果在一个缓存中状态为M或O，那么LLC/内存的状态就是I。 后面我们都使用以缓存为中心的命名方法来表示LLC/内存中的缓存行的状态。事务大多数协议都有一组类似的事务，下表列出了一组常见事务。常见事务常见的由内核向缓存控制器发出的请求" }, { "title": "《数据并行C++》笔记（六）：面向FPGA编程", "url": "/posts/DPCPP6/", "categories": "读书笔记, 数据并行C++", "tags": "", "date": "2022-06-28 21:15:00 +0000", "snippet": "基于kernel的编程一开始是作为访问GPU的一种方式而流行的，但也慢慢普及到其他类型的加速器上，包括FPGA。为了更好地利用FPGA进行加速，我们需要回答一些问题，如我们应该什么时候使用FPGA、程序的哪些部分可以用FPGA加速、以及应该如何编写FPGA可以运行的高性能代码。在SYCL标准之上，DPC++额外支持FPGA选择器和管道（pipe）。基本概念对于传统的基于ISA的处理器，芯片在...", "content": "基于kernel的编程一开始是作为访问GPU的一种方式而流行的，但也慢慢普及到其他类型的加速器上，包括FPGA。为了更好地利用FPGA进行加速，我们需要回答一些问题，如我们应该什么时候使用FPGA、程序的哪些部分可以用FPGA加速、以及应该如何编写FPGA可以运行的高性能代码。在SYCL标准之上，DPC++额外支持FPGA选择器和管道（pipe）。基本概念对于传统的基于ISA的处理器，芯片在每个时钟周期内执行程序的指令，硬件被不同的指令复用，如下图所示。基于ISA的架构空间架构则不同，整个程序是一个整体，硬件的不同区域实现程序中的不同指令，每条指令都有自己的专用硬件，如下图所示。基于空间的架构注意在这种结构中，我们不再需要取指、解码、程序计数器或寄存器堆，也不需要存储未来指令需要用到的数据，而是直接把一条指令的输出与下一条指令的输入连接起来，这也是为什么空间架构经常被称为数据流架构（dataflow architecture）。流水线并行上面的图中经常出现的问题是程序的空间实现和时钟频率的关系，如果关键路径过长，不仅时钟频率低，而且大多数硬件只在一小部分时间内做有用的工作。大多数编译器可以通过流水线来解决这种问题，在一些操作之间插入寄存器，如下图所示。流水线化的计算消耗面积的KernelFPGA上的多个kernelDPC++程序中的每个kernel都会生成流水线，消耗一部分FPGA资源，如上图所示。不同的kernel可以同时执行，如果一个kernel被阻塞（例如等待内存访问），其他kernel可以继续执行。什么时候使用FPGA大量工作可以利用FPGA流水线并行，可能流水线有数千级，我们把流水线级的平均利用率称为流水线占用率（occupancy），有多种方法可以在FPGA上尽可能往流水线中填充工作。FPGA最初的设计是为了执行高效的整数和位运算，可以有效实现任意数据类型，比如定制化一个33位的整数乘法器等，很容易应用于机器学习等领域。FPGA也提供了丰富的低延时接口，可以充分利用网络接口，而不经过操作系统，如下图所示。低延时IO串流FPGA上的存储系统也是高度定制的，有小块的片上存储器，且带宽很大，编译器可以基于这一点进行优化，如下图所示。编译器基于FPGA定制的内存系统进行优化在FPGA上运行程序在FPGA上运行kernel有两个步骤： 将源代码编译成二进制文件 在运行时选择需要的加速器 可以使用以下命令将kernel编译到FPGA硬件：dpcpp -fintelfpga my_source_code.cpp -Xshardware编译与运行时的流程如下图所示。在运行时自动对FPGA编程编译期在冗长的硬件编译之前还有大多数验证和优化过程FPGA仿真器在代码中使用INTEL::fpga_emulator_selector选择器，并使用以下命令编译：dpcpp -fintelfpga my_source_code.cppFPGA硬件编译提前完成上图中Full Compile和Hardware Profiling是SYCL术语中的提前编译（ahead-of-time compile），在编译阶段就把kernel编译为设备二进制文件，这是因为编译需要一定时间，且DPC++程序在部署的性能较弱的主机上同样可以执行。为FPGA编写Kernel利用FPGA并行性五级简单流水线，需要6个周期来处理一个元素的数据如果一次只处理一个元素，流水线大部分都是闲置的流水线被有效利用上面三张图很好地展示了如何高效利用FPGA流水线，我们接下来介绍如何充分利用流水线，主要研究ND-range kernel和循环。通过ND-range为流水线提供数据FPGA可以非常有效地使用ND-range来填充流水线，如上图所示，在每个时钟周期都有一个work-item进入流水线。只要我们可以将算法或程序结构化为独立的work-item，不需要经常通信或根本不需要通信，就可以使用ND-range。FPGA流水线上也可以实现数据依赖，可以实现work-item之间的细粒度通信（甚至不同work-grouop的work-item也可以），比如下面的例子以及下图。int state = 0;for (int i = 0; i &lt; size; i++) { state = generate_random_number(state); output[i] = state;}后向通信（传输数据到流水线的之前阶段）可以帮助实现有效的数据依赖通信后向通信能力是空间架构的关键，但如何编写这样的代码并不显而易见，两种常见方法可以实现这一点，分别是循环和ND-range kernel的kernel内管道，我们在此暂时只讨论循环。在下面的例子中，不同循环间有数据依赖关系，注意这里使用single_task而非parallel_for，因为在代码中已经有循环了。h.single_task([=]() {int state = seed;for (int i = 0; i &lt; size; i++) { state = generate_incremental_random_number(state); output[i] = state;}});从程序员的角度来说，每一次iteration应该是一个接一个执行的，但流水线化之后每一次iteration执行在时间上是重叠的，如下图所示。循环流水线化允许iteration在时间上重叠即使有数据依赖，仍然可以有效利用流水线，如下图所示。随机数生成器的流水线实现流水线同时处理多个iteration的不同阶段然而在实际算法中，往往不可能每一个时钟周期都启动一次新的iteration，可能是因为数据依赖可能需要多个时钟周期来计算。我们把每隔多少个时钟周期启动一次新的iteration的时钟周期称为initiation interval，即II，如下图所示。II=2的情况管道空间架构的一个重要概念是FIFO缓冲区，有两个重要性质： FIFO有隐含的控制信息，即empty和full信号 FIFO有存储能力，存在动态行为的情况下更容易提升性能（如控制访问内存的延迟） FIFO在DPC++中，我们通过管道来访问FIFO，FIFO可以让我们把问题分解为更小的部分，以模块化的方法来专注于开发和优化，并利用FPGA丰富的通信功能，如下图所示。管道简化了模块化设计和外围硬件的访问如下图所示，一般有四种类型的管道可以使用，我们主要介绍第一种。DPC++的四种管道类型// Create alias for pipe type so that consistent across usesusing my_pipe = pipe&lt;class some_pipe, int&gt;;// ND-range kernelQ.submit([&amp;](handler&amp; h) { auto A = accessor(B_in, h); h.parallel_for(count, [=](auto idx) { my_pipe::write( A[idx] ); }); });// Single_task kernelQ.submit([&amp;](handler&amp; h) { auto A = accessor(B_out, h); h.single_task([=]() { for (int i=0; i &lt; count; i++) { A[i] = my_pipe::read(); } }); });上面的例子展示了如何在ND-range的parallel_for和包含了一个循环的single_task两个kernel之间通信，如果kernel之间没有accessor或事件的依赖，DPC++可以同时执行两个kernel。下面的代码是管道类的模板类型定义，三个模板参数共同定义了管道类型。template &lt;typename name, typename dataT, size_t min_capacity = 0&gt;class pipe;管道有两种风格的接口：阻塞和非阻塞。前者等待操作成功，后者则立即返回，并设置一个布尔值表示操作是否成功。下面的代码是访问管道类成员函数的两种形式。// BlockingT read();void write( const T &amp;data );// Non-blockingT read( bool &amp;success_code );void write( const T &amp;data, bool &amp;success_code );优化访存访存延迟导致流水线停顿访存延迟可能导致流水线停顿，我们可以通过下面一些优化来提升性能，优化可以通过显式控制和修改代码，让编译器推断出我们想要的结构，常见的优化包括： 静态合并：编译器回尽量将一些访存请求合并，减少load或store单元数量、内存系统端口、仲裁网络的大小和复杂性等硬件。 内存访问方式：编译器会为访存创建load或store单元，这些单元会根据访问的内存（如片上存储器、DDR、HBM等）以及源代码推断出的访问模式（流式、动态合并等）来进行定制。 内存系统结构：片上或片外内存系统可以由编译器实现bank结构或其他优化。 " }, { "title": "《数据并行C++》笔记（五）：常见并行模式", "url": "/posts/DPCPP5/", "categories": "读书笔记, 数据并行C++", "tags": "", "date": "2022-06-28 00:40:00 +0000", "snippet": "在并行编程中有一些常见的模式反复出现，这些模式都是通用的，可以在任何级别的并行和任何设备上应用，但我们仍然要考虑这些模式的某些属性（比如可扩展性），可能需要在不同的设备上微调参数，或者直接采用不同的模式来提高性能。我们在本节中主要讨论以下一些问题： 哪些模式是最重要的？ 这些模式和不同设备的特性有什么关联？ DPC++函数和库已经支持了哪些模式？ ...", "content": "在并行编程中有一些常见的模式反复出现，这些模式都是通用的，可以在任何级别的并行和任何设备上应用，但我们仍然要考虑这些模式的某些属性（比如可扩展性），可能需要在不同的设备上微调参数，或者直接采用不同的模式来提高性能。我们在本节中主要讨论以下一些问题： 哪些模式是最重要的？ 这些模式和不同设备的特性有什么关联？ DPC++函数和库已经支持了哪些模式？ 如何实现这些模式？ 了解并行模式我们不讨论与并行类型有关的模式（如fork-join、branch-and-bound等），只关注对编写数据并行kernel最相关的算法模式。下图展示了不同模式的概述，包括主要用途、关键属性以及应用到设备上的能力。并行模式概述Map这是最简单的并行模式，如下图所示。不过需要注意的是，这种模式可能会忽略一些可以提高性能的优化（如数据重用、融合kernel等）。Map模式StencilStencil模式如下图所示。由于stencil很容易描述，但有效实现却很复杂，因而stencil也是DSL开发中最活跃的领域之一。Stencil模式ReductionReduction也是一种常见的并行模式，如求和或计算最小最大值等，下图展示了树型结构的实现方法，这是一种比较流行的实现，但也有其他实现方式。Reduction模式ScanScan使用一个二元可结合的运算符来计算广义的前缀和。Scan看起来是串行的，但仍然有潜在的并行机会（可能可扩展性较差），如下图所示。一个经验法是在产生数据的同一设备上执行扫描操作。Scan模式Pack和Unpack这两者和scan密切相关，通常在scan基础上实现。Pack模式根据一个条件丢弃一些元素，并将剩下元素pack到输出范围的连续位置，如下图所示。Pack模式Unpack模式则相反，输入范围的连续元素被unpack到输出范围的非连续位置，如下图所示。Unpack模式使用DPC++函数和库DPC++ Reduction库下面是一个说明如何使用reduction库的数据并行kernel的例子#include &lt;CL/sycl.hpp&gt;#include &lt;iostream&gt;#include &lt;numeric&gt;using namespace sycl;int main() { constexpr size_t N = 16; constexpr size_t B = 4; queue Q; int* data = malloc_shared&lt;int&gt;(N, Q); int* sum = malloc_shared&lt;int&gt;(1, Q); std::iota(data, data + N, 1); *sum = 0; Q.submit([&amp;](handler&amp; h) { h.parallel_for( nd_range&lt;1&gt;{N, B}, reduction(sum, plus&lt;&gt;()), [=](nd_item&lt;1&gt; it, auto&amp; sum) { int i = it.get_global_id(0); sum += data[i]; }); }).wait(); std::cout &lt;&lt; \"sum = \" &lt;&lt; *sum &lt;&lt; \"\\n\"; bool passed = (*sum == ((N * (N + 1)) / 2)); std::cout &lt;&lt; ((passed) ? \"SUCCESS\" : \"FAILURE\") &lt;&lt; \"\\n\"; free(sum, Q); free(data, Q); return (passed) ? 0 : 1;}下面是reduction类的函数原型，我们可以通过下面的函数来构造一个reduction对象。template &lt;typename T, typename BinaryOperation&gt;unspecified reduction(T* variable, BinaryOperation combiner);template &lt;typename T, typename BinaryOperation&gt;unspecified reduction(T* variable, T identity, BinaryOperation combiner);下面是reducer类的简化定义，对reduction暴露了有限的接口，避免我们通过任何可能不安全的方式来更新reduction变量。template &lt;typename T, typename BinaryOperation, /* implementation-defined */&gt;class reducer { // Combine partial result with reducer's value void combine(const T&amp; partial);};// Other operators are available for standard binary operationstemplate &lt;typename T&gt;auto&amp; operator +=(reducer&lt;T,plus::&lt;T&gt;&gt;&amp;, const T&amp;);用户也可以定义简单的reduction，如下面的例子展示了如何使用用户定义reduction来计算一个向量中的最小元素及其位置。#include &lt;CL/sycl.hpp&gt;#include &lt;iostream&gt;#include &lt;random&gt;using namespace sycl;template &lt;typename T, typename I&gt;struct pair { bool operator&lt;(const pair&amp; o) const { return val &lt;= o.val || (val == o.val &amp;&amp; idx &lt;= o.idx); } T val; I idx;};template &lt;typename T, typename I&gt;using minloc = minimum&lt;pair&lt;T, I&gt;&gt;;int main() { constexpr size_t N = 16; constexpr size_t L = 4; queue Q; float* data = malloc_shared&lt;float&gt;(N, Q); pair&lt;float, int&gt;* res = malloc_shared&lt;pair&lt;float, int&gt;&gt;(1, Q); std::generate(data, data + N, std::mt19937{}); pair&lt;float, int&gt; identity = { std::numeric_limits&lt;float&gt;::max(), std::numeric_limits&lt;int&gt;::min()}; *res = identity; auto red = sycl::reduction(res, identity, minloc&lt;float, int&gt;()); Q.submit([&amp;](handler&amp; h) { h.parallel_for(nd_range&lt;1&gt;{N, L}, red, [=](nd_item&lt;1&gt; item, auto&amp; res) { int i = item.get_global_id(0); pair&lt;float, int&gt; partial = {data[i], i}; res.combine(partial); }); }).wait(); std::cout &lt;&lt; \"minimum value = \" &lt;&lt; res-&gt;val &lt;&lt; \" at \" &lt;&lt; res-&gt;idx &lt;&lt; \"\\n\"; pair&lt;float, int&gt; gold = identity; for (int i = 0; i &lt; N; ++i) { if (data[i] &lt;= gold.val || (data[i] == gold.val &amp;&amp; i &lt; gold.idx)) { gold.val = data[i]; gold.idx = i; } } bool passed = (res-&gt;val == gold.val) &amp;&amp; (res-&gt;idx == gold.idx); std::cout &lt;&lt; ((passed) ? \"SUCCESS\" : \"FAILURE\") &lt;&lt; \"\\n\"; free(res, Q); free(data, Q); return (passed) ? 0 : 1;}oneAPI DPC++库C++ STL有几种算法与之前讨论的并行模式相对应，且从C++17开始，可以支持一个执行策略参数，表示这些算法应该串行还是并行执行。oneAPI DPC++库（oneDPL）借此提供了一种高效的并行编程方法，如果一个程序可以完全使用STL算法的功能来表达，那么oneDPL就有可能利用系统中的加速器，而无需编写DPC++ kernel代码。并行模式与C++17中的算法库并行模式的简单实现MapQ.parallel_for(N, [=](id&lt;1&gt; i) { output[i] = sqrt(input[i]); }).wait();Stencilbuffer&lt;float, 2&gt; input_buf(input.data(), alloc_range);buffer&lt;float, 2&gt; output_buf(output.data(), alloc_range);Q.submit([&amp;](handler&amp; h) { accessor input{ input_buf, h }; accessor output{ output_buf, h }; // Compute the average of each cell and its immediate neighbors h.parallel_for(stencil_range, [=](id&lt;2&gt; idx) { int i = idx[0] + 1; int j = idx[1] + 1; float self = input[i][j]; float north = input[i - 1][j]; float east = input[i][j + 1]; float south = input[i + 1][j]; float west = input[i][j - 1]; output[i][j] = (self + north + east + south + west) / 5.0f; });});需要注意的是，上面的代码只是最简单的实现，但我们可以利用work-group本地内存来减少内存访问，提高性能，如下所示。// Create SYCL buffers associated with input/outputbuffer&lt;float, 2&gt; input_buf(input.data(), alloc_range);buffer&lt;float, 2&gt; output_buf(output.data(), alloc_range);Q.submit([&amp;](handler&amp; h) { accessor input{ input_buf, h }; accessor output{ output_buf, h }; constexpr size_t B = 4; range&lt;2&gt; local_range(B, B); range&lt;2&gt; tile_size = local_range + range&lt;2&gt;(2, 2); // Includes boundary cells auto tile = local_accessor&lt;float, 2&gt;(tile_size, h); // Compute the average of each cell and its immediate neighbors h.parallel_for( nd_range&lt;2&gt;(stencil_range, local_range), [=](nd_item&lt;2&gt; it) { // Load this tile into work-group local memory id&lt;2&gt; lid = it.get_local_id(); range&lt;2&gt; lrange = it.get_local_range(); for (int ti = lid[0]; ti &lt; B + 2; ti += lrange[0]) { int gi = ti + B * it.get_group(0); for (int tj = lid[1]; tj &lt; B + 2; tj += lrange[1]) { int gj = tj + B * it.get_group(1); tile[ti][tj] = input[gi][gj]; } } it.barrier(access::fence_space::local_space); // Compute the stencil using values from local memory int gi = it.get_global_id(0) + 1; int gj = it.get_global_id(1) + 1; int ti = it.get_local_id(0) + 1; int tj = it.get_local_id(1) + 1; float self = tile[ti][tj]; float north = tile[ti - 1][tj]; float east = tile[ti][tj + 1]; float south = tile[ti + 1][tj]; float west = tile[ti][tj - 1]; output[gi][gj] = (self + north + east + south + west) / 5.0f; });});ReductionReduction的实现稍显复杂，需要利用work-item之间同步和通信的语言特性（如原子操作、work-groupo和sub-group函数、shuffle函数等），下面的代码是两种可能的实现方式，第一种是最简单的实现，对每个work-item使用基本的parallel_for和原子操作，第二种进行了简单的优化，分别使用ND-range parallel_for和work-group reduce函数。Q.parallel_for(N, [=](id&lt;1&gt; i) { ext::oneapi::atomic_ref&lt; int, memory_order::relaxed, memory_scope::system, access::address_space::global_space&gt;(*sum) += data[i]; }).wait();Q.parallel_for(nd_range&lt;1&gt;{N, B}, [=](nd_item&lt;1&gt; it) { int i = it.get_global_id(0); int group_sum = reduce_over_group(it.get_group(), data[i], plus&lt;&gt;()); if (it.get_local_id(0) == 0) { ext::oneapi::atomic_ref&lt; int, memory_order::relaxed, memory_scope::system, access::address_space::global_space&gt;(*sum) += group_sum; } }).wait();Scan实现并行scan需要对数据多次scan，每次scan之间都要进行同步。由于DPC++没有提供同步ND-range中所有work-item的机制，直接实现设备范围内的scan必须使用多个kernel来实现，并使用全局内存共享中间结果。下面的代码展示了使用三个kernel来实现scan，这三个kernel对应了之前介绍scan模式的图中的三个层次。// Create a temporary allocation that will only be used by the deviceint32_t* tmp = malloc_device&lt;int32_t&gt;(G, q);// Phase 1: Compute local scans over input blocksq.submit([&amp;](handler&amp; h) { auto local = local_accessor&lt;int32_t, 1&gt;(L, h); h.parallel_for(nd_range&lt;1&gt;(N, L), [=](nd_item&lt;1&gt; it) { int i = it.get_global_id(0); int li = it.get_local_id(0); // Copy input to local memory local[li] = input[i]; it.barrier(); // Perform inclusive scan in local memory for (int32_t d = 0; d &lt;= log2((float)L) - 1; ++d) { uint32_t stride = (1 &lt;&lt; d); int32_t update = (li &gt;= stride) ? local[li - stride] : 0; it.barrier(); local[li] += update; it.barrier(); } // Write the result for each item to the output buffer // Write the last result from this block to the temporary buffer output[i] = local[li]; if (li == it.get_local_range()[0] - 1) { tmp[it.get_group(0)] = local[li]; } }); }).wait();// Phase 2: Compute scan over partial resultsq.submit([&amp;](handler&amp; h) { auto local = local_accessor&lt;int32_t, 1&gt;(G, h); h.parallel_for(nd_range&lt;1&gt;(G, G), [=](nd_item&lt;1&gt; it) { int i = it.get_global_id(0); int li = it.get_local_id(0); // Copy input to local memory local[li] = tmp[i]; it.barrier(); // Perform inclusive scan in local memory for (int32_t d = 0; d &lt;= log2((float)G) - 1; ++d) { uint32_t stride = (1 &lt;&lt; d); int32_t update = (li &gt;= stride) ? local[li - stride] : 0; it.barrier(); local[li] += update; it.barrier(); } // Overwrite result from each work-item in the temporary buffer tmp[i] = local[li]; }); }).wait();// Phase 3: Update local scans using partial resultsq.parallel_for(nd_range&lt;1&gt;(N, L), [=](nd_item&lt;1&gt; it) { int g = it.get_group(0); if (g &gt; 0) { int i = it.get_global_id(0); output[i] += tmp[g - 1]; } }).wait();Pack和Unpack由于pack依赖于scan，适用于ND-range中所有元素的pack也必须通过全局内存和多个kernel来实现。然而，有一个常见的情况是只对特定的work-group或sub-group内的pack，不需要对ND-range内所有元素进行操作。原书中给出了一个实际的例子，可以参考源代码。Unpack和pack同理，同样依赖于scan，也可以参考原书中给出的例子。" }, { "title": "《数据并行C++》笔记（四）：深入DPC++并行编程", "url": "/posts/DPCPP4/", "categories": "读书笔记, 数据并行C++", "tags": "", "date": "2022-06-24 05:54:00 +0000", "snippet": "我们继续深入讨论DPC++并行编程，首先我们从任务图开始研究kernel调度的机制以及数据移动的机制，接下来讨论如何将一个大任务分解成一些小任务，用并行的思维去看待问题，最后我们讨论如何有多种方法定义kernel。Kernel调度与数据移动图调度我们之前讨论过依赖关系的概念，包括RAW、WAR与WAW。我们之前讨论过，一个命令组可以包含一个动作、依赖关系以及其他主机代码，其中动作是不可或缺的...", "content": "我们继续深入讨论DPC++并行编程，首先我们从任务图开始研究kernel调度的机制以及数据移动的机制，接下来讨论如何将一个大任务分解成一些小任务，用并行的思维去看待问题，最后我们讨论如何有多种方法定义kernel。Kernel调度与数据移动图调度我们之前讨论过依赖关系的概念，包括RAW、WAR与WAW。我们之前讨论过，一个命令组可以包含一个动作、依赖关系以及其他主机代码，其中动作是不可或缺的，命令组通常被表达为传递给submit方法的lambda表达式，也可以通过队列的快捷方法来表达。一个命令组可以执行两种类型的动作：kernel和显式内存操作，且一个命令组只能执行一个动作。kernel是通过调用parallel_for或single_task方法来定义的；显式数据移动的操作包括USM的memcpy、memset、fill操作以及缓冲区的copy、fill、update_host操作。命令组有几种方式指定依赖关系。首先可以使用顺序队列；可以使用基于事件的依赖关系，可以通过两种方式指定，第一种是通过depends_on函数传递一个事件作为参数，第二种是在队列上直接调用parallel_for或single_task传递事件；最后也可以通过accessor对象来指定数据依赖关系。我们知道，一旦kernel的依赖关系被满足，kernel就会被执行。同时，当一个命令组被提交到队列时，命令组的代码将立即在主机上执行（在submit函数返回之前），且只执行一次。数据移动我们之前已经讨论过一部分数据移动的内容，数据移动也会影响DPC++中的任务图调度。对显式数据移动而言，优点在于数据移动明确出现在图中。隐式数据移动则可能会对DPC++中的命令组和任务图产生隐藏的作用。与主机同步我们讨论的最后一个话题是如何使图的执行与主机同步。第一种方法是等待，队列对象由两种方法，wait和wait_and_throw，不过这种方法过于简单，我们可能需要更细致的控制；第二种方法是对事件进行同步，允许程序只在特定的动作或命令组上同步；第三种方法是使用主机accessor，即host_accessor，确保被复制回主机的数据是计算完成之后的值，但需要注意的是主机上的原始内存不能保证包含最新的值，主机accessor存在并保持数据可用时，设备不能访问该缓冲区，因此一个常见的模式是在某个C++作用域内创建主机accessor；最后一种方法是利用缓冲区的属性use_mutex，不过这种方式并不常见。通信与同步Work-Groups和Work-Items我们之前讨论过ND-range和分层并行kernel将work-item组织称work-group的形式，保证work-group中的work-item并发执行。大小为(8,8)的二维ND-range被切分成四个大小为(4,4)的work-groups同一work-group内的work-item可以保证同时执行，不同work-group内的work-item不能保证同时执行，这种情况下如果不同work-group的work-item进行通信，可能会出现死锁。如何高效通信通过Barrier进行同步Barrier由两个关键目的：首先是保证一个work-group内的work-item同步执行，其次是同步每个work-item访问内存的状态，即确保内存一致性。同一个work-group内的四个work-items通过barrier进行同步Work-Group本地内存通信可以通过USM或缓冲区进行，但可能效率不高，因而可以专门划分一部分内存用于通信，作为work-group的本地内存。每个work-group都可以访问全局内存或自己的本地内存当一个work-group开始执行时，本地内存的内容是未初始化的，且执行完成后会释放本地内存，因而本地内存只能用于临时存储。对CPU来说，本地内存和全局内存在物理上没什么区别，但是对GPU等其他设备而言，访问本地内存的性能会更好。我们可以使用info::device::local_mem_type来确定一个加速器是否有专门的资源用于本地内存。使用Work-Group Barriers和本地内存注意Work-item之间的通信需要work-group的存在，因此这些概念只在ND-range kernel和分层kernel中出现。我们以矩阵乘法kernel为例介绍work-groupo之间的通信，在许多设备（但不是所有设备上），通过本地内存进行通信可以提高矩阵乘法kernel的性能。h.parallel_for(range{M, N}, [=](id&lt;2&gt; id) { int m = id[0]; int n = id[1]; T sum = 0; for (int k = 0; k &lt; K; k++) { sum += matrixA[m][k] * matrixB[k][n]; } matrixC[m][n] = sum;});需要注意的是，很多供应商专门对矩阵乘法做了优化，例如可以从oneMKL中寻找合适的解决方案，而非自己重新实现一个矩阵乘法的kernel。我们之前已经注意到通过对work-item进行分组可以提升访问局部性，提高缓存命中率，在这一节中我们不再依靠隐式缓存行为，而使用本地内存作为显式缓存，在确定数据访问位置的情况下来提升性能。对很多算法而言，我们确实可以把本地内存看作是显式的缓存。把矩阵乘法映射到work-groups和work-items我们按上图所示的方式划分work-group，我们注意到绿色的部分很适合放在本地内存中。由于本地内存大小优先，我们把将要在本地内存上处理的矩阵部分称为tile，对每一个tile我们把矩阵的数据load到本地内存中，同步work-items，然后就可以从本地内存中load数据了，如下所示。注意我们选择的tile大小与工作组大小相等，这并非必须，但是选择一个work-group大小倍数的tile是很常见且方便的。绿色的数据从本地内存访问，蓝色和淡橙色的部分从全局内存访问ND-Range Kernel中的Work-Group Barriers和本地内存Kernel声明并操作一个本地accessor，代表本地地址空间中的内存分配，并调用一个barrier函数来同步work-group中的work-items。访问本地内存需要使用本地accessor，但本地accessor不是从缓冲区对象创建的。本地accessor必须是read-write属性的。为了同步ND-range kernel work-group内的work-item，需要调用nd_item类中的barrier函数。ND-range kernel版本的矩阵乘法如下所示。// Traditional accessors, representing matrices in global memory:accessor matrixA{bufA, h};accessor matrixB{bufB, h};accessor matrixC{bufC, h};// Local accessor, for one matrix tile:constexpr int tile_size = 16;auto tileA = accessor&lt;T, 1, access::mode::read_write, access::target::local&gt;( tile_size, h);h.parallel_for( nd_range&lt;2&gt;{ {M, N}, {1, tile_size}}, [=](nd_item&lt;2&gt; item) { // Indices in the global index space: int m = item.get_global_id()[0]; int n = item.get_global_id()[1]; // Index in the local index space: int i = item.get_local_id()[1]; T sum = 0; for (int kk = 0; kk &lt; K; kk += tile_size) { // Load the matrix tile from matrix A, and synchronize // to ensure all work-items have a consistent view // of the matrix tile in local memory. tileA[i] = matrixA[m][kk + i]; item.barrier(); // Perform computation using the local memory tile, and // matrix B in global memory. for (int k = 0; k &lt; tile_size; k++) { sum += tileA[k] * matrixB[kk + k][n]; } // After computation, synchronize again, to ensure all // reads from the local memory tile are complete. item.barrier(); } // Write the final result to global memory. matrixC[m][n] = sum; });分层Kernel中的Work-Group Barriers和本地内存与ND-range kernel不同，分层kernel中的本地内存和barrier是隐式的，不需要特殊的语法或函数调用。在分层kernel中，我们使用parallel_for_work_group和parallel_for_work_item函数来表达两个层次的并行执行，可以通过这两个层次来表达一个变量是否在work-group的本地内存中，以及是否被work-group内的所有work-item共享。分层kernel版本的矩阵乘法如下所示。const int tileSize = 16;range group_size{1, tileSize};range num_groups{M, N / tileSize};h.parallel_for_work_group(num_groups, group_size, [=](group&lt;2&gt; group) { // Because this array is declared at work-group scope // it is in local memory T tileA[16]; for (int kk = 0; kk &lt; K; kk += tileSize) { // A barrier may be inserted between scopes here // automatically, unless the compiler can prove it is // not required // Load the matrix tile from matrix A group.parallel_for_work_item([&amp;](h_item&lt;2&gt; item) { int m = item.get_global_id()[0]; int i = item.get_local_id()[1]; tileA[i] = matrixA[m][kk + i]; }); // A barrier gets inserted here automatically, so all // work items have a consistent view of memory group.parallel_for_work_item([&amp;](h_item&lt;2&gt; item) { int m = item.get_global_id()[0]; int n = item.get_global_id()[1]; for (int k = 0; k &lt; tileSize; k++) { matrixC[m][n] += tileA[k] * matrixB[kk + k][n]; } }); // A barrier gets inserted here automatically, too }});Sub-GroupND-range kernel中还有一种分组方式叫sub-group。Sub-group也有自己的barrier函数，只同步sub-group内的work-items。不过sub-group没有专门的内存空间，通常使用集体函数（collective function）来交换数据。我们接下来用broadcast广播集体函数来实现基于sub-group的矩阵乘法。ND-range中使用sub-group的kernel版本的矩阵乘法如下所示。// Note: This example assumes that the sub-group size is greater than or// equal to the tile size!static const int tileSize = 4;h.parallel_for( nd_range&lt;2&gt;{ {M, N}, {1, tileSize}}, [=](nd_item&lt;2&gt; item) { auto sg = item.get_sub_group(); // Indices in the global index space: int m = item.get_global_id()[0]; int n = item.get_global_id()[1]; // Index in the local index space: int i = item.get_local_id()[1]; T sum = 0; for (int_fast64_t kk = 0; kk &lt; K; kk += tileSize) { // Load the matrix tile from matrix A. T tileA = matrixA[m][kk + i]; // Perform computation by broadcasting from the matrix // tile and loading from matrix B in global memory. The loop // variable k describes which work-item in the sub-group to // broadcast data from. for (int k = 0; k &lt; tileSize; k++) { sum += group_broadcast(sg, tileA, k) * matrixB[kk + k][n]; } } // Write the final result to global memory. matrixC[m][n] = sum; });集体函数我们现在进一步讨论集体函数，对常见通信模式使用集体函数可以简化代码并提升性能。 broadcast广播函数运行一个work-item和其他组内的work-item共享变量的值，work-group和sub-group都支持广播。 any_of和all_of投票函数可以用来比较一个work-group内bool条件的结果，work-group和sub-group都支持投票。 shuffle操作可以在一个sub-group内进行任意的通信。 Sub-group的load和store可以通知编译器组内的所有work-item都是从同一位置开始load连续的数据，以及可以优化大量连续数据的load和store。 广播函数投票函数使用通用shuffle操作，基于计算好的排列下标来对x的值进行排序使用shuffle_down来移动sub-group内x的值使用shuffle_xor来交换相邻的x使用shuffle_xor来反转xSub-group访问4个相邻区块的内存访问模式定义Kernel我们主要讨论三种表示kernel的方法：lambda表达式、命名函数对象（functors）和其他语言或API创建的kernel。Lambda表达式到目前为止我们都在使用lambda表达式来定义kernel，如下是lambda表达式的一般形式。Kernel lambda表达式的一般形式，包括了可选元素一般的C++ lambda表达式可以通过复制或引用来捕获一个变量，但是对kernel lambda表达式，变量只能通过复制来捕获。异常必须使用noexcept，因为kernel不支持任何形式的异常。lambda属性也是支持的，可以用来控制kernel的编译方式，例如图中的reqd_work_group_size可以用来表示kernel要求特定的work-group大小。可以指定返回类型，但是对kernel来说必须是void。命名函数对象命名函数对象，也叫functors，是C++定义的一种特殊的类，相比lambda表达式往往需要更多代码，但也提供了更多的控制和额外的能力，和其他C++类一样也可以使用模板。下面是使用命名函数对象的一个例子。#include &lt;CL/sycl.hpp&gt;#include &lt;array&gt;#include &lt;iostream&gt;using namespace sycl;class Add {public: Add(accessor&lt;int&gt; acc) : data_acc(acc) {} void operator()(id&lt;1&gt; i) const { data_acc[i] = data_acc[i] + 1; }private: accessor&lt;int&gt; data_acc;};int main() { constexpr size_t size = 16; std::array&lt;int, size&gt; data; for (int i = 0; i &lt; size; i++) { data[i] = i; } { buffer data_buf{data}; queue Q{ host_selector{} }; std::cout &lt;&lt; \"Running on device: \" &lt;&lt; Q.get_device().get_info&lt;info::device::name&gt;() &lt;&lt; \"\\n\"; Q.submit([&amp;](handler&amp; h) { accessor data_acc {data_buf, h}; h.parallel_for(size, Add(data_acc)); }); } for (int i = 0; i &lt; size; i++) { if (data[i] != i + 1) { std::cout &lt;&lt; \"Results did not validate at index \" &lt;&lt; i &lt;&lt; \"!\\n\"; return -1; } } std::cout &lt;&lt; \"Success!\\n\"; return 0;}其他API下面的一个例子展示了SYCL kernel如何写成OpenCL C kernel。// Note: This must select a device that supports interop with OpenCL kernel// objects!queue Q{ cpu_selector{} };context sc = Q.get_context();const char* kernelSource = R\"CLC( kernel void add(global int* data) { int index = get_global_id(0); data[index] = data[index] + 1; } )CLC\";cl_context c = get_native&lt;backend::opencl, context&gt;(sc);cl_program p = clCreateProgramWithSource(c, 1, &amp;kernelSource, nullptr, nullptr);clBuildProgram(p, 0, nullptr, nullptr, nullptr, nullptr);cl_kernel k = clCreateKernel(p, \"add\", nullptr);std::cout &lt;&lt; \"Running on device: \" &lt;&lt; Q.get_device().get_info&lt;info::device::name&gt;() &lt;&lt; \"\\n\";Q.submit([&amp;](handler&amp; h) { accessor data_acc{data_buf, h}; h.set_args(data_acc); h.parallel_for(size, make_kernel&lt;backend::opencl&gt;(k, sc));});clReleaseContext(c);clReleaseProgram(p);clReleaseKernel(k);具体的细节比较复杂，可以参考原书。" }, { "title": "《数据并行C++》笔记（三）：数据管理", "url": "/posts/DPCPP3/", "categories": "读书笔记, 数据并行C++", "tags": "", "date": "2022-06-23 19:43:00 +0000", "snippet": "我们之前简单讨论过管理数据的两种方式，即USM和缓冲区，前者主要使用指针，后者则是一个更高层次的接口，我们接下来深入讨论如何管理数据。USMUSM基于C++的指针，可以很容易地将现有代码迁移到USM上。分配类型USM内存分配类型USM定义了三种不同类型的内存分配，每种分配都有独特的语义，需要注意的是，一个设备甚至可能不支持所有类型的USM分配。设备分配这种分配类型可以在设备存储器上分配内存，...", "content": "我们之前简单讨论过管理数据的两种方式，即USM和缓冲区，前者主要使用指针，后者则是一个更高层次的接口，我们接下来深入讨论如何管理数据。USMUSM基于C++的指针，可以很容易地将现有代码迁移到USM上。分配类型USM内存分配类型USM定义了三种不同类型的内存分配，每种分配都有独特的语义，需要注意的是，一个设备甚至可能不支持所有类型的USM分配。设备分配这种分配类型可以在设备存储器上分配内存，如(G)DDR或HBM等，但不能在主机上访问。如果需要在主机上访问，需要使用USM memcpy机制来复制数据。主机分配主机分配在主机的内存中分配空间，在主机和设备上都可以访问，但不能迁移到设备的存储器中。从设备访问是远程进行的，通常会通过一个较慢的总线，如PCI-Express。共享分配共享分配结合了前两者的优点，可以在主机和设备上访问，且可以自行迁移数据而无需干预。不过，共享分配仍然有潜在的问题，如果自动迁移数据可能会增加延迟，无法控制。内存分配在C++中，我们只需要关心需要多少内存以及还剩多少内存可以分配，但USM需要更多的信息。首先需要知道指定哪种类型的分配；其次需要指定一个context上下文对象，上下文代表一个或一组设备，可以在上面执行kernel，USM分配不保证可以在不同的上下文中使用；最后，设备分配要求我们指定在哪个设备上分配内存。USM提供了不同风格的语法，我们接下来简单讨论一下。C风格内存分配主要使用malloc函数，可以使用两种风格：命名函数和单一函数。命名函数有malloc_device、malloc_host和malloc_shared，单一函数则需要malloc附加参数。此外，每个malloc都有一个aligned_alloc的对应函数，可以返回对齐的内存指针。// Named Functionsvoid *malloc_device(size_t size, const device &amp;dev, const context &amp;ctxt);void *malloc_device(size_t size, const queue &amp;q);void *aligned_alloc_device(size_t alignment, size_t size, const device &amp;dev, const context &amp;ctxt);void *aligned_alloc_device(size_t alignment, size_t size, const queue &amp;q);void *malloc_host(size_t size, const context &amp;ctxt);void *malloc_host(size_t size, const queue &amp;q);void *aligned_alloc_host(size_t alignment, size_t size, const context &amp;ctxt);void *aligned_alloc_host(size_t alignment, size_t size, const queue &amp;q);void *malloc_shared(size_t size, const device &amp;dev, const context &amp;ctxt);void *malloc_shared(size_t size, const queue &amp;q);void *aligned_alloc_shared(size_t alignment, size_t size, const device &amp;dev, const context &amp;ctxt);void *aligned_alloc_shared(size_t alignment, size_t size, const queue &amp;q);// Single Functionvoid *malloc(size_t size, const device &amp;dev, const context &amp;ctxt, usm::alloc kind);void *malloc(size_t size, const queue &amp;q, usm::alloc kind);void *aligned_alloc(size_t alignment, size_t size, const device &amp;dev, const context &amp;ctxt, usm::alloc kind);void *aligned_alloc(size_t alignment, size_t size, const queue &amp;q, usm::alloc kind);C++风格内存分配我们同样有命名和单一函数版本的分配函数，以及默认和对齐的函数。不过在C++风格中，我们可以使用模板。// Named Functionstemplate &lt;typename T&gt;T *malloc_device(size_t Count, const device &amp;Dev, const context &amp;Ctxt);template &lt;typename T&gt;T *malloc_device(size_t Count, const queue &amp;Q);template &lt;typename T&gt;T *aligned_alloc_device(size_t Alignment, size_t Count, const device &amp;Dev, const context &amp;Ctxt);T *aligned_alloc_device(size_t Alignment, size_t Count, const queue &amp;Q);template &lt;typename T&gt;T *malloc_host(size_t Count, const context &amp;Ctxt);template &lt;typename T&gt;T *malloc_host(size_t Count, const queue &amp;Q);template &lt;typename T&gt;T *aligned_alloc_host(size_t Alignment, size_t Count, const context &amp;Ctxt);template &lt;typename T&gt;T *aligned_alloc_host(size_t Alignment, size_t Count, const queue &amp;Q);template &lt;typename T&gt;T *malloc_shared(size_t Count, const device &amp;Dev, const context &amp;Ctxt);template &lt;typename T&gt;T *malloc_shared(size_t Count, const queue &amp;Q);template &lt;typename T&gt;T *aligned_alloc_shared(size_t Alignment, size_t Count, const device &amp;Dev, const context &amp;Ctxt);template &lt;typename T&gt;T *aligned_alloc_shared(size_t Alignment, size_t Count, const queue &amp;Q);// Single Functiontemplate &lt;typename T&gt;T *malloc(size_t Count, const device &amp;Dev, const context &amp;Ctxt, usm::alloc Kind);template &lt;typename T&gt;T *malloc(size_t Count, const queue &amp;Q, usm::alloc Kind);template &lt;typename T&gt;T *aligned_alloc(size_t Alignment, size_t Count, const device &amp;Dev, const context &amp;Ctxt, usm::alloc Kind);template &lt;typename T&gt;T *aligned_alloc(size_t Alignment, size_t Count, const queue &amp;Q, usm::alloc Kind);C++分配器这种方式基于C++的分配器接口，如果代码大量使用容器的话，这种方式是最方便的。内存释放USM定义了一个free函数来释放由malloc或aligned_malloc分配的内存，也可以接受上下文作为一个额外的参数。如果是用C++分配器对象分配的，也应该用该对象的方法来释放内存。数据管理数据初始化有各种方式可以对一段内存空间进行初始化。首先可以通过一个kernel来完成；我们也可以通过主机代码中的一个循环来实现，不过无法访问设备内存；最后我们也可以使用memset函数，往内存中填充字节，USM还提供了类似的fill函数，让我们可以用一个任意的模式来填充内存数据移动USM定义了两种我们可以用来管理数据的策略：显式和隐式。我们哪种策略与硬件支持的或USM分配类型有关。显式数据移动可以通过调用memcpy函数在主机和设备之间复制数据。隐式数据移动则不需要我们手动干预，主要涉及到USM共享分配，但也涉及到一些问题，如主机和设备不应该试图在同一时间访问共享分配内存、分配受到设备内存的限制、影响性能等。当设备支持共享分配的按需迁移时，数据移动可能会提前完成，但kernel在等待数据移动完成之前可能会停顿。DPC++提供了方法来修改自动迁移机制，主要由两个函数prefetch和mem_advise，具体的细节比较复杂，应该参考文档。查询不是所有的设备都支持USM的所有功能，我们可以查询一个设备是否支持某些功能，包括指针查询和设备能力查询。指针查询可以回答两个问题，使用get_pointer_type函数可以查询指针指向什么类型的USM分配，使用get_pointer_device函数可以查询USM指针是针对什么设备分配的；设备能力查询可以通过调用设备对象的get_info来测试一个设备支持哪些类型的USM分配。USM设备信息描述符缓冲区缓冲区是一个更高层次的抽象，本身只代表数据，而如何管理数据（包括如何存储以及移动）则是运行时的工作。我们接下来讨论如何创建和使用缓冲区，以及accessor对象的概念。缓冲区的创建和使用template &lt;typename T, int Dimensions, AllocatorT allocator&gt;class buffer;创建我们在下面的代码中展示了几种创建缓冲区对象的方法。// Create a buffer of 2x5 ints using the default allocatorbuffer&lt;int, 2, buffer_allocator&gt; b1{range&lt;2&gt;{2, 5}};// Create a buffer of 2x5 ints using the default allocator// and CTAD for rangebuffer&lt;int, 2&gt; b2{range{2, 5}};// Create a buffer of 20 floats using a// default-constructed std::allocatorbuffer&lt;float, 1, std::allocator&lt;float&gt;&gt; b3{range{20}};// Create a buffer of 20 floats using a passed-in allocatorstd::allocator&lt;float&gt; myFloatAlloc;buffer&lt;float, 1, std::allocator&lt;float&gt;&gt; b4{range(20), myFloatAlloc};注意在b2缓冲区的创建中我们使用了C++ 17引入的模板类参数推断（class template argument deduction, CTAD）功能，省略了range&lt;2&gt;{2, 5}中的&lt;2&gt;，因为我们可以推断出这是一个二维的范围。我们也可以使用现有的C++内存分配来创建缓冲区，并且在这种方式中可以初始化其中的数据。// Create a buffer of 4 doubles and initialize it from a host pointerdouble myDoubles[4] = {1.1, 2.2, 3.3, 4.4};buffer b5{myDoubles, range{4}};// Create a buffer of 5 doubles and initialize it from a host pointer// to const doubleconst double myConstDbls[5] = {1.0, 2.0, 3.0, 4.0, 5.0};buffer b6{myConstDbls, range{5}};// Create a buffer from a shared pointer to intauto sharedPtr = std::make_shared&lt;int&gt;(42);buffer b7{sharedPtr, range{1}};最后，我们可以使用C++容器来创建缓冲区，可以通过一个缓冲区创建一个子缓冲区。// Create a buffer of ints from an input iteratorstd::vector&lt;int&gt; myVec;buffer b8{myVec.begin(), myVec.end()};buffer b9{myVec};// Create a buffer of 2x5 ints and 2 non-overlapping// sub-buffers of 5 ints.buffer&lt;int, 2&gt; b10{range{2, 5}};buffer b11{b10, id{0, 0}, range{1, 5}};buffer b12{b10, id{1, 0}, range{1, 5}};使用我们不能直接访问缓冲区的数据，只能使用accessor，我们会在之后进一步讨论这一点。不过我们仍然可以访问缓冲区的一些特性，比如可以查询缓冲区的大小、分配器对象、是否是子缓冲区等。Accessoraccessor对象由accessor类创建，有五个模板参数，分别为访问数据的类型、维度、访问模式、访问目标、是否为placeholder。访问模式访问目标创建下面的例子展示了如何创建并使用accessor。#include &lt;cassert&gt;#include &lt;CL/sycl.hpp&gt;using namespace sycl;constexpr int N = 42;int main() { queue Q; // create 3 buffers of 42 ints buffer&lt;int&gt; A{range{N}}; buffer&lt;int&gt; B{range{N}}; buffer&lt;int&gt; C{range{N}}; accessor pC{C}; Q.submit([&amp;](handler &amp;h) { accessor aA{A, h}; accessor aB{B, h}; accessor aC{C, h}; h.parallel_for(N, [=](id&lt;1&gt; i) { aA[i] = 1; aB[i] = 40; aC[i] = 0; }); }); Q.submit([&amp;](handler &amp;h) { accessor aA{A, h}; accessor aB{B, h}; accessor aC{C, h}; h.parallel_for(N, [=](id&lt;1&gt; i) { aC[i] += aA[i] + aB[i]; }); }); Q.submit([&amp;](handler &amp;h) { h.require(pC); h.parallel_for(N, [=](id&lt;1&gt; i) { pC[i]++; }); }); host_accessor result{C}; for (int i = 0; i &lt; N; i++) { assert(result[i] == N); } return 0;}我们介绍一个工具，叫做访问标记（access tag），这是一种简洁的可以表达访问模式和目标组合的方式，作为参数传递给accessor的构造函数，可能的标记如下所示，C++ CTAD可以正确推断出所需的访问模式和目标。访问标记修改后的程序如下所示。注意还有一个新参数叫noinit，运行时可以知道缓冲区之前的内容可以被直接丢弃，因而可以消除不必要的数据移动。#include &lt;CL/sycl.hpp&gt;#include &lt;cassert&gt;using namespace sycl;constexpr int N = 42;int main() { queue Q; // Create 3 buffers of 42 ints buffer&lt;int&gt; A{range{N}}; buffer&lt;int&gt; B{range{N}}; buffer&lt;int&gt; C{range{N}}; accessor pC{C}; Q.submit([&amp;](handler &amp;h) { accessor aA{A, h, write_only, no_init}; accessor aB{B, h, write_only, no_init}; accessor aC{C, h, write_only, no_init}; h.parallel_for(N, [=](id&lt;1&gt; i) { aA[i] = 1; aB[i] = 40; aC[i] = 0; }); }); Q.submit([&amp;](handler &amp;h) { accessor aA{A, h, read_only}; accessor aB{B, h, read_only}; accessor aC{C, h, read_write}; h.parallel_for(N, [=](id&lt;1&gt; i) { aC[i] += aA[i] + aB[i]; }); }); Q.submit([&amp;](handler &amp;h) { h.require(pC); h.parallel_for(N, [=](id&lt;1&gt; i) { pC[i]++; }); }); host_accessor result{C, read_only}; for (int i = 0; i &lt; N; i++) { assert(result[i] == N); } return 0;}使用之前我们提到，accessor最重要的功能是让我们可以访问数据，通常是通过[]操作符来完成的。accessor可以返回一个指向底层数据的指针，也可以查询访问的元素的数量、数据范围等。" }, { "title": "《数据并行C++》笔记（二）：并行编程", "url": "/posts/DPCPP2/", "categories": "读书笔记, 数据并行C++", "tags": "", "date": "2022-06-23 00:00:00 +0000", "snippet": "在前一节中我们简单介绍了主机与设备代码、数据管理与移动等基本概念，这一节我们进一步深入讨论如何处理、开发代码以充分挖掘程序中的并行性。表达并行性并行编程往往很有挑战性，且我们会有如下一些问题： 为什么有不止一种方法来表达并行性？ 我们应该使用哪种方法来表达并行性？ 我需要了解多少关于执行模型的概念？ Kernel内的并行性Kernel开发应该保证可移植...", "content": "在前一节中我们简单介绍了主机与设备代码、数据管理与移动等基本概念，这一节我们进一步深入讨论如何处理、开发代码以充分挖掘程序中的并行性。表达并行性并行编程往往很有挑战性，且我们会有如下一些问题： 为什么有不止一种方法来表达并行性？ 我们应该使用哪种方法来表达并行性？ 我需要了解多少关于执行模型的概念？ Kernel内的并行性Kernel开发应该保证可移植性，也就是说并行性不应该被硬编码以适应不同的硬件系统（如GPU、SIMD单元等），但可移植性可能也会牺牲性能。在应用程序开发过程中，在性能、可移植性和生产力之间取得适当的平衡是我们必须面对的挑战。需要关注的两个概念是多维kernel以及kernel与loop之间的区别。SYCL语言特性当我们编写一个并行kernel时，我们需要决定使用哪种类型的kernel以及如何在我们的程序中表达。我们在本节中使用lambda表达式来表达kernel。Kernel有三种形式：基本数据并行、ND-range数据并行、分层数据并行。基本数据并行基本数据并行kernel是以单程序多数据流（SPMD）的方式编写的，优势在于同一个程序可以被隐式映射到多个级别与并行资源，例如可以被流水线化、打包在一起使用SIMD单元执行、或者分散到多个线程执行等。基本数据并行的执行空间称为执行范围（range），kernel的每一个实例称为一个item，如下图所示。基本数据并行kernel的执行空间基本的数据并行kernel的功能是通过三个C++类表示的：range、id和item。range表示一个一维、二维或三维的范围，维度需要在编译期确定，但每个维度的大小可以是动态的，下面的代码是range类的简化定义。template &lt;int Dimensions = 1&gt;class range {public: // Construct a range with one, two or three dimensions range(size_t dim0); range(size_t dim0, size_t dim1); range(size_t dim0, size_t dim1, size_t dim2); // Return the size of the range in a specific dimension size_t get(int dimension) const; size_t &amp;operator[](int dimension); size_t operator[](int dimension) const; // Return the product of the size of each dimension size_t size() const; // Arithmetic operations on ranges are also supported};id表示一个一维、二维或三维范围内的索引。尽管我们可以构造一个id来代表一个任意的索引，但为了获得与特定kernel实例相关的索引，我们必须将其作为kernel函数的一个参数。下面的代码是id类的简化定义。template &lt;int Dimensions = 1&gt;class id {public: // Construct an id with one, two or three dimensions id(size_t dim0); id(size_t dim0, size_t dim1); id(size_t dim0, size_t dim1, size_t dim2); // Return the component of the id in a specific dimension size_t get(int dimension) const; size_t &amp;operator[](int dimension); size_t operator[](int dimension) const; // Arithmetic operations on ranges are also supported};item代表了一个kernel函数的单个实例，同时封装了其执行范围和实例在该范围内的索引。和id的主要区别在于item给出了额外的函数来查询范围与线性化的索引。与id一样，获得与特定kernel实例相关item的唯一方法是作为kernel函数的参数传入。下面的代码是item类的简化定义。template &lt;int Dimensions = 1, bool WithOffset = true&gt;class item {public: // Return the index of this item in the kernel's execution range id&lt;Dimensions&gt; get_id() const; size_t get_id(int dimension) const; size_t operator[](int dimension) const; // Return the execution range of the kernel executed by this item range&lt;Dimensions&gt; get_range() const; size_t get_range(int dimension) const; // Return the offset of this item (if with_offset == true) id&lt;Dimensions&gt; get_offset() const; // Return the linear index of this item // e.g. id(0) * range(1) * range(2) + id(1) * range(2) + id(2) size_t get_linear_id() const;};ND-range数据并行这种类型同样适用SPMD风格编写，不过引入了组的概念。一个ND-range kernel的执行范围被划分为work-groups、sub-groups和work-items，sub-group总是一维的，如下图所示。三维ND-range被划分为work-groups、sub-groups和work-items每种类型的组到硬件资源的映射是由实现定义的，正是这种灵活性使程序能够在各种各样的硬件上执行。Work-item代表了一个kernel函数的实例，可以按任何顺序执行，除了对全局内存的原子内存操作之外，不能相互通信或同步；Work-group可以按任何顺序执行，work-group内的work-items可以访问work-group本地内存，可以映射到一些设备上的本地存储器，可以使用work-group barriers进行同步，使用work-group内存fences保证内存一致性，可以访问groupo函数；Sub-group可以进一步进行局部的调度，例如可以使用编译器向量化的功能使sub-group内的work-item并行执行，sub-group内的work-item可以进行同步，但没有自己的本地内存，但可以使用shuffle操作直接交换数据。// Create buffers associated with inputs and outputbuffer&lt;float, 2&gt; a_buf(a.data(), range&lt;2&gt;(N, N)), b_buf(b.data(), range&lt;2&gt;(N, N)), c_buf(c.data(), range&lt;2&gt;(N, N));// Submit the kernel to the queueQ.submit([&amp;](handler&amp; h) { accessor a{a_buf, h}; accessor b{b_buf, h}; accessor c{c_buf, h}; range global{N, N}; range local{B, B}; h.parallel_for(nd_range{global, local}, [=](nd_item&lt;2&gt; it) { int j = it.get_global_id(0); int i = it.get_global_id(1); for (int k = 0; k &lt; N; ++k) { c[j][i] += a[j][k] * b[k][i]; } });});将矩阵乘法映射到work-groups和work-items和基本数据并行kernel相比，ND-range数据并行kernel使用了不同的类。nd_range类表示一个分组的执行范围，构造函数的参数是两个range类的实例，第一个表示全局执行范围，第二个表示每个work-group的局部执行范围。下面的代码是nd_range类的简化定义。template &lt;int Dimensions = 1&gt;class nd_range {public: // Construct an nd_range from global and work-group local ranges nd_range(range&lt;Dimensions&gt; global, range&lt;Dimensions&gt; local); // Return the global and work-group local ranges range&lt;Dimensions&gt; get_global_range() const; range&lt;Dimensions&gt; get_local_range() const; // Return the number of work-groups in the global range range&lt;Dimensions&gt; get_group_range() const;};nd_item类似于item，不同之处在于范围内的查询和表示方式。下面的代码是nd_item类的简化定义。template &lt;int Dimensions = 1&gt;class nd_item {public: // Return the index of this item in the kernel's execution range id&lt;Dimensions&gt; get_global_id() const; size_t get_global_id(int dimension) const; size_t get_global_linear_id() const; // Return the execution range of the kernel executed by this item range&lt;Dimensions&gt; get_global_range() const; size_t get_global_range(int dimension) const; // Return the index of this item within its parent work-group id&lt;Dimensions&gt; get_local_id() const; size_t get_local_id(int dimension) const; size_t get_local_linear_id() const; // Return the execution range of this item's parent work-group range&lt;Dimensions&gt; get_local_range() const; size_t get_local_range(int dimension) const; // Return a handle to the work-group // or sub-group containing this item group&lt;Dimensions&gt; get_group() const; sub_group get_sub_group() const;};group类封装了有关work-group的功能，简化的代码如下所示。template &lt;int Dimensions = 1&gt;class group {public: // Return the index of this group in the kernel's execution range id&lt;Dimensions&gt; get_id() const; size_t get_id(int dimension) const; size_t get_linear_id() const; // Return the number of groups in the kernel's execution range range&lt;Dimensions&gt; get_group_range() const; size_t get_group_range(int dimension) const; // Return the number of work-items in this group range&lt;Dimensions&gt; get_local_range() const; size_t get_local_range(int dimension) const;};sub_group类封装了有关sub-group的功能，简化的代码如下所示。注意sub_group类是访问sub-group的唯一途径。class sub_group {public: // Return the index of the sub-group id&lt;1&gt; get_group_id() const; // Return the number of sub-groups in this item's parent work-group range&lt;1&gt; get_group_range() const; // Return the index of the work-item in this sub-group id&lt;1&gt; get_local_id() const; // Return the number of work-items in this sub-group range&lt;1&gt; get_local_range() const; // Return the maximum number of work-items in any // sub-group in this item's parent work-group range&lt;1&gt; get_max_local_range() const;};分层数据并行分层数据并行kernel提供了一种实验性的替代语法，通过work-groups和work-itmes来表达kernel，但使用嵌套调用的parallel_for函数，其复杂性在于parallel_for每次嵌套调用都会创建一个单独的SPMD环境。在分层数据并行kernel中，parallel_for函数被paralle_for_work_group和parallel_for_work_item函数取代。具体的实现细节与示例代码可以参考原书。在不同kernel形式之间选择主要是个人偏好的问题，同时也会影响如何表达所需的某些功能。具体如何选择kernel的形式可以参考下图。选择合适的kernel形式每种kernel形式的特性" }, { "title": "《数据并行C++》笔记（一）：基础", "url": "/posts/DPCPP1/", "categories": "读书笔记, 数据并行C++", "tags": "", "date": "2022-06-22 06:39:00 +0000", "snippet": "《数据并行C++》原书英文名为Data Parallel C++，可以在这里下载pdf。入门数据并行性让我们有机会可以利用异构系统中的并行资源来对计算进行加速，如使用CPU中的SIMD单元、GPU、FPGA或者其他ASIC加速器。SYCL是一个行业驱动的标准，不过SYCL只是一个名字，不是什么缩写。DPC++是一个开源的编译器项目，基于LLVM，由Intel发起，是SYCL的一种实现。搭建环...", "content": "《数据并行C++》原书英文名为Data Parallel C++，可以在这里下载pdf。入门数据并行性让我们有机会可以利用异构系统中的并行资源来对计算进行加速，如使用CPU中的SIMD单元、GPU、FPGA或者其他ASIC加速器。SYCL是一个行业驱动的标准，不过SYCL只是一个名字，不是什么缩写。DPC++是一个开源的编译器项目，基于LLVM，由Intel发起，是SYCL的一种实现。搭建环境和使用DPC++编译器可以参考这篇文档，本书用到的示例代码在这里，SYCL语言标准可以参考这里，DPC++编译器的文档可以参考这里。DPC++与SYCL的一些概念 SYCL程序可以是单源的，也就是说compute kernel的代码和协调这些kernel执行的主机代码可以放在同一个源文件中。 每个程序从主机上开始执行，主机通常是一个CPU。 在一个程序中使用多个设备才叫做异构编程，设备可以包括GPU、FPGA、DSP、ASIC、CPU、AI加速器等。 多个程序可以共享设备。 一个设备执行的代码叫做kernel，kernel代码通常不支持的功能包括动态内存分配、静态变量、函数指针、调用virtual成员函数、递归等。 任务通过队列传输给设备，主机程序不需要等待，这种模式称为异步编程，程序可以看作是一个异步任务图。 代码在哪执行本章描述了代码可以在哪里执行、何时执行、以及用于控制执行位置的机制。单源单源代码同时包含主机代码和设备代码简单的SYCL程序主机代码是一个应用程序的骨干，定义并控制对可用设备的工作分配，并在运行时管理数据和依赖关系。主机通过队列的机制将任务提交给设备，设备代码有一些重要性质：设备代码异步执行（所有依赖满足之后就会开始工作，不阻塞主机代码执行）、设备代码有一些限制（如有时不支持动态内存分配）、SYCL定义的一些函数和查询功能只能在设备代码中使用。我们把包含了设备代码的、提交给队列的任务称为动作（actions），动作不仅包括设备代码的执行，也包括了内存数据移动。选择执行代码的设备方法1：在任何类型的设备上运行当我们不关心我们的设备代码将在哪里运行时，我们可以直接让运行时系统进行选择。我们在讨论选择什么样的设备之前，先介绍程序和设备互动的机制，也就是队列。一个队列被绑定到一个单一的设备上，绑定的过程发生在构造函数中，如下图所示。一个队列绑定到一个单一的设备可以在一个程序中创建多个队列，多个不同的队列也可以绑定到一个设备（如GPU）上，提交到不同的队列将导致在设备上进行合并的工作。如果没有指定队列应该绑定的设备，就会在运行时选择可用的设备，SYCL保证至少有一个设备总是可用的，即主机设备本身。方法2：使用主机设备进行开发和调试主机设备提供了一个保证，即设备代码总是可以被运行（不依赖加速器硬件），并有几个主要用途，如在没有加速器的系统上开发设备代码、用非加速器工具对设备代码进行调试、作为备份等。方法3：使用GPU（或者其他加速器）设备被分为几个大类：主机设备、加速器设备，加速器又可以分为CPU设备、GPU设备、以及其他加速器（包括FPGA、DSP等）。SYCL为这些大类提供了内置的选择器类，包括default_selector、host_selector、cpu_selector、gpu_selector、accelerator_selector，DPC++在CL/sycl/intel/fpga_extensions.hpp提供了一个额外的选择器INTEL::fpga_selector。使用多个设备#include &lt;CL/sycl.hpp&gt;#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt; // For fpga_selector#include &lt;iostream&gt;using namespace sycl;int main() { queue my_gpu_queue( gpu_selector{} ); queue my_fpga_queue( ext::intel::fpga_selector{} ); std::cout &lt;&lt; \"Selected device 1: \" &lt;&lt; my_gpu_queue.get_device().get_info&lt;info::device::name&gt;() &lt;&lt; \"\\n\"; std::cout &lt;&lt; \"Selected device 2: \" &lt;&lt; my_fpga_queue.get_device().get_info&lt;info::device::name&gt;() &lt;&lt; \"\\n\"; return 0;}自定义设备选择我们也可以编写一个自定义的选择器，例如从系统中可用的一组GPU中挑选出一个所需的GPU，选择器派生自device_selector这个抽象类。例如下面的代码负责选择Intel Arria系列的FPGA器件。class my_selector : public device_selector { public: int operator()(const device &amp;dev) const override { if ( dev.get_info&lt;info::device::name&gt;().find(\"Arria\") != std::string::npos &amp;&amp; dev.get_info&lt;info::device::vendor&gt;().find(\"Intel\") != std::string::npos) { return 1; } return -1; }};在一个设备上创建工作应用程序通常包含主机代码和设备代码的组合。有几个类成员允许我们提交设备代码来执行，由于这些工作调度结构是提交设备代码的唯一方法，它们允许我们很容易地将设备代码与主机代码区分开来。本章的其余部分介绍了一些工作调度结构，目的是帮助我们理解和识别设备代码和在主机处理器上原生执行的主机代码之间的划分。任务图该图中的每个节点（工作单元）都包含了一个要在设备上执行的动作，最常见的动作是数据并行 设备kernel的调用。任务图定义了一个或多个设备上异步执行的动作以及动作间的依赖关系传递设备代码之前例子中的lambda表达式是要在设备上执行的设备代码，其中parallel_for是让我们区分设备代码和主机代码的结构，是设备调度机制中的一种，且为handler类的成员。下面的代码给出了handler类的一个简化定义。class handler {public: // Specify event(s) that must be complete before the action // defined in this command group executes. void depends_on(std::vector&lt;event&gt;&amp; events); // Guarantee that the memory object accessed by the accessor // is updated on the host after this action executes. template &lt;typename AccessorT&gt; void update_host(AccessorT acc); // Submit a memset operation writing to the specified pointer. // Return an event representing this operation. event memset(void *ptr, int value, size_t count); // Submit a memcpy operation copying from src to dest. // Return an event representing this operation. event memcpy(void *dest, const void *src, size_t count); // Copy to/from an accessor and host memory. // Accessors are required to have appropriate correct permissions. // Pointer can be a raw pointer or shared_ptr. template &lt;typename SrcAccessorT, typename DestPointerT&gt; void copy(SrcAccessorT src, DestPointerT dest); template &lt;typename SrcPointerT, typename DestAccessorT&gt; void copy(SrcPointerT src, DestAccessorT dest); // Copy between accessors. // Accessors are required to have appropriate correct permissions. template &lt;typename SrcAccessorT, typename DestAccessorT&gt; void copy(SrcAccessorT src, DestAccessorT dest); // Submit different forms of kernel for execution. template &lt;typename KernelName, typename KernelType&gt; void single_task(KernelType kernel); template &lt;typename KernelName, typename KernelType, int Dims&gt; void parallel_for(range&lt;Dims&gt; num_work_items, KernelType kernel); template &lt;typename KernelName, typename KernelType, int Dims&gt; void parallel_for(nd_range&lt;Dims&gt; execution_range, KernelType kernel); template &lt;typename KernelName, typename KernelType, int Dims&gt; void parallel_for_work_group(range&lt;Dims&gt; num_groups, KernelType kernel); template &lt;typename KernelName, typename KernelType, int Dims&gt; void parallel_for_work_group(range&lt;Dims&gt; num_groups, range&lt;Dims&gt; group_size, KernelType kernel);};除了使用handler之外，也有一些queue类的成员函数可以用来提交工作，可以作为简化代码的快捷方式，如下面的代码所示。class queue {public: // Submit a memset operation writing to the specified pointer. // Return an event representing this operation. event memset(void *ptr, int value, size_t count); // Submit a memcpy operation copying from src to dest. // Return an event representing this operation. event memcpy(void *dest, const void *src, size_t count); // Submit different forms of kernel for execution. // Return an event representing the kernel operation. template &lt;typename KernelName, typename KernelType&gt; event single_task(KernelType kernel); template &lt;typename KernelName, typename KernelType, int Dims&gt; event parallel_for(range&lt;Dims&gt; num_work_items, KernelType kernel); template &lt;typename KernelName, typename KernelType, int Dims&gt; event parallel_for(nd_range&lt;Dims&gt; execution_range, KernelType kernel); // Submit different forms of kernel for execution. // Wait for the specified event(s) to complete // before executing the kernel. // Return an event representing the kernel operation. template &lt;typename KernelName, typename KernelType&gt; event single_task(const std::vector&lt;event&gt;&amp; events, KernelType kernel); template &lt;typename KernelName, typename KernelType, int Dims&gt; event parallel_for(range&lt;Dims&gt; num_work_items, const std::vector&lt;event&gt;&amp; events, KernelType kernel); template &lt;typename KernelName, typename KernelType, int Dims&gt; event parallel_for(nd_range&lt;Dims&gt; execution_range, const std::vector&lt;event&gt;&amp; events, KernelType kernel);};动作之前提到的parallel_for定义了要在一个设备上执行的工作，parallel_for属于提交到一个队列的命令组（command group, CG），在命令组中，由两类代码：动作调用（等待队列执行设备代码或进行手动内存操作）与主机代码（负责设置依赖关系），具体如下图所示。调用设备代码或进行内存操作的动作一个命令组中只能调用上面的一个动作，且每次提交调用只能向队列提交一个命令组，因而可以生成任务图节点之间的依赖关系。提交设备代码上图中存在三类代码：主机代码、命令组内的主机代码、动作。数据管理有两种方式管理数据：统一共享内存（unified shared memory, USM）和缓冲区，前者基于指针，后者提供了更高层次的抽象。由于我们的代码可能在多个设备上运行，而这些设备不一定共享内存，所以我们需要管理数据移动。即使数据是共享的（如USM），同步和一致性也是我们需要理解和管理的。我们把对加速器内部存储器的访问称为本地访问。对另一个设备的存储器的访问称为远程访问，远程访问往往比本地访问要慢。管理多个存储器管理多个存储器可以通过两种方式完成：在程序中显式管理或在运行时隐式管理。前者的优点是可以完全控制数据在不同存储器之间传输的时机，缺点在于这样的过程可能很繁琐且容易出错；后者的优点是数据移动的工作由运行时系统自动完成，可以保证正确性，但缺点就是性能较差。USM、缓冲区、Images有三种管理内存的抽象方法：USM、缓冲区、Images。USM基于指针，对程序员比较熟悉；缓冲区有自己的模板类，可以描述一维、二维或三维数组，通过accessor对象进行访问；Images是特殊的缓冲区，提供了专门用于图像处理的额外功能。由于缓冲区和Images的接口基本相同，我们主要关注前两者。USM支持USM的设备有一个统一的虚拟地址空间，也就是说任何由主机上的USM动态内存分配函数返回的指针也可以被设备访问。USM定义了三种不同类型的内存分配：设备、主机和共享。所有类型的分配都在主机上进行，三种类型的区别如下图所示。USM内存分配类型缓冲区缓冲区是一个数据抽象，代表一个或多个特定C++类型的对象，缓冲区对象的元素可以是标量数据类型（如int、float或double）、向量数据类型，或用户定义的类或struct。缓冲区代表数据对象，而不是具体的内存地址，所以不能像常规的C++数组那样被直接访问，而需要用accessor对象进行访问。缓冲区访问模式在创建accessor时，我们可以告知运行时我们将如何使用缓冲区，以便为优化提供更多信息，默认为read_write。访问数据的顺序Kernel可以看作是提交执行的异步任务，任务之间可能存在数据依赖的关系，数据依赖也会产生额外的数据传输。我们把一个程序中的所有任务和它们之间存在的依赖关系可视化为一个图，即任务图，是一个有向无环图（DAG）。任务的完成可以是顺序的，也可以是乱序的。顺序队列队列默认是乱序的，因而必须提供方法来排序提交的任务，我们需要说明任务之间的依赖关系来进行排序，这些依赖关系可以使用命令组（CG）显式或隐式定义。使用events和depends_on隐式的数据依赖也可能来自对缓冲区通过accessor的访问，依赖关系如下图所示。三种数据依赖关系" }, { "title": "CONNECT AXI Wrapper Documentation (Draft)", "url": "/posts/CONNECT-AXI-DOCS/", "categories": "CALCM, CONNECT", "tags": "", "date": "2022-06-16 22:00:00 +0000", "snippet": "IntroductionThe AXI4[-Stream] wrapper is built over the CONNECT NoC, which is a network generator targeting FPGA.Getting StartedFirst, clone the CONNECT repository and build a sample network. TODO...", "content": "IntroductionThe AXI4[-Stream] wrapper is built over the CONNECT NoC, which is a network generator targeting FPGA.Getting StartedFirst, clone the CONNECT repository and build a sample network. TODO: Replace the git repository address in the future.$ git clone https://github.com/shili2017/connect.git$ cd connect$ python gen_network.py -t double_ring -w 38 -n 4 -v 2 -d 4 --gen_rtl --gen_graphThe network will be generated and put in the build directory. Then we can run the sample test bench.$ cp Prims/run_modelsim.sh build/$ cd build$ ./run_modelsim.shThe default sample test bench is for AXI4-Stream protocol. To test on AXI4 standard protocol, in run_modelsim.sh, replace CONNECT_testbench_sample_axi4stream by CONNECT_testbench_sample_axi4.To change (a) flit width or (b) number of virtual channels or (c) flit buffer depth, first re-generate the network.$ python gen_network.py -t double_ring -w &lt;flit_width&gt; -n 4 -v &lt;num_vcs&gt; -d &lt;flit_buffer_depth&gt; --gen_rtl --gen_graphThen change the parameters in connect_parameters.v.`define NUM_VCS &lt;num_vcs&gt;`define FLIT_DATA_WIDTH &lt;flit_depth&gt;`define FLIT_BUFFER_DEPTH &lt;flit_buffer_depth&gt; Do not change number of send or receive ports for now. So far the wrapper is still written manually, and only supports 4 terminals in the network. TODO: Consider generating the network wrapper automatically with scripts. To prevent potential deadlocks, AXI4 standard protocol requires at least 2 virtual channels, one for requests and the other for responses, and responses have higher priority. Meanwhile, AXI4-Stream only needs 1 virtual channel.Overall DesignThe overall design is shown in the following blocks.AXI4: CLK_FPGA | CLK_NOC---------------------------------------------------------------------------------------------------------------------+--------+ +------------+ +--------------+ +-------------+ +---------+| Device | &lt;--AXI4--&gt; | AXI4Bridge | ---flits--&gt; | Serializer | ---flits--&gt; | InPortFIFO | ---flits--&gt; | |+--------+ +------------+ 97 bits +--------------+ n bits +-------------+ n bits | | ^ | Network | | +--------------+ +-------------+ | | +-----------flits--- | Deserializer | &lt;--flits--- | OutPortFIFO | &lt;--flits--- | | 97 bits +--------------+ n bits +-------------+ n bits +---------+AXI4-Stream: CLK_FPGA | CLK_NOC-----------------------------------------------------------------------------------------------------------------------------------+---------------+ +------------+ +--------------+ +-------------+ +---------+| Master Device | ---AXI4-Stream--&gt; | AXI4Bridge | ---flits--&gt; | Serializer | ---flits--&gt; | InPortFIFO | ---flits--&gt; | |+---------------+ +------------+ 106 bits +--------------+ n bits +-------------+ n bits | | | Network |+---------------+ +------------+ +--------------+ +-------------+ | || Slave Device | &lt;--AXI4-Stream--- | AXI4Bridge | &lt;--flits--- | Deserializer | &lt;--flits--- | OutPortFIFO | &lt;--flits--- | |+---------------+ +------------+ 106 bits +--------------+ n bits +-------------+ n bits +---------+Related Source Files NetworkAXI4Wrapper.sv NetworkAXI4StreamWrapper.sv AXI4[-Stream] Bridge DesignAn AXI4 bridge is used to convert a pair of send &amp; recv port to a standard AXI4 protocol interface, either as a master device or a slave device. For example, for a network with 4 endpoints, we can have 2 AXI4 master devices and 2 AXI4 slave devices.An AXI4-Stream bridge is used to convert a send or recv port to a AXI4-Stream protocol master or slave interface. For example, for a network with 4 endpoints, we can have 4 AXI4-Stream master devices and 4 AXI4-Stream slave devices.Related ParametersParameters are defined in AXI4Package.sv.parameter ADDR_WIDTH = 32;parameter DATA_WIDTH = 64;parameter STRB_WIDTH = DATA_WIDTH / 8;parameter ID_WIDTH = 8;parameter USER_WIDTH = 8;parameter KEEP_WIDTH = DATA_WIDTH / 8;parameter DEST_WIDTH = 4;Users also need to calculate the standard AXI4[-Stream] flit data width by themselves. For example, with the default parameters, we haveparameter AXI4_FLIT_DATA_WIDTH = 76;parameter AXI4S_FLIT_DATA_WIDTH = 93;AXI4AXI4 requests and responses are encoded in a single AXI4 flit. A standard AXI4 request or response with the aforementioned parameters is encoded into a 92-bit flit. The 3 most significant bits represent for the channel flag for the 5 AXI4 channels. User devices are required to mark the source ID in AXI4 ID field, and AXI4 ID field is put in the LSB part in the flit to be identified by the serializer &amp; deserializer.AXI4 requests are sent in virtual channel 1, and AXI4 responses are sent in virtual channel 0, which has a higher priority to prevent potential deadlocks.AW/AR ChannelAXI_x - [75 ]axid - [74 : 67]------ padding ------axregion - [60 : 57]axqos - [56 : 53]axprot - [52 : 50]axcache - [49 : 46]axlock - [45 ]axburst - [44 : 43]axsize - [42 : 40]axlen - [39 : 32]axaddr - [31 : 0]W ChannelAXI_W - [75 ]------ padding ------wlast - [72 ]wstrb - [71 : 64]wdata - [63 : 0]B ChannelAXI_W - [75 ]bid - [74 : 67]------ padding ------bresp - [65 : 64]------ padding ------R ChannelAXI_R - [75 ]rid - [74 : 67]rlast - [66 ]rresp - [65 : 64]rdata - [63 : 0]AXI4-StreamAXI4-Stream requests are encoded in a single AXI4-Stream flit. A AXI4-Stream request with the aforementioned parameters is encoded into a 101-bit flit. User devices are required to mark the source ID in AXI4-Stream ID field, and AXI4-Stream ID field is put in the LSB part in the flit to be identified by the serializer &amp; deserializer.T Channeltdest - [92 : 89]tid - [88 : 81]tlast - [80 ]tkeep - [79 : 72]tstrb - [71 : 64]tdata - [63 : 0]Related Source Files AXI4Bridge.sv AXI4StreamBridge.sv AXI4Interface.sv AXI4Package.sv Serializer &amp; Deserializer DesignTo handle mismatched sizes of AXI4[-Stream] protocol flits and actual flits to be sent, a serializer &amp; deserializer for each master &amp; slave device is needed. For example, for a AXI4-Stream request flit of 101+5 bits (5 is metadata, including valid bit, tail bit, etc.) and actual flit width of 38+5 bits, a single request needs to be split into 3 small flits by the serializer on the sender side and then small flits can be reassembled to a complete AXI request on the receiver side.What makes things more complicated is that in the case that two master devices send data to a single slave device, flits may interleave (when virtual link is not enabled in CONNECT, or in general NoCs). Thus we need to encode source ID into the small flit such that the deserializer can distinguish interleaving flits. Fortunately, both AXI4 and AXI4-Stream supports interleaving requests and responses on the protocol level, so we don’t need to store the whole packet, whose size is unknown at the beginnning. Namely, for every single transfer in AXI4[-Stream], we set the tail bit to 1.For the serializer, the number of flits is ceil(IN_FLIT_DATA_WIDTH / OUT_FLIT_EFF_DATA_WIDTH), where OUT_FLIT_EFF_DATA_WIDTH = OUT_FLIT_DATA_WIDTH - SRC_BITS. Similarly, for the deserializer, the number of flits is ceil(OUT_FLIT_DATA_WIDTH / IN_FLIT_EFF_DATA_WIDTH), where IN_FLIT_EFF_DATA_WIDTH = IN_FLIT_DATA_WIDTH - SRC_BITS. The structure of small flit is shown as follows, ----------------------------------------------------------------------------------- | &lt;valid bit&gt; | &lt;is_tail&gt; | &lt;destination&gt; | &lt;virtual channel&gt; | &lt;source&gt; | &lt;data&gt; | ----------------------------------------------------------------------------------- 1 1 `DEST_BITS `VC_BITS `SRC_BITS OUT_FLIT_EFF_DATA_WIDTHTo handle interleaving flits, currently we adopt only a simple solution, as we know that flits are in-order and we know the number of senders. We make a table with one row per source ID and just fill in the table with successive flits until the tail flit arrives. At that moment the flits can be reassembled into a AXI4 flit and be processed by a later stage. We also implement a priority encoder as an arbiter to handle multiple valid reassembled flits in a single cycle. See the code below.// Use a priority encoder to decide which flit to sendlogic [`SRC_BITS - 1 : 0] out_idx;always_comb begin out_idx = 0; for (int i = `NUM_USER_SEND_PORTS - 1; i &gt;= 0; i--) begin if (out_valid[i]) out_idx = i; endendalways_comb begin for (int i = 0; i &lt; `NUM_USER_SEND_PORTS; i++) begin out_ready[i] = 0; out_flit = 0; end out_ready[out_idx] = out_flit_ready; out_flit = {flit_meta_reg[out_idx], flit_data_reg[out_idx][OUT_FLIT_DATA_WIDTH - 1 : 0]};endRelated Source Files FlitSerializer.svFlow Control Layer DesignCredit-based flow control is supported in CONNECT and the wrapper. To support flow control, a synchronous FIFO, i.e., SCFIFO, is implemented as a buffer between the user devices (or more accurately, the serailizer or deserializer) and the network. On the sender side, we need to maintain a credit counter. Each time we send a flit, we need to decrement the counter, and each time we receive a credit from the network, we increment the counter. On the receiver side, each time we dequeue a flit from the FIFO to the user device, we send a credit to the network.The network itself can run at a higher clock frequency as ASIC, and the remaining FPGA logic may run at a lower clock frequency. Thus a FIFO with different read and write clock, i.e., DCFIFO, is also needed. The block diagram of a FIFO is shown below.InPortFIFO: (WR) CLK_FPGA CLK_NOC (RD) CLK_NOC | | | v v v +---------------+ +--------+ +--------+ | | ---send_ports_putFlit_flit_in--&gt;---put_flit--------&gt; | | ---sc_enq_data---&gt; | | ---deq_flit_data---&gt; | Combinational | ---EN_send_ports_putFlit-------&gt;---put_flit_valid--&gt; | DCFIFO | ---sc_enq_valid--&gt; | SCFIFO | ---deq_flit_valid--&gt; | Logic | &lt;--send_ports_getCredits--------&lt;--put_flit_ready--- | | &lt;--sc_enq_ready--- | | &lt;--deq_flit_ready--- | | ---EN_send_ports_getCredits----&gt; +--------+ +--------+ +---------------+OutPortFIFO: (RD) CLK_FPGA CLK_NOC (WR) CLK_NOC | | | v v v +---------------+ +--------+ +--------+ | | &lt;--recv_ports_getFlit------------&lt;--get_flit--------- | | &lt;--sc_deq_data---- | | &lt;--enq_flit_data---- | Combinational | ---EN_recv_ports_getFlit--------&gt;&lt;--get_flit_valid--- | DCFIFO | &lt;--sc_deq_valid--- | SCFIFO | &lt;--enq_flit_valid--- | Logic | ---recv_ports_putCredits_cr_in--&gt;---get_flit_ready--&gt; | | ---sc_deq_ready--&gt; | | ---enq_flit_ready--&gt; | | ---EN_recv_ports_putCredits-----&gt; +--------+ +--------+ +---------------+ TODO: 1) Call AXI4 flit a packet. 2) One FIFO for each VC. 3) Add one more VC for W channel. 4) Do read and write in the same cycle? 5) #rows in deserializer should be #src * #vc. 6) One DCFIFO, handle credits approximately. 7) Serializer &amp; deserializer should be close to NoC.Related ParametersParameters of both DCFIFO and SCFIFO can be modified by users.SCFIFO:parameter DEPTH = `FLIT_BUFFER_DEPTH;parameter DATA_WIDTH = `FLIT_WIDTH;parameter ALMOST_FULL_VALUE = `FLIT_BUFFER_DEPTH - 1;parameter ALMOST_EMPTY_VALUE = 1;DCFIFO:parameter DEPTH = `FLIT_BUFFER_DEPTH;parameter DATA_WIDTH = `FLIT_WIDTH;Related Source Files FlitFIFO.svTestbenchTo test the AXI4[-Stream] wrapper, we also need AXI4[-Stream] devices to send requests or receive responses. We also include testbenches to verify the correctness of AXI4[-Stream] wrapper design. See related source files.Related Source Files AXI4Device.sv AXI4StreamDevice.sv testbench_sample_axi4.sv testbench_sample_axi4stream.sv " }, { "title": "《内存一致性与缓存一致性》笔记（五）：宽松内存一致性模型（下）", "url": "/posts/MCCC5/", "categories": "读书笔记, 内存一致性与缓存一致性", "tags": "", "date": "2022-06-15 22:13:00 +0000", "snippet": "之前我们讨论了宽松内存一致性模型的一些基本概念，并介绍了XC内存模型，在这一节中我们继续深入讨论宽松内存模型。宽松内存模型案例研究：RISC-V弱内存一致性模型（RVWMO）RVWMO可以理解为RC和XC的混合体。和XC一样，RVWMO是根据全局内存顺序来定义的，并且有几种FENCE指令的变体。和RC一样，load和store可以携带一些属性：load指令可以携带ACQUIRE，存储指令可以...", "content": "之前我们讨论了宽松内存一致性模型的一些基本概念，并介绍了XC内存模型，在这一节中我们继续深入讨论宽松内存模型。宽松内存模型案例研究：RISC-V弱内存一致性模型（RVWMO）RVWMO可以理解为RC和XC的混合体。和XC一样，RVWMO是根据全局内存顺序来定义的，并且有几种FENCE指令的变体。和RC一样，load和store可以携带一些属性：load指令可以携带ACQUIRE，存储指令可以携带RELEASE，RMW指令可以携带RELEASE、ACQURE或两者同时。RELEASE/ACQUIRE顺序ACQUIRE分为ACQUIRE-RC_PC和ACQUIRE-RC_SC，RELEASE分为RELEASE-RC_PC和RELEASE-RC_SC。load/store可以携带任意一种ACQURE/RELEASE，而RMW智能携带RC_SC。我们保证以下顺序： ACQUIRE -&gt; Load, Store Load, Store -&gt; RELEASE RELEASE-RC_SC -&gt; ACQUIRE-RC_SC FENCE顺序有一个强FENCE指令，即FENCE RW, RW，和XC中的FENCE一样。还有其他五种组合，包括： FENCE RW, W FENCE R, RW FENCE R, R FENCE W, W FENCE.TSO RVWMO：FENCE和RELEASE-ACQUIRE顺序保证了r1和r2不会同时为0语法依赖引起的顺序RVWMO在某些方面的约束比XC强，如地址、数据和控制依赖可以约束RVWMO中的内存顺序。地址依赖引起的顺序：是否可能r1=%data2且同时r2=0？数据依赖引起的顺序：是否可能r1=42且同时r2=42（42是load推测值）？控制依赖引起的顺序：是否可能r1=42，导致S1间接影响自己是否执行？在RVWMO中，以上三个问题的答案都是不可能。需要注意的是，上述所有依赖关系都是指语法上的依赖关系，而非语义上的依赖关系，也就是说是否存在依赖关系由寄存器决定，而非具体的值决定。流水线依赖除此以外，RVWMO还有流水线依赖关系，上图与以下内容摘自gitee上的一篇blog。 这四段特殊的代码范例都是前两条指令构成语法依赖，第三条指令进行相关的load或不知是否相关的store。RISC-V根据“几乎所有真实CPU流水线执行机构的行为”，将指令a和c的关系称为流水线依赖，并明确规定不能乱序。 约束代码1和2的出发点是“在store地址或值未知时，不能（无法）load这个store的值”：b的地址或值未确定时，不能执行c，又因为b的地址或数据依赖a，因此c在全局内存顺序上不能在a之前。 约束代码3和4的出发点是“前面load或store地址未知时不能store”：b地址未确定时c不能执行，以防止地址冲突，又因为b的地址依赖a，因此c在全局内存顺序上也不能在a之前。同一地址顺序在XC中，对同一地址的内存访问遵循TSO顺序。类似于XC，RVWMO对同一地址的访问同样要求Load -&gt; Store和Store -&gt; Store，不过在任何情况下都不要求Load -&gt; Load顺序。不过关于Load -&gt; Load顺序，同样引用自之前的blog，有以下需要注意的点。 对于同一地址的两个load，只要后一个load不会读到更旧的值，就不约束两者的内存顺序，这个特性称为Coherence for Read-Read pairs。反过来说，当且仅当两个load中间没有对这一地址的store，且两个load返回不同的值时（实际上是返回不同的store，不一定值不相同），要保证两个load不乱序。RMWRISC-V支持两种类型的RMW操作：原子内存指令（AMO）和lr/sc（保留读和条件写）。不同的是，前者只是一条指令，而后者两条指令通常组合使用。比较不同的内存模型我们先前讨论过如何比较不同的内存模型。我们发现，RVWMO比TSO宽松，而TSO又比SC宽松。不过不同的宽松内存模型可能无法比较，如Alpha、ARM、RVWMO之间无法进行比较。宽松内存模型怎么样？ 可编程性：我们引入了DRF for SC的概念，宽松内存模型的可编程性还是可以接受的，不过深入理解可能比较困难（如对于编译器工程师而言）。 性能：宽松的内存模型可以比TSO有更好的性能，但是对于有许多核心的处理器来说，两者之间的差异比较小。 可移植性：在保证DRF for SC的前提下，可以比较容易地进行移植，但在一般情况下，尤其是在无法比较的内存模型之间进行移植可能是比较困难的。 精确：很多宽松内存模型没有被正式精确定义，或者定义很模糊。 高级语言模型高级语言与硬件的内存模型为高级语言定义一套内存模型也同样重要，我们需要规定： 使用高级语言的程序员应该期望得到什么样的结果 编译器、运行时系统与硬件应该怎样实现 Java可能是第一个包含了内存模型的主流高级语言，程序员可以使用一些高级语言的特性来避免数据竞争。寄存器分配会影响内存顺序编译器可能会对内存指令进行重新排序，或消除一些内存访问，如上图中的例子所示。但是，假设由另一个核心C2可以更新A和C的值，C1只能观测到C的更新，这种执行方式在高级语言级别违反了SC。更一般地来说，一个线程可以观测到数据竞争或者非SC操作的条件是什么？Java要求所有程序都有安全保证，但C++出于性能的考虑，支持各种比SC更弱的同步操作。Java和C++必须按照以下目标对所有情况规定具体的行为： 允许针对所有高性能DRF程序的优化 明确规定所有程序的允许行为 这种明确规定尽量简单 如何比较好地满足上述三个目标仍然是一个开放性的问题。 测验问题5：相对于高级语言的一致性模型（如Java），程序员编写适当的同步代码，不需要考虑架构的内存一致性模型。 答：不一定。对通常的应用程序来说，确实不需要考虑架构级别的内存模型，但对编译器或运行时系统的程序员来说，就需要仔细考虑了。" }, { "title": "《内存一致性与缓存一致性》笔记（四）：宽松内存一致性模型（上）", "url": "/posts/MCCC4/", "categories": "读书笔记, 内存一致性与缓存一致性", "tags": "", "date": "2022-06-13 15:07:00 +0000", "snippet": "前两章我们讨论了两种内存一致性模型：SC和TSO，这两种模型的全局内存顺序通常都保留了每个线程的程序顺序。在这一章中，我们讨论更宽松的（更弱的）一致性模型，目的在于只保留程序员真正需要的顺序，通过允许更激进的优化（编译器或运行时）减少排序约束，进一步提升性能。缺点也很明显，需要显式地说明什么时候需要排序，并需要处理器提供相关机制，可能会影响可移植性。动机对内存指令进行重排的机会什么样的顺序可...", "content": "前两章我们讨论了两种内存一致性模型：SC和TSO，这两种模型的全局内存顺序通常都保留了每个线程的程序顺序。在这一章中，我们讨论更宽松的（更弱的）一致性模型，目的在于只保留程序员真正需要的顺序，通过允许更激进的优化（编译器或运行时）减少排序约束，进一步提升性能。缺点也很明显，需要显式地说明什么时候需要排序，并需要处理器提供相关机制，可能会影响可移植性。动机对内存指令进行重排的机会什么样的顺序可以保证r2和r3始终得到NEW的值？我们期望r2和r3总是得到NEW的值，因而需要如下顺序： S1 -&gt; S3 -&gt; L1 loads SET -&gt; L2 S2 -&gt; S3 -&gt; L1 loads SET -&gt; L3 除了以上顺序之外，SC和TSO还会额外保证S1和S2以及L2和L3的顺序。什么样的顺序可以保证CS1到2的顺利过渡？上图描述了一个更普遍的情况，即两个critical section（以下简称CS）使用同一个锁进行交接，其正确性由以下顺序进行保证： All L1i, All S1j -&gt; R1 -&gt; A2 -&gt; All L2i, All S2j注意正确性并不取决于CS内的load和store的顺序（除非load和store的地址相同）。在这种情况下，或许可以通过放松load和store之间的顺序来获取更高的性能。利用重排的机会我们现在假设有一个宽松的内存模型，我们可以对任何内存操作进行重排，并由FENCE指令进行必要的排序。这种模型需要程序员手动推理哪些操作需要排序，但也可以进行很多优化以提高性能，我们现在讨论一些常见的优化方法。不基于FIFO的合并式Write Buffer在TSO中我们要求write buffer必须是基于FIFO的，但一个更优化的设计是使用不基于FIFO的write buffer，且允许同一地址或相邻地址的多个store请求合并为一个store。对乱序内核的支持更简单在强一致性模型的系统中，一个内核可能在指令提交之前推测性地执行不按程序顺序的load，如MIPS R10000，但这样需要额外的检查流程，如MIPS R10000通过比较evict的缓存块的地址和内核已经推测性load但还未提交的地址来进行检查。相比之下，在宽松一致性模型中，内核可以不按程序顺序进行load，且不需要额外的检查。耦合的内存一致性与缓存一致性我们之前提倡将两个概念解耦，降低理解的复杂性，但宽松一致性模型通过“打开缓存一致性的黑盒”来提供更好的性能。例如，我们可以允许多处理器中一部分内核从之前的store请求中load新的值，另一部分仍然可以load旧的值。这种情况在现实中是有可能发生的，如一个内核上运行两个线程，共享一个write buffer，或者两个内核共享同一个L1 data cache。宽松内存模型：XC我们虚构了一个宽松一致性模型，称为XC。XC假设存在一个全局的内存顺序。XC模型的基本概念XC提供了一个FENCE指令，在没有FENCE指令的情况下，load和store都可以是乱序的。FENCE指令不指定地址，同一个核的FENCE指令之间严格顺序，但FENCE不影响其他内核的内存指令顺序。现实中的FENCE可能会附带有更多的属性，例如可以保证所有内存顺序，但是除了FENCE之前的store与之后的load。XC内存顺序保证了以下程序顺序： Load -&gt; FENCE Store -&gt; FENCE FENCE -&gt; FENCD FENCE -&gt; Load FENCE -&gt; Store XC仅对同一地址的两个内存访问请求维护TSO规则： Load -&gt; Load to same address Load -&gt; Store to same address Store -&gt; Store to same address 对同一地址的顺序约束还是有必要的，比如Store -&gt; Store规则可以避免出现先执行A=1再执行A=2但结果是A=1的情况，或者Load -&gt; Load规则可以避免出现一开始A=0且有一个处理器写入A=1时，另一个处理器上第一次load A结果为1但是第二次load A结果为0的情况。XC保证了load立即看到自己的store请求，类似TSO的write buffer bypassing，这样保证了单线程的顺序性。XC模型中使用FENCE的例子在XC中使用FENCE在XC中通过FENCE来实现CS之间的过渡XC的形式化描述XC的执行需要如下要求： 所有内核都按如下规则将load、store和FENCE插入内存顺序&lt;m： 若L(a) &lt;p FENCE，则L(a) &lt;m FENCE 若S(a) &lt;p FENCE，则S(a) &lt;m FENCE 若FENCE &lt;p FENCE，则FENCE &lt;m FENCE 若FENCE &lt;p L(a)，则FENCE &lt;m L(a) 若FENCE &lt;p S(a)，则FENCE &lt;m S(a) 所有内核都将相同地址的load和store请求按如下顺序插入内存顺序&lt;m： 若L(a) &lt;p L'(a)，则L(a) &lt;m L'(a) 若L(a) &lt;p S(a)，则L(a) &lt;m S(a) 若SL(a) &lt;p S'(a)，则S(a) &lt;m S'(a) 每一次load都从之前最后一次相同地址的store中获取值： L(a)的值为MAX_&lt;m {S(a) | S(a) &lt;m L(a) or S(a) &lt;p L(a)} XC顺序规则（X表示强制顺序，B表示访问同一地址需要bypassing，A表示只有访问同一地址才需要保证顺序）XC如何保证程序正确执行第一个例子中两种等价的执行方式第二个例子中两种等价的执行方式实现XC我们采用了和SC或TSO类似的方法，也就是把内存访问请求的顺序与缓存一致性分开。在TSO中，每个内核都被一个基于FIFO的write buffer与共享内存分开，而对XC来说，这个write buffer由更一般的重排单元分开。两种XC的实现具体的细节和SC或TSO类似，就不详细展开了。那么，从TSO到XC这样的模型会带来多大的性能提升呢？这取决于具体的微架构实现，如之前讨论过的不基于FIFO的合并式write buffer或其他优化方法等。实现原子指令有几种可行的方式来实现原子RMW指令，RMW的实现也取决于系统如何实现XC。在本节中，我们假设XC系统由乱序内核组成，每个内核通过一个不基于FIFO的合并式write buffer与内存系统相连。一个简单可行的解决方案是借用我们在TSO中讨论过的实现方式，在执行一个原子指令之前，清空write buffer，然后获得缓存块的读写权限（如M状态），然后进行load和store。但这种方案过于保守，牺牲了一部分性能。我们事实上不需要清空write buffer，因为XC允许RMW的load和store可以直接bypass之前的store。TSO与XC实现同步的区别上图描述了TSO和XC实现CS的区别，在XC中，获取锁之后与释放锁之前都需要插入FENCE。实现FENCE有三种基本的实现方法： 实现SC，并将FENCE视为no-ops。 等待FENCE之前的所有内存操作完成，然后开始FENCE之后的内存操作。 在前一种方法基础上实现更激进的优化，在保证顺序的前提下执行更多指令。 无数据竞争程序的顺序一致性（SC for DRF）程序员希望既可以用简单的SC模型进行推理，又可以获得宽松模型的高性能。对于无数据竞争（data-race-free, DRF）程序而言，可以同时实现这两个目标。数据竞争的概念是，当两个线程访问同一个内存位置，且至少有一个访问是store，且中间没有同步操作。在SC for DRF中，程序员需要通过编写正确的同步指令来确保程序在SC下是DRF的，同时在架构的实现中，将同步指令映射到宽松一致性模型中的FENCE和RMW指令，实现类似SC的执行效果。XC和大多数商用的宽松内存模型都有必要的FENCE指令和RMW指令来保证SC的效果。数据竞争的例子从上面的例子中我们可以看出，插入必要的FENCE之后，XC的结果与SC的结果是完全一样的。我们通过这个例子进一步讨论SC for DRF。 如果存在数据竞争，那么就会暴露XC对load和store的重排。 如果不存在数据竞争，那么XC和SC执行结果没有区别。 如果需要更深入理解SC for DRF，需要以下一些定义。 一些内存操作标记为“同步”，包括锁的获取和释放，其余的内存操作标记为“数据”。 如果两个数据操作Di和Dj来自不同的内核或不同的线程，且访问相同的内存位置，并且至少有一个是store，那么这两个数据操作发生冲突。 如果两个同步操作Si和Sj来自不同的内核或不同的线程，且访问相同的内存位置（如相同的锁)，并且至少有一个同步操作是store（例如，自旋锁的获取和释放会发生冲突，而读写锁上的两个读操作不会发生冲突)，那么这两个同步操作发生冲突。 两个同步操作Si和Sj在以下两种情况发生过渡冲突： Si和Sj发生冲突，或 Si与某个同步操作Sk发生冲突，且Sk &lt;p Sk'（即Sk在内核K的程序顺序中在Sk’之前)，且Sk’与Sj发生冲突。 两个数据操作Di和Dj发生数据竞争的条件是： Di和Dj发生冲突，且 在全局内存顺序中，Di和Dj之间没有过渡冲突的一对由i和j发出的同步指令（也就是说，一对冲突的数据操作Di &lt;m Dj不属于数据竞争，当且仅当存在一对相互冲突的同步操作Si和Sj，使得Di &lt;m Si &lt;m Sj &lt;m Dj）。 如果没有数据操作的竞争，我们就称SC执行是无数据竞争的（DRF）。 如果一个程序的所有SC执行都是DRF的，那么这个程序是DRF的。 如果所有DRF程序的执行都是SC的执行，那么这个内存模型就支持SC for DRF，这种支持通常需要特别的同步操作。 可以证明XC支持SC for DRF。为DRF程序支持SC可以让程序员直白地使用SC来进行推理，无需记住复杂的规则，同时也可以获得性能的提升，但问题在于要保证DRF程序的高性能（也就是不要把太多的操作标记为同步）也可能很困难。插入FENCE的最优位置是不确定的如上图所示，很难准确描述哪些内存操作可能发生数据竞争，因而必须保守地标记同步操作，插入FENCE。在极端情况中我们可能需要在所有内存操作之间插入FENCE以保证SC。同时，程序可能会因为bug而违反DRF，发生数据竞争，且在此之后可能不再遵守SC。综上所述，程序员可以使用两种选择来推理他们的程序： 直接使用XC的原始规则 插入充足的同步指令以确保没有数据竞争，然后用相对简单的SC来进行推理 学过计算机体系结构或者操作系统课程的同学会知道，后者的应用更普遍。一些宽松内存模型的概念释放一致性（Release Consistency）释放一致性的关键观点是，用FENCE包围所有的同步操作是多余的，对“获取锁”的同步只需要后面的FENCE，对“释放锁”的同步只需要前面的FENCE。我们回顾之前的一个例子来更清晰地说明这个概念，如下图所示。回顾之前的例子：在XC中通过FENCE来实现CS之间的过渡上图中，F11、F14、F21和F24可以省略。以C1的释放锁为例，F13不能省略，但F14可以，因为C1后续的内存操作在释放锁之前执行也是没有问题的。XC不允许有内存指令绕过FENCE，但RC允许这样的提前执行。事实上，RC提供了类似于FENCE的ACQUIRE和RELEASE指令，但只在一个方向上对内存访问进行排序，不同于在往前和王后两个方向进行排序的FENCE。更一般地来说，RC只要求： ACQUIRE -&gt; Load, Store（但没有Load, Store -&gt; ACQUIRE） Load, Store -&gt; RELEASE（但没有RELEASE -&gt; Load, Store） 以及ACQUIRE和RELEASE之间的SC顺序： ACQUIRE -&gt; ACQUIRE ACQUIRE -&gt; RELEASE RELEASE -&gt; ACQUIRE RELEASE -&gt; RELEASE 因果性和写入原子性因果性：如果我看观测到了store并通知了你，你是否也一定会观测到store？上图说明了因果性的一个例子。如果C3能保证观察到S1的完成，那么因果关系就成立，反之则不成立。写入原子性则要求一个内核的store在逻辑上被所有其他内核观测到。根据定义，XC符合写原子性，因为内存顺序规定了一个逻辑上的原子的点，在这一点上，store请求生效，在此之前，没有其他内核可以看到store请求，在此之后，所有其他内核都可以看到store之后的值。不过需要注意的是，写入原子性允许发出store请求的内核提前看到自己的store请求。IRIW的例子：store是否写入原子性的一个必要不充分条件是可以正确处理独立读独立写（Independent Read Independent Write, IRIW），如上图所示。假设C3的L1观测到S1，C4的L3观测到S2，是否有可能L2的结果和L4的结果同时为0？在这种情况下，S1和S2不仅是重新排序，而是违反了写入原子性。不过，正确处理IRIW并不能推导出写入原子性。写入原子性可以推出因果性，但因果性并不能推出写入原子性。后者的反例是，假设C1和C3两个线程共享一个write buffer，C2和C4两个线程共享一个write buffer，L1为NEW而L2可能为0，且L3为NEW而L4可能为0。" }, { "title": "CONNECT Note (6) - Serializer & Deserializer", "url": "/posts/CONNECT6/", "categories": "CALCM, CONNECT", "tags": "", "date": "2022-06-12 15:56:00 +0000", "snippet": "Source code: link.AXI4[-Stream] In FlitsTo handle mismatched sizes of AXI4 protocol flits and actual flits to be sent, a serializer &amp; deserializer for each master &amp; slave device is needed. ...", "content": "Source code: link.AXI4[-Stream] In FlitsTo handle mismatched sizes of AXI4 protocol flits and actual flits to be sent, a serializer &amp; deserializer for each master &amp; slave device is needed. For example, for a AXI4-Stream request flit of 101+5 bits (5 is metadata, including valid bit, tail bit, etc.) and actual flit width of 38+5 bits, a single request needs to be split into 3 small flits by the serializer on the sender side and then small flits can be reassembled to a complete AXI request on the receiver side.What makes things more complicated is that in the case that two master devices send data to a single slave device, flits may interleave (when virtual link is not enabled in CONNECT, or in more general NoCs). Thus we need to encode source ID into the small flit such that the deserializer can distinguish interleaving flits. Fortunately, both AXI4 and AXI4-Stream supports interleaving requests and responses on the protocol level, so we don’t need to store the whole packet, whose size is unknown at the beginnning. Namely, for every single transfer in AXI4[-Stream], we set the tail bit to 1.For the serializer, the number of flits is ceil(IN_FLIT_DATA_WIDTH / OUT_FLIT_EFF_DATA_WIDTH), where OUT_FLIT_EFF_DATA_WIDTH = OUT_FLIT_DATA_WIDTH - SRC_BITS. Similarly, for the deserializer, the number of flits is ceil(OUT_FLIT_DATA_WIDTH / IN_FLIT_EFF_DATA_WIDTH), where IN_FLIT_EFF_DATA_WIDTH = IN_FLIT_DATA_WIDTH - SRC_BITS. The structure of small flit is shown as follows, ----------------------------------------------------------------------------------- | &lt;valid bit&gt; | &lt;is_tail&gt; | &lt;destination&gt; | &lt;virtual channel&gt; | &lt;source&gt; | &lt;data&gt; | ----------------------------------------------------------------------------------- 1 1 `DEST_BITS `VC_BITS `SRC_BITS OUT_FLIT_EFF_DATA_WIDTHTo handle interleaving flits, currently we adopt only a simple solution, as we know that flits are in-order and we know the number of senders. We make a table with one row per source ID and just fill in the table with successive flits until the tail flit arrives. At that moment the flits can be reassembled into a AXI4 flit and be processed by a later stage. We also implement a priority encoder as an arbiter to handle multiple valid reassembled flits in a single cycle. See the code below.// Use a priority encoder to decide which flit to sendlogic [`SRC_BITS - 1 : 0] out_idx;always_comb begin out_idx = 0; for (int i = `NUM_USER_SEND_PORTS - 1; i &gt;= 0; i--) begin if (out_valid[i]) out_idx = i; endendalways_comb begin for (int i = 0; i &lt; `NUM_USER_SEND_PORTS; i++) begin out_ready[i] = 0; out_flit = 0; end out_ready[out_idx] = out_flit_ready; out_flit = {flit_meta_reg[out_idx], flit_data_reg[out_idx][OUT_FLIT_DATA_WIDTH - 1 : 0]};endHowever, if the flits arrive out-of-order, we need to have much more complex solutions, e.g., the reassembler to process out-of-order TCP packets in Pigasus.Block DiagramAXI4:+--------+ +------------+ +--------------+ +-------------+ +---------+| Device | &lt;--AXI4--&gt; | AXI4Bridge | ---flits--&gt; | Serializer | ---flits--&gt; | InPortFIFO | ---flits--&gt; | |+--------+ +------------+ 97 bits +--------------+ n bits +-------------+ n bits | | ^ | Network | | +--------------+ +-------------+ | | +-----------flits--- | Deserializer | &lt;--flits--- | OutPortFIFO | &lt;--flits--- | | 97 bits +--------------+ n bits +-------------+ n bits +---------+AXI4-Stream:+---------------+ +------------+ +--------------+ +-------------+ +---------+| Master Device | ---AXI4-Stream--&gt; | AXI4Bridge | ---flits--&gt; | Serializer | ---flits--&gt; | InPortFIFO | ---flits--&gt; | |+---------------+ +------------+ 106 bits +--------------+ n bits +-------------+ n bits | | | Network |+---------------+ +------------+ +--------------+ +-------------+ | || Slave Device | &lt;--AXI4-Stream--- | AXI4Bridge | &lt;--flits--- | Deserializer | &lt;--flits--- | OutPortFIFO | &lt;--flits--- | |+---------------+ +------------+ 106 bits +--------------+ n bits +-------------+ n bits +---------+" }, { "title": "CONNECT Note (5) - AXI4 Stream Interface", "url": "/posts/CONNECT5/", "categories": "CALCM, CONNECT", "tags": "", "date": "2022-06-08 14:54:00 +0000", "snippet": "Source code: link.AXI4-Stream In FlitsProtocol spec: AXI4-Stream.T Channeltuser - [100 : 93]tdest - [ 92 : 89]tid - [ 88 : 81]tlast - [ 80 : 80]tkeep - [ 79 : 72]tstrb - [ 71 : 64]tdata - [ 63 : ...", "content": "Source code: link.AXI4-Stream In FlitsProtocol spec: AXI4-Stream.T Channeltuser - [100 : 93]tdest - [ 92 : 89]tid - [ 88 : 81]tlast - [ 80 : 80]tkeep - [ 79 : 72]tstrb - [ 71 : 64]tdata - [ 63 : 0]Updated on 06/10, we need to unify AXI4-Stream and AXI4 id/user field.tlast - [100 :100]tkeep - [ 99 : 92]tstrb - [ 91 : 84]tdata - [ 83 : 20]tdest - [ 19 : 16]tuser - [ 15 : 8]tid - [ 7 : 0]Bridge DesignDifferent from the implementation of AXI4 protocol that each device requires both in and out ports of the network, each device of AXI4-Stream only needs one in port or one out port.AXI4:+--------+ +------------+ +-------------+ +---------+| Device | &lt;--AXI4--&gt; | AXI4Bridge | &lt;--flits--&gt; | InPortFIFO | &lt;--flits--&gt;| |+--------+ +------------+ +-------------+ | | ^ | Network | | +-------------+ | | +-----------flits--&gt; | OutPortFIFO | &lt;--flits--&gt;| | +-------------+ +---------+AXI4-Stream:+---------------+ +------------+ +-------------+ +---------+| Master Device | &lt;--AXI4-Stream--&gt; | AXI4Bridge | &lt;--flits--&gt; | InPortFIFO | &lt;--flits--&gt;| |+---------------+ +------------+ +-------------+ | | | Network |+---------------+ +------------+ +-------------+ | || Slave Device | &lt;--AXI4-Stream--&gt; | AXI4Bridge | &lt;--flits--&gt; | OutPortFIFO | &lt;--flits--&gt;| |+---------------+ +------------+ +-------------+ +---------+TestSee testbench_sample_axi4stream.sv for test bench.FIFOs Used In Tests Chisel3 FIFO. Intel FPGA FIFO IP. Result (Chisel3 FIFO)# ---- Performing Reset ----# 9: Port 0 send flit 2a00400ffffdeadbeef00000000 (tail=0 dst=2 vc=1)# 9: Device 0 send flit 2a00400ffffdeadbeef00000000# 9: [m0- t] addr=deadbeef00000000 dest=2# 10: Port 0 send flit 2a00400ffffdeadbeef00000001 (tail=0 dst=2 vc=1)# 10: Port 1 send flit 2e00602ffffdeadbeef00000000 (tail=0 dst=3 vc=1)# 10: Port 0 get a credit (vc=1)# 10: Device 0 send flit 2a00400ffffdeadbeef00000001# 10: Device 1 send flit 2e00602ffffdeadbeef00000000# 10: [m0- t] addr=deadbeef00000001 dest=2# 10: [m1- t] addr=deadbeef00000000 dest=3# 11: Port 0 send flit 2a00400ffffdeadbeef00000002 (tail=0 dst=2 vc=1)# 11: Port 1 send flit 2e00602ffffdeadbeef00000001 (tail=0 dst=3 vc=1)# 11: Port 2 send flit 2200002ffffdeadbeef00000000 (tail=0 dst=0 vc=1)# 11: Port 0 get a credit (vc=1)# 11: Device 0 send flit 2a00400ffffdeadbeef00000002# 11: Device 1 send flit 2e00602ffffdeadbeef00000001# 11: Device 2 send flit 2200002ffffdeadbeef00000000# 11: [m0- t] addr=deadbeef00000002 dest=2# 11: [m1- t] addr=deadbeef00000001 dest=3# 12: Port 0 send flit 2a00400ffffdeadbeef00000003 (tail=0 dst=2 vc=1)# 12: Port 1 send flit 2e00602ffffdeadbeef00000002 (tail=0 dst=3 vc=1)# 12: Port 2 send flit 2200002ffffdeadbeef00000001 (tail=0 dst=0 vc=1)# 12: Port 3 send flit 2600202ffffdeadbeef00000000 (tail=0 dst=1 vc=1)# 12: Port 2 recv flit 2a00400ffffdeadbeef00000000 (tail=0 dst=2 vc=1) # 12: Port 0 get a credit (vc=1)# 12: Port 1 get a credit (vc=1)# 12: Port 2 get a credit (vc=1)# 12: Port 2 put a credit (vc=1)# 12: Device 0 send flit 2a00400ffffdeadbeef00000003# 12: Device 1 send flit 2e00602ffffdeadbeef00000002# 12: Device 2 send flit 2200002ffffdeadbeef00000001# 12: Device 3 send flit 2600202ffffdeadbeef00000000# 12: Device 2 recv flit 2a00400ffffdeadbeef00000000# 12: [m0- t] addr=deadbeef00000003 dest=2# 12: [m1- t] addr=deadbeef00000002 dest=3# 13: Port 0 send flit 2a00400ffffdeadbeef00000004 (tail=0 dst=2 vc=1)# 13: Port 1 send flit 2e00602ffffdeadbeef00000003 (tail=0 dst=3 vc=1)# 13: Port 2 send flit 2200002ffffdeadbeef00000002 (tail=0 dst=0 vc=1)# 13: Port 3 send flit 2600202ffffdeadbeef00000001 (tail=0 dst=1 vc=1)# 13: Port 0 get a credit (vc=1)# 13: Port 1 get a credit (vc=1)# 13: Port 2 get a credit (vc=1)# 13: Port 3 get a credit (vc=1)# 13: Device 0 send flit 2a00400ffffdeadbeef00000004# 13: Device 1 send flit 2e00602ffffdeadbeef00000003# 13: Device 2 send flit 2200002ffffdeadbeef00000002# 13: Device 3 send flit 2600202ffffdeadbeef00000001# 13: [m0- t] addr=deadbeef00000004 dest=2# 13: [m1- t] addr=deadbeef00000003 dest=3# 14: Port 0 send flit 2a00400ffffdeadbeef00000005 (tail=0 dst=2 vc=1)# 14: Port 1 send flit 2e00602ffffdeadbeef00000004 (tail=0 dst=3 vc=1)# 14: Port 2 send flit 2200002ffffdeadbeef00000003 (tail=0 dst=0 vc=1)# 14: Port 3 send flit 2600202ffffdeadbeef00000002 (tail=0 dst=1 vc=1)# 14: Port 0 recv flit 2200002ffffdeadbeef00000000 (tail=0 dst=0 vc=1) # 14: Port 3 recv flit 2e00602ffffdeadbeef00000000 (tail=0 dst=3 vc=1) # 14: Port 0 get a credit (vc=1)# 14: Port 3 get a credit (vc=1)# 14: Port 0 put a credit (vc=1)# 14: Port 3 put a credit (vc=1)# 14: Device 0 send flit 2a00400ffffdeadbeef00000005# 14: Device 1 send flit 2e00602ffffdeadbeef00000004# 14: Device 2 send flit 2200002ffffdeadbeef00000003# 14: Device 3 send flit 2600202ffffdeadbeef00000002# 14: Device 0 recv flit 2200002ffffdeadbeef00000000# 14: Device 3 recv flit 2e00602ffffdeadbeef00000000# 14: [m0- t] addr=deadbeef00000005 dest=2# 14: [m1- t] addr=deadbeef00000004 dest=3# 15: Port 0 send flit 2a00400ffffdeadbeef00000006 (tail=0 dst=2 vc=1)# 15: Port 1 send flit 2e00602ffffdeadbeef00000005 (tail=0 dst=3 vc=1)# 15: Port 2 send flit 2200002ffffdeadbeef00000004 (tail=0 dst=0 vc=1)# 15: Port 3 send flit 2600202ffffdeadbeef00000003 (tail=0 dst=1 vc=1)# 15: Port 0 recv flit 2200002ffffdeadbeef00000001 (tail=0 dst=0 vc=1) # 15: Port 1 recv flit 2600202ffffdeadbeef00000000 (tail=0 dst=1 vc=1) # 15: Port 2 recv flit 2a00400ffffdeadbeef00000001 (tail=0 dst=2 vc=1) # 15: Port 3 recv flit 2e00602ffffdeadbeef00000001 (tail=0 dst=3 vc=1) # 15: Port 0 get a credit (vc=1)# 15: Port 1 get a credit (vc=1)# 15: Port 3 get a credit (vc=1)# 15: Port 1 put a credit (vc=1)# 15: Port 2 put a credit (vc=1)# 15: Device 0 send flit 2a00400ffffdeadbeef00000006# 15: Device 1 send flit 2e00602ffffdeadbeef00000005# 15: Device 2 send flit 2200002ffffdeadbeef00000004# 15: Device 3 send flit 2600202ffffdeadbeef00000003# 15: Device 1 recv flit 2600202ffffdeadbeef00000000# 15: Device 2 recv flit 2a00400ffffdeadbeef00000001# 15: [m0- t] addr=deadbeef00000006 dest=2# 15: [m1- t] addr=deadbeef00000005 dest=3# 15: [s0- t] addr=deadbeef00000000 dest=0# 16: Port 0 send flit 2a00400ffffdeadbeef00000007 (tail=0 dst=2 vc=1)# 16: Port 1 send flit 2e00602ffffdeadbeef00000006 (tail=0 dst=3 vc=1)# 16: Port 2 send flit 2200002ffffdeadbeef00000005 (tail=0 dst=0 vc=1)# 16: Port 3 send flit 2600202ffffdeadbeef00000004 (tail=0 dst=1 vc=1)# 16: Port 1 recv flit 2600202ffffdeadbeef00000001 (tail=0 dst=1 vc=1) # 16: Port 1 get a credit (vc=1)# 16: Port 2 get a credit (vc=1)# 16: Port 3 get a credit (vc=1)# 16: Port 0 put a credit (vc=1)# 16: Port 3 put a credit (vc=1)# 16: Device 0 send flit 2a00400ffffdeadbeef00000007# 16: Device 1 send flit 2e00602ffffdeadbeef00000006# 16: Device 2 send flit 2200002ffffdeadbeef00000005# 16: Device 3 send flit 2600202ffffdeadbeef00000004# 16: Device 0 recv flit 2200002ffffdeadbeef00000001# 16: Device 3 recv flit 2e00602ffffdeadbeef00000001# 16: [m0- t] addr=deadbeef00000007 dest=2# 16: [m1- t] addr=deadbeef00000006 dest=3# 16: [s1- t] addr=deadbeef00000000 dest=1# 17: Port 0 send flit 2a00400ffffdeadbeef00000008 (tail=0 dst=2 vc=1)# 17: Port 1 send flit 2e00602ffffdeadbeef00000007 (tail=0 dst=3 vc=1)# 17: Port 2 send flit 2200002ffffdeadbeef00000006 (tail=0 dst=0 vc=1)# 17: Port 3 send flit 2600202ffffdeadbeef00000005 (tail=0 dst=1 vc=1)# 17: Port 3 recv flit 2e00602ffffdeadbeef00000002 (tail=0 dst=3 vc=1) # 17: Port 3 get a credit (vc=1)# 17: Port 1 put a credit (vc=1)# 17: Device 0 send flit 2a00400ffffdeadbeef00000008# 17: Device 1 send flit 2e00602ffffdeadbeef00000007# 17: Device 2 send flit 2200002ffffdeadbeef00000006# 17: Device 3 send flit 2600202ffffdeadbeef00000005# 17: Device 1 recv flit 2600202ffffdeadbeef00000001# 17: [m0- t] addr=deadbeef00000008 dest=2# 17: [m1- t] addr=deadbeef00000007 dest=3# 17: [s0- t] addr=deadbeef00000001 dest=0# 18: Port 0 send flit 2a00400ffffdeadbeef00000009 (tail=0 dst=2 vc=1)# 18: Port 3 send flit 2600202ffffdeadbeef00000006 (tail=0 dst=1 vc=1)# 18: Port 0 recv flit 2200002ffffdeadbeef00000002 (tail=0 dst=0 vc=1) # 18: Port 1 recv flit 2600202ffffdeadbeef00000002 (tail=0 dst=1 vc=1) # 18: Port 2 recv flit 2a00400ffffdeadbeef00000002 (tail=0 dst=2 vc=1) # 18: Port 3 recv flit 2e00602ffffdeadbeef00000003 (tail=0 dst=3 vc=1) # 18: Port 0 get a credit (vc=1)# 18: Port 1 get a credit (vc=1)# 18: Port 3 get a credit (vc=1)# 18: Port 0 put a credit (vc=1)# 18: Port 2 put a credit (vc=1)# 18: Port 3 put a credit (vc=1)# 18: Device 0 send flit 2a00400ffffdeadbeef00000009# 18: Device 1 send flit 2e00602ffffdeadbeef00000008# 18: Device 2 send flit 2200002ffffdeadbeef00000007# 18: Device 3 send flit 2600202ffffdeadbeef00000006# 18: Device 0 recv flit 2200002ffffdeadbeef00000002# 18: Device 2 recv flit 2a00400ffffdeadbeef00000002# 18: Device 3 recv flit 2e00602ffffdeadbeef00000002# 18: [m0- t] addr=deadbeef00000009 dest=2# 18: [m1- t] addr=deadbeef00000008 dest=3# 18: [s1- t] addr=deadbeef00000001 dest=1# 19: Port 0 send flit 2a00400ffffdeadbeef0000000a (tail=0 dst=2 vc=1)# 19: Port 1 send flit 2e00602ffffdeadbeef00000008 (tail=0 dst=3 vc=1)# 19: Port 3 send flit 2600202ffffdeadbeef00000007 (tail=0 dst=1 vc=1)# 19: Port 1 recv flit 2600202ffffdeadbeef00000003 (tail=0 dst=1 vc=1) # 19: Port 1 get a credit (vc=1)# 19: Port 2 get a credit (vc=1)# 19: Port 3 get a credit (vc=1)# 19: Port 1 put a credit (vc=1)# 19: Device 0 send flit 2a00400ffffdeadbeef0000000a# 19: Device 1 send flit 2e00602ffffdeadbeef00000009# 19: Device 2 send flit 2200002ffffdeadbeef00000008# 19: Device 3 send flit 2600202ffffdeadbeef00000007# 19: Device 1 recv flit 2600202ffffdeadbeef00000002# 19: [m0- t] addr=deadbeef0000000a dest=2# 19: [m1- t] addr=deadbeef00000009 dest=3# 19: [s0- t] addr=deadbeef00000002 dest=0# 20: Port 1 send flit 2e00602ffffdeadbeef00000009 (tail=0 dst=3 vc=1)# 20: Port 2 send flit 2200002ffffdeadbeef00000007 (tail=0 dst=0 vc=1)# 20: Port 3 send flit 2600202ffffdeadbeef00000008 (tail=0 dst=1 vc=1)# 20: Port 3 recv flit 2e00602ffffdeadbeef00000004 (tail=0 dst=3 vc=1) # 20: Port 3 get a credit (vc=1)# 20: Port 3 put a credit (vc=1)# 20: Device 0 send flit 2a00400ffffdeadbeef0000000b# 20: Device 1 send flit 2e00602ffffdeadbeef0000000a# 20: Device 2 send flit 2200002ffffdeadbeef00000009# 20: Device 3 send flit 2600202ffffdeadbeef00000008# 20: Device 3 recv flit 2e00602ffffdeadbeef00000003# 20: [m0- t] addr=deadbeef0000000b dest=2# 20: [m1- t] addr=deadbeef0000000a dest=3# 20: [s1- t] addr=deadbeef00000002 dest=1# 21: Port 3 send flit 2600202ffffdeadbeef00000009 (tail=0 dst=1 vc=1)# 21: Port 0 recv flit 2200002ffffdeadbeef00000003 (tail=0 dst=0 vc=1) # 21: Port 1 recv flit 2600202ffffdeadbeef00000004 (tail=0 dst=1 vc=1) # 21: Port 2 recv flit 2a00400ffffdeadbeef00000003 (tail=0 dst=2 vc=1) # 21: Port 3 recv flit 2e00602ffffdeadbeef00000005 (tail=0 dst=3 vc=1) # 21: Port 0 get a credit (vc=1)# 21: Port 1 get a credit (vc=1)# 21: Port 3 get a credit (vc=1)# 21: Port 0 put a credit (vc=1)# 21: Port 1 put a credit (vc=1)# 21: Port 2 put a credit (vc=1)# 21: Device 0 send flit 2a00400ffffdeadbeef0000000c# 21: Device 1 send flit 2e00602ffffdeadbeef0000000b# 21: Device 2 send flit 2200002ffffdeadbeef0000000a# 21: Device 3 send flit 2600202ffffdeadbeef00000009# 21: Device 0 recv flit 2200002ffffdeadbeef00000003# 21: Device 1 recv flit 2600202ffffdeadbeef00000003# 21: Device 2 recv flit 2a00400ffffdeadbeef00000003# 21: [m0- t] addr=deadbeef0000000c dest=2# 21: [m1- t] addr=deadbeef0000000b dest=3# 22: Port 0 send flit 2a00400ffffdeadbeef0000000b (tail=0 dst=2 vc=1)# 22: Port 1 send flit 2e00602ffffdeadbeef0000000a (tail=0 dst=3 vc=1)# 22: Port 3 send flit 2600202ffffdeadbeef0000000a (tail=0 dst=1 vc=1)# 22: Port 1 recv flit 2600202ffffdeadbeef00000005 (tail=0 dst=1 vc=1) # 22: Port 1 get a credit (vc=1)# 22: Port 2 get a credit (vc=1)# 22: Port 3 get a credit (vc=1)# 22: Port 3 put a credit (vc=1)# 22: Device 0 send flit 2a00400ffffdeadbeef0000000d# 22: Device 1 send flit 2e00602ffffdeadbeef0000000c# 22: Device 2 send flit 2200002ffffdeadbeef0000000b# 22: Device 3 send flit 2600202ffffdeadbeef0000000a# 22: Device 3 recv flit 2e00602ffffdeadbeef00000004# 22: [m0- t] addr=deadbeef0000000d dest=2# 22: [m1- t] addr=deadbeef0000000c dest=3# 22: [s0- t] addr=deadbeef00000003 dest=0# 22: [s1- t] addr=deadbeef00000003 dest=1# 23: Port 1 send flit 2e00602ffffdeadbeef0000000b (tail=0 dst=3 vc=1)# 23: Port 2 send flit 2200002ffffdeadbeef00000008 (tail=0 dst=0 vc=1)# 23: Port 3 send flit 2600202ffffdeadbeef0000000b (tail=0 dst=1 vc=1)# 23: Port 3 recv flit 2e00602ffffdeadbeef00000006 (tail=0 dst=3 vc=1) # 23: Port 1 put a credit (vc=1)# 23: Device 0 send flit 2a00400ffffdeadbeef0000000e# 23: Device 1 send flit 2e00602ffffdeadbeef0000000d# 23: Device 2 send flit 2200002ffffdeadbeef0000000c# 23: Device 3 send flit 2600202ffffdeadbeef0000000b# 23: Device 1 recv flit 2600202ffffdeadbeef00000004# 23: [m0- t] addr=deadbeef0000000e dest=2# 23: [m1- t] addr=deadbeef0000000d dest=3# 24: Port 3 send flit 2600202ffffdeadbeef0000000c (tail=0 dst=1 vc=1)# 24: Port 0 recv flit 2200002ffffdeadbeef00000004 (tail=0 dst=0 vc=1) # 24: Port 1 recv flit 2600202ffffdeadbeef00000006 (tail=0 dst=1 vc=1) # 24: Port 2 recv flit 2a00400ffffdeadbeef00000004 (tail=0 dst=2 vc=1) # 24: Port 3 recv flit 2e00602ffffdeadbeef00000007 (tail=0 dst=3 vc=1) # 24: Port 0 get a credit (vc=1)# 24: Port 1 get a credit (vc=1)# 24: Port 3 get a credit (vc=1)# 24: Port 0 put a credit (vc=1)# 24: Port 2 put a credit (vc=1)# 24: Port 3 put a credit (vc=1)# 24: Device 0 send flit 2a00400ffffdeadbeef0000000f# 24: Device 1 send flit 2e00602ffffdeadbeef0000000e# 24: Device 3 send flit 2600202ffffdeadbeef0000000c# 24: Device 0 recv flit 2200002ffffdeadbeef00000004# 24: Device 2 recv flit 2a00400ffffdeadbeef00000004# 24: Device 3 recv flit 2e00602ffffdeadbeef00000005# 24: [m0- t] addr=deadbeef0000000f dest=2# 24: [m1- t] addr=deadbeef0000000e dest=3# 24: [s1- t] addr=deadbeef00000004 dest=1# 25: Port 0 send flit 2a00400ffffdeadbeef0000000c (tail=0 dst=2 vc=1)# 25: Port 1 send flit 2e00602ffffdeadbeef0000000c (tail=0 dst=3 vc=1)# 25: Port 3 send flit 2600202ffffdeadbeef0000000d (tail=0 dst=1 vc=1)# 25: Port 1 recv flit 2600202ffffdeadbeef00000007 (tail=0 dst=1 vc=1) # 25: Port 1 get a credit (vc=1)# 25: Port 2 get a credit (vc=1)# 25: Port 3 get a credit (vc=1)# 25: Port 1 put a credit (vc=1)# 25: Device 0 send flit 2a00400ffffdeadbeef00000010# 25: Device 1 send flit 2e00602ffffdeadbeef0000000f# 25: Device 3 send flit 2600202ffffdeadbeef0000000d# 25: Device 1 recv flit 2600202ffffdeadbeef00000005# 25: [m0- t] addr=deadbeef00000010 dest=2# 25: [m1- t] addr=deadbeef0000000f dest=3# 25: [s0- t] addr=deadbeef00000004 dest=0# 26: Port 1 send flit 2e00602ffffdeadbeef0000000d (tail=0 dst=3 vc=1)# 26: Port 2 send flit 2200002ffffdeadbeef00000009 (tail=0 dst=0 vc=1)# 26: Port 3 send flit 2600202ffffdeadbeef0000000e (tail=0 dst=1 vc=1)# 26: Port 3 recv flit 2e00602ffffdeadbeef00000008 (tail=0 dst=3 vc=1) # 26: Port 3 put a credit (vc=1)# 26: Device 1 send flit 2e00602ffffdeadbeef00000010# 26: Device 2 send flit 2200002ffffdeadbeef0000000d# 26: Device 3 send flit 2600202ffffdeadbeef0000000e# 26: Device 3 recv flit 2e00602ffffdeadbeef00000006# 26: [m1- t] addr=deadbeef00000010 dest=3# 26: [s1- t] addr=deadbeef00000005 dest=1# 27: Port 3 send flit 2600202ffffdeadbeef0000000f (tail=0 dst=1 vc=1)# 27: Port 0 recv flit 2200002ffffdeadbeef00000005 (tail=0 dst=0 vc=1) # 27: Port 1 recv flit 2600202ffffdeadbeef00000008 (tail=0 dst=1 vc=1) # 27: Port 2 recv flit 2a00400ffffdeadbeef00000005 (tail=0 dst=2 vc=1) # 27: Port 3 recv flit 2e00602ffffdeadbeef00000009 (tail=0 dst=3 vc=1) # 27: Port 0 get a credit (vc=1)# 27: Port 1 get a credit (vc=1)# 27: Port 3 get a credit (vc=1)# 27: Port 0 put a credit (vc=1)# 27: Port 1 put a credit (vc=1)# 27: Port 2 put a credit (vc=1)# 27: Device 1 send flit 2e00602ffffdeadbeef00000011# 27: Device 3 send flit 2600202ffffdeadbeef0000000f# 27: Device 0 recv flit 2200002ffffdeadbeef00000005# 27: Device 1 recv flit 2600202ffffdeadbeef00000006# 27: Device 2 recv flit 2a00400ffffdeadbeef00000005# 27: [m1- t] addr=deadbeef00000011 dest=3# 28: Port 0 send flit 2a00400ffffdeadbeef0000000d (tail=0 dst=2 vc=1)# 28: Port 1 send flit 2e00602ffffdeadbeef0000000e (tail=0 dst=3 vc=1)# 28: Port 3 send flit 2600202ffffdeadbeef00000010 (tail=0 dst=1 vc=1)# 28: Port 1 recv flit 2600202ffffdeadbeef00000009 (tail=0 dst=1 vc=1) # 28: Port 1 get a credit (vc=1)# 28: Port 2 get a credit (vc=1)# 28: Port 3 get a credit (vc=1)# 28: Port 3 put a credit (vc=1)# 28: Device 0 send flit 2a00400ffffdeadbeef00000011# 28: Device 1 send flit 2e00602ffffdeadbeef00000012# 28: Device 3 send flit 2600202ffffdeadbeef00000010# 28: Device 3 recv flit 2e00602ffffdeadbeef00000007# 28: [m0- t] addr=deadbeef00000011 dest=2# 28: [m1- t] addr=deadbeef00000012 dest=3# 28: [s0- t] addr=deadbeef00000005 dest=0# 28: [s1- t] addr=deadbeef00000006 dest=1# 29: Port 1 send flit 2e00602ffffdeadbeef0000000f (tail=0 dst=3 vc=1)# 29: Port 2 send flit 2200002ffffdeadbeef0000000a (tail=0 dst=0 vc=1)# 29: Port 3 send flit 2600202ffffdeadbeef00000011 (tail=0 dst=1 vc=1)# 29: Port 3 recv flit 2e00602ffffdeadbeef0000000a (tail=0 dst=3 vc=1) # 29: Port 1 put a credit (vc=1)# 29: Device 1 send flit 2e00602ffffdeadbeef00000013# 29: Device 2 send flit 2200002ffffdeadbeef0000000e# 29: Device 3 send flit 2600202ffffdeadbeef00000011# 29: Device 1 recv flit 2600202ffffdeadbeef00000007# 29: [m1- t] addr=deadbeef00000013 dest=3# 30: Port 0 recv flit 2200002ffffdeadbeef00000006 (tail=0 dst=0 vc=1) # 30: Port 1 recv flit 2600202ffffdeadbeef0000000a (tail=0 dst=1 vc=1) # 30: Port 2 recv flit 2a00400ffffdeadbeef00000006 (tail=0 dst=2 vc=1) # 30: Port 3 recv flit 2e00602ffffdeadbeef0000000b (tail=0 dst=3 vc=1) # 30: Port 0 get a credit (vc=1)# 30: Port 1 get a credit (vc=1)# 30: Port 3 get a credit (vc=1)# 30: Port 0 put a credit (vc=1)# 30: Port 2 put a credit (vc=1)# 30: Port 3 put a credit (vc=1)# 30: Device 3 send flit 2600202ffffdeadbeef00000012# 30: Device 0 recv flit 2200002ffffdeadbeef00000006# 30: Device 2 recv flit 2a00400ffffdeadbeef00000006# 30: Device 3 recv flit 2e00602ffffdeadbeef00000008# 30: [s1- t] addr=deadbeef00000007 dest=1# 31: Port 0 send flit 2a00400ffffdeadbeef0000000e (tail=0 dst=2 vc=1)# 31: Port 1 send flit 2e00602ffffdeadbeef00000010 (tail=0 dst=3 vc=1)# 31: Port 3 send flit 2600202ffffdeadbeef00000012 (tail=0 dst=1 vc=1)# 31: Port 1 recv flit 2600202ffffdeadbeef0000000b (tail=0 dst=1 vc=1) # 31: Port 1 get a credit (vc=1)# 31: Port 2 get a credit (vc=1)# 31: Port 3 get a credit (vc=1)# 31: Port 1 put a credit (vc=1)# 31: Device 0 send flit 2a00400ffffdeadbeef00000012# 31: Device 1 send flit 2e00602ffffdeadbeef00000014# 31: Device 3 send flit 2600202ffffdeadbeef00000013# 31: Device 1 recv flit 2600202ffffdeadbeef00000008# 31: [m0- t] addr=deadbeef00000012 dest=2# 31: [m1- t] addr=deadbeef00000014 dest=3# 31: [s0- t] addr=deadbeef00000006 dest=0# 32: Port 1 send flit 2e00602ffffdeadbeef00000011 (tail=0 dst=3 vc=1)# 32: Port 2 send flit 2200002ffffdeadbeef0000000b (tail=0 dst=0 vc=1)# 32: Port 3 send flit 2600202ffffdeadbeef00000013 (tail=0 dst=1 vc=1)# 32: Port 3 recv flit 2e00602ffffdeadbeef0000000c (tail=0 dst=3 vc=1) # 32: Port 3 put a credit (vc=1)# 32: Device 1 send flit 2e00602ffffdeadbeef00000015# 32: Device 2 send flit 2200002ffffdeadbeef0000000f# 32: Device 3 send flit 2600202ffffdeadbeef00000014# 32: Device 3 recv flit 2e00602ffffdeadbeef00000009# 32: [m1- t] addr=deadbeef00000015 dest=3# 32: [s1- t] addr=deadbeef00000008 dest=1# 33: Port 0 recv flit 2200002ffffdeadbeef00000007 (tail=0 dst=0 vc=1) # 33: Port 1 recv flit 2600202ffffdeadbeef0000000c (tail=0 dst=1 vc=1) # 33: Port 2 recv flit 2a00400ffffdeadbeef00000007 (tail=0 dst=2 vc=1) # 33: Port 3 recv flit 2e00602ffffdeadbeef0000000d (tail=0 dst=3 vc=1) # 33: Port 0 get a credit (vc=1)# 33: Port 1 get a credit (vc=1)# 33: Port 3 get a credit (vc=1)# 33: Port 0 put a credit (vc=1)# 33: Port 1 put a credit (vc=1)# 33: Port 2 put a credit (vc=1)# 33: Device 3 send flit 2600202ffffdeadbeef00000015# 33: Device 0 recv flit 2200002ffffdeadbeef00000007# 33: Device 1 recv flit 2600202ffffdeadbeef00000009# 33: Device 2 recv flit 2a00400ffffdeadbeef00000007# 34: Port 0 send flit 2a00400ffffdeadbeef0000000f (tail=0 dst=2 vc=1)# 34: Port 1 send flit 2e00602ffffdeadbeef00000012 (tail=0 dst=3 vc=1)# 34: Port 3 send flit 2600202ffffdeadbeef00000014 (tail=0 dst=1 vc=1)# 34: Port 1 recv flit 2600202ffffdeadbeef0000000d (tail=0 dst=1 vc=1) # 34: Port 1 get a credit (vc=1)# 34: Port 2 get a credit (vc=1)# 34: Port 3 get a credit (vc=1)# 34: Port 3 put a credit (vc=1)# 34: Device 0 send flit 2a00400ffffdeadbeef00000013# 34: Device 1 send flit 2e00602ffffdeadbeef00000016# 34: Device 3 send flit 2600202ffffdeadbeef00000016# 34: Device 3 recv flit 2e00602ffffdeadbeef0000000a# 34: [m0- t] addr=deadbeef00000013 dest=2# 34: [m1- t] addr=deadbeef00000016 dest=3# 34: [s0- t] addr=deadbeef00000007 dest=0# 34: [s1- t] addr=deadbeef00000009 dest=1# 35: Port 1 send flit 2e00602ffffdeadbeef00000013 (tail=0 dst=3 vc=1)# 35: Port 2 send flit 2200002ffffdeadbeef0000000c (tail=0 dst=0 vc=1)# 35: Port 3 send flit 2600202ffffdeadbeef00000015 (tail=0 dst=1 vc=1)# 35: Port 3 recv flit 2e00602ffffdeadbeef0000000e (tail=0 dst=3 vc=1) # 35: Port 1 put a credit (vc=1)# 35: Device 1 send flit 3e00603ffffdeadbeef00000017# 35: Device 2 send flit 2200002ffffdeadbeef00000010# 35: Device 3 send flit 3600203ffffdeadbeef00000017# 35: Device 1 recv flit 2600202ffffdeadbeef0000000a# 35: [m1- t] addr=deadbeef00000017 dest=3# 36: Port 0 recv flit 2200002ffffdeadbeef00000008 (tail=0 dst=0 vc=1) # 36: Port 1 recv flit 2600202ffffdeadbeef0000000e (tail=0 dst=1 vc=1) # 36: Port 2 recv flit 2a00400ffffdeadbeef00000008 (tail=0 dst=2 vc=1) # 36: Port 0 get a credit (vc=1)# 36: Port 1 get a credit (vc=1)# 36: Port 3 get a credit (vc=1)# 36: Port 0 put a credit (vc=1)# 36: Port 2 put a credit (vc=1)# 36: Port 3 put a credit (vc=1)# 36: Device 0 recv flit 2200002ffffdeadbeef00000008# 36: Device 2 recv flit 2a00400ffffdeadbeef00000008# 36: Device 3 recv flit 2e00602ffffdeadbeef0000000b# 36: [s1- t] addr=deadbeef0000000a dest=1# 37: Port 0 send flit 2a00400ffffdeadbeef00000010 (tail=0 dst=2 vc=1)# 37: Port 1 send flit 2e00602ffffdeadbeef00000014 (tail=0 dst=3 vc=1)# 37: Port 3 send flit 2600202ffffdeadbeef00000016 (tail=0 dst=1 vc=1)# 37: Port 3 recv flit 2e00602ffffdeadbeef0000000f (tail=0 dst=3 vc=1) # 37: Port 1 get a credit (vc=1)# 37: Port 2 get a credit (vc=1)# 37: Port 3 get a credit (vc=1)# 37: Port 1 put a credit (vc=1)# 37: Device 0 send flit 2a00400ffffdeadbeef00000014# 37: Device 1 recv flit 2600202ffffdeadbeef0000000b# 37: [m0- t] addr=deadbeef00000014 dest=2# 37: [s0- t] addr=deadbeef00000008 dest=0# 38: Port 1 send flit 2e00602ffffdeadbeef00000015 (tail=0 dst=3 vc=1)# 38: Port 2 send flit 2200002ffffdeadbeef0000000d (tail=0 dst=0 vc=1)# 38: Port 3 send flit 3600203ffffdeadbeef00000017 (tail=1 dst=1 vc=1)# 38: Port 1 recv flit 2600202ffffdeadbeef0000000f (tail=0 dst=1 vc=1) # 38: Port 3 put a credit (vc=1)# 38: Device 2 send flit 2200002ffffdeadbeef00000011# 38: Device 3 recv flit 2e00602ffffdeadbeef0000000c# 38: [s1- t] addr=deadbeef0000000b dest=1# 39: Port 2 recv flit 2a00400ffffdeadbeef00000009 (tail=0 dst=2 vc=1) # 39: Port 3 recv flit 2e00602ffffdeadbeef00000010 (tail=0 dst=3 vc=1) # 39: Port 0 get a credit (vc=1)# 39: Port 1 get a credit (vc=1)# 39: Port 3 get a credit (vc=1)# 39: Port 1 put a credit (vc=1)# 39: Port 2 put a credit (vc=1)# 39: Device 1 recv flit 2600202ffffdeadbeef0000000c# 39: Device 2 recv flit 2a00400ffffdeadbeef00000009# 40: Port 0 send flit 2a00400ffffdeadbeef00000011 (tail=0 dst=2 vc=1)# 40: Port 1 send flit 2e00602ffffdeadbeef00000016 (tail=0 dst=3 vc=1)# 40: Port 0 recv flit 2200002ffffdeadbeef00000009 (tail=0 dst=0 vc=1) # 40: Port 1 recv flit 2600202ffffdeadbeef00000010 (tail=0 dst=1 vc=1) # 40: Port 1 get a credit (vc=1)# 40: Port 2 get a credit (vc=1)# 40: Port 3 get a credit (vc=1)# 40: Port 0 put a credit (vc=1)# 40: Port 3 put a credit (vc=1)# 40: Device 0 send flit 2a00400ffffdeadbeef00000015# 40: Device 0 recv flit 2200002ffffdeadbeef00000009# 40: Device 3 recv flit 2e00602ffffdeadbeef0000000d# 40: [m0- t] addr=deadbeef00000015 dest=2# 40: [s1- t] addr=deadbeef0000000c dest=1# 41: Port 1 send flit 3e00603ffffdeadbeef00000017 (tail=1 dst=3 vc=1)# 41: Port 2 send flit 2200002ffffdeadbeef0000000e (tail=0 dst=0 vc=1)# 41: Port 3 recv flit 2e00602ffffdeadbeef00000011 (tail=0 dst=3 vc=1) # 41: Port 1 put a credit (vc=1)# 41: Device 2 send flit 2200002ffffdeadbeef00000012# 41: Device 1 recv flit 2600202ffffdeadbeef0000000d# 41: [s0- t] addr=deadbeef00000009 dest=0# 42: Port 1 recv flit 2600202ffffdeadbeef00000011 (tail=0 dst=1 vc=1) # 42: Port 2 recv flit 2a00400ffffdeadbeef0000000a (tail=0 dst=2 vc=1) # 42: Port 0 get a credit (vc=1)# 42: Port 1 get a credit (vc=1)# 42: Port 3 get a credit (vc=1)# 42: Port 2 put a credit (vc=1)# 42: Port 3 put a credit (vc=1)# 42: Device 2 recv flit 2a00400ffffdeadbeef0000000a# 42: Device 3 recv flit 2e00602ffffdeadbeef0000000e# 42: [s1- t] addr=deadbeef0000000d dest=1# 43: Port 0 send flit 2a00400ffffdeadbeef00000012 (tail=0 dst=2 vc=1)# 43: Port 3 recv flit 2e00602ffffdeadbeef00000012 (tail=0 dst=3 vc=1) # 43: Port 1 get a credit (vc=1)# 43: Port 2 get a credit (vc=1)# 43: Port 3 get a credit (vc=1)# 43: Port 1 put a credit (vc=1)# 43: Device 0 send flit 2a00400ffffdeadbeef00000016# 43: Device 1 recv flit 2600202ffffdeadbeef0000000e# 43: [m0- t] addr=deadbeef00000016 dest=2# 44: Port 2 send flit 2200002ffffdeadbeef0000000f (tail=0 dst=0 vc=1)# 44: Port 0 recv flit 2200002ffffdeadbeef0000000a (tail=0 dst=0 vc=1) # 44: Port 1 recv flit 2600202ffffdeadbeef00000012 (tail=0 dst=1 vc=1) # 44: Port 0 put a credit (vc=1)# 44: Port 3 put a credit (vc=1)# 44: Device 2 send flit 2200002ffffdeadbeef00000013# 44: Device 0 recv flit 2200002ffffdeadbeef0000000a# 44: Device 3 recv flit 2e00602ffffdeadbeef0000000f# 44: [s1- t] addr=deadbeef0000000e dest=1# 45: Port 2 recv flit 2a00400ffffdeadbeef0000000b (tail=0 dst=2 vc=1) # 45: Port 3 recv flit 2e00602ffffdeadbeef00000013 (tail=0 dst=3 vc=1) # 45: Port 0 get a credit (vc=1)# 45: Port 1 get a credit (vc=1)# 45: Port 1 put a credit (vc=1)# 45: Port 2 put a credit (vc=1)# 45: Device 1 recv flit 2600202ffffdeadbeef0000000f# 45: Device 2 recv flit 2a00400ffffdeadbeef0000000b# 45: [s0- t] addr=deadbeef0000000a dest=0# 46: Port 0 send flit 2a00400ffffdeadbeef00000013 (tail=0 dst=2 vc=1)# 46: Port 1 recv flit 2600202ffffdeadbeef00000013 (tail=0 dst=1 vc=1) # 46: Port 1 get a credit (vc=1)# 46: Port 3 put a credit (vc=1)# 46: Device 0 send flit 3a00401ffffdeadbeef00000017# 46: Device 3 recv flit 2e00602ffffdeadbeef00000010# 46: [m0- t] addr=deadbeef00000017 dest=2# 46: [s1- t] addr=deadbeef0000000f dest=1# 47: Port 3 recv flit 2e00602ffffdeadbeef00000014 (tail=0 dst=3 vc=1) # 47: Port 1 put a credit (vc=1)# 47: Device 1 recv flit 2600202ffffdeadbeef00000010# 48: Port 0 recv flit 2200002ffffdeadbeef0000000b (tail=0 dst=0 vc=1) # 48: Port 1 recv flit 2600202ffffdeadbeef00000014 (tail=0 dst=1 vc=1) # 48: Port 2 recv flit 2a00400ffffdeadbeef0000000c (tail=0 dst=2 vc=1) # 48: Port 0 get a credit (vc=1)# 48: Port 0 put a credit (vc=1)# 48: Port 2 put a credit (vc=1)# 48: Port 3 put a credit (vc=1)# 48: Device 0 recv flit 2200002ffffdeadbeef0000000b# 48: Device 2 recv flit 2a00400ffffdeadbeef0000000c# 48: Device 3 recv flit 2e00602ffffdeadbeef00000011# 48: [s1- t] addr=deadbeef00000010 dest=1# 49: Port 0 send flit 2a00400ffffdeadbeef00000014 (tail=0 dst=2 vc=1)# 49: Port 2 recv flit 2a00400ffffdeadbeef0000000d (tail=0 dst=2 vc=1) # 49: Port 3 recv flit 2e00602ffffdeadbeef00000015 (tail=0 dst=3 vc=1) # 49: Port 0 get a credit (vc=1)# 49: Port 2 get a credit (vc=1)# 49: Port 1 put a credit (vc=1)# 49: Device 1 recv flit 2600202ffffdeadbeef00000011# 49: [s0- t] addr=deadbeef0000000b dest=0# 50: Port 0 send flit 2a00400ffffdeadbeef00000015 (tail=0 dst=2 vc=1)# 50: Port 2 send flit 2200002ffffdeadbeef00000010 (tail=0 dst=0 vc=1)# 50: Port 1 recv flit 2600202ffffdeadbeef00000015 (tail=0 dst=1 vc=1) # 50: Port 2 recv flit 2a00400ffffdeadbeef0000000e (tail=0 dst=2 vc=1) # 50: Port 0 get a credit (vc=1)# 50: Port 2 put a credit (vc=1)# 50: Port 3 put a credit (vc=1)# 50: Device 2 send flit 2200002ffffdeadbeef00000014# 50: Device 2 recv flit 2a00400ffffdeadbeef0000000d# 50: Device 3 recv flit 2e00602ffffdeadbeef00000012# 50: [s1- t] addr=deadbeef00000011 dest=1# 51: Port 0 send flit 2a00400ffffdeadbeef00000016 (tail=0 dst=2 vc=1)# 51: Port 2 recv flit 2a00400ffffdeadbeef0000000f (tail=0 dst=2 vc=1) # 51: Port 3 recv flit 2e00602ffffdeadbeef00000016 (tail=0 dst=3 vc=1) # 51: Port 0 get a credit (vc=1)# 51: Port 2 get a credit (vc=1)# 51: Port 1 put a credit (vc=1)# 51: Device 1 recv flit 2600202ffffdeadbeef00000012# 52: Port 0 send flit 3a00401ffffdeadbeef00000017 (tail=1 dst=2 vc=1)# 52: Port 2 send flit 2200002ffffdeadbeef00000011 (tail=0 dst=0 vc=1)# 52: Port 1 recv flit 2600202ffffdeadbeef00000016 (tail=0 dst=1 vc=1) # 52: Port 2 recv flit 2a00400ffffdeadbeef00000010 (tail=0 dst=2 vc=1) # 52: Port 0 get a credit (vc=1)# 52: Port 2 put a credit (vc=1)# 52: Port 3 put a credit (vc=1)# 52: Device 2 send flit 2200002ffffdeadbeef00000015# 52: Device 2 recv flit 2a00400ffffdeadbeef0000000e# 52: Device 3 recv flit 2e00602ffffdeadbeef00000013# 52: [s1- t] addr=deadbeef00000012 dest=1# 53: Port 2 recv flit 2a00400ffffdeadbeef00000011 (tail=0 dst=2 vc=1) # 53: Port 3 recv flit 3e00603ffffdeadbeef00000017 (tail=1 dst=3 vc=1) # 53: Port 0 get a credit (vc=1)# 53: Port 2 get a credit (vc=1)# 53: Port 1 put a credit (vc=1)# 53: Device 1 recv flit 2600202ffffdeadbeef00000013# 54: Port 2 send flit 2200002ffffdeadbeef00000012 (tail=0 dst=0 vc=1)# 54: Port 1 recv flit 3600203ffffdeadbeef00000017 (tail=1 dst=1 vc=1) # 54: Port 2 recv flit 2a00400ffffdeadbeef00000012 (tail=0 dst=2 vc=1) # 54: Port 0 get a credit (vc=1)# 54: Port 2 put a credit (vc=1)# 54: Port 3 put a credit (vc=1)# 54: Device 2 send flit 2200002ffffdeadbeef00000016# 54: Device 2 recv flit 2a00400ffffdeadbeef0000000f# 54: Device 3 recv flit 2e00602ffffdeadbeef00000014# 54: [s1- t] addr=deadbeef00000013 dest=1# 55: Port 2 recv flit 2a00400ffffdeadbeef00000013 (tail=0 dst=2 vc=1) # 55: Port 0 get a credit (vc=1)# 55: Port 2 get a credit (vc=1)# 55: Port 1 put a credit (vc=1)# 55: Device 1 recv flit 2600202ffffdeadbeef00000014# 56: Port 2 send flit 2200002ffffdeadbeef00000013 (tail=0 dst=0 vc=1)# 56: Port 0 recv flit 2200002ffffdeadbeef0000000c (tail=0 dst=0 vc=1) # 56: Port 2 get a credit (vc=1)# 56: Port 0 put a credit (vc=1)# 56: Port 2 put a credit (vc=1)# 56: Port 3 put a credit (vc=1)# 56: Device 2 send flit 3200003ffffdeadbeef00000017# 56: Device 0 recv flit 2200002ffffdeadbeef0000000c# 56: Device 2 recv flit 2a00400ffffdeadbeef00000010# 56: Device 3 recv flit 2e00602ffffdeadbeef00000015# 56: [s1- t] addr=deadbeef00000014 dest=1# 57: Port 2 send flit 2200002ffffdeadbeef00000014 (tail=0 dst=0 vc=1)# 57: Port 0 recv flit 2200002ffffdeadbeef0000000d (tail=0 dst=0 vc=1) # 57: Port 2 recv flit 2a00400ffffdeadbeef00000014 (tail=0 dst=2 vc=1) # 57: Port 2 get a credit (vc=1)# 57: Port 1 put a credit (vc=1)# 57: Device 1 recv flit 2600202ffffdeadbeef00000015# 57: [s0- t] addr=deadbeef0000000c dest=0# 58: Port 2 send flit 2200002ffffdeadbeef00000015 (tail=0 dst=0 vc=1)# 58: Port 0 recv flit 2200002ffffdeadbeef0000000e (tail=0 dst=0 vc=1) # 58: Port 2 get a credit (vc=1)# 58: Port 0 put a credit (vc=1)# 58: Port 2 put a credit (vc=1)# 58: Port 3 put a credit (vc=1)# 58: Device 0 recv flit 2200002ffffdeadbeef0000000d# 58: Device 2 recv flit 2a00400ffffdeadbeef00000011# 58: Device 3 recv flit 2e00602ffffdeadbeef00000016# 58: [s1- t] addr=deadbeef00000015 dest=1# 59: Port 2 send flit 2200002ffffdeadbeef00000016 (tail=0 dst=0 vc=1)# 59: Port 0 recv flit 2200002ffffdeadbeef0000000f (tail=0 dst=0 vc=1) # 59: Port 2 recv flit 2a00400ffffdeadbeef00000015 (tail=0 dst=2 vc=1) # 59: Port 2 get a credit (vc=1)# 59: Port 1 put a credit (vc=1)# 59: Device 1 recv flit 2600202ffffdeadbeef00000016# 59: [s0- t] addr=deadbeef0000000d dest=0# 60: Port 2 send flit 3200003ffffdeadbeef00000017 (tail=1 dst=0 vc=1)# 60: Port 0 recv flit 2200002ffffdeadbeef00000010 (tail=0 dst=0 vc=1) # 60: Port 2 get a credit (vc=1)# 60: Port 0 put a credit (vc=1)# 60: Port 2 put a credit (vc=1)# 60: Port 3 put a credit (vc=1)# 60: Device 0 recv flit 2200002ffffdeadbeef0000000e# 60: Device 2 recv flit 2a00400ffffdeadbeef00000012# 60: Device 3 recv flit 3e00603ffffdeadbeef00000017# 60: [s1- t] addr=deadbeef00000016 dest=1# 61: Port 0 recv flit 2200002ffffdeadbeef00000011 (tail=0 dst=0 vc=1) # 61: Port 2 recv flit 2a00400ffffdeadbeef00000016 (tail=0 dst=2 vc=1) # 61: Port 2 get a credit (vc=1)# 61: Port 1 put a credit (vc=1)# 61: Device 1 recv flit 3600203ffffdeadbeef00000017# 61: [s0- t] addr=deadbeef0000000e dest=0# 62: Port 0 recv flit 2200002ffffdeadbeef00000012 (tail=0 dst=0 vc=1) # 62: Port 2 get a credit (vc=1)# 62: Port 0 put a credit (vc=1)# 62: Port 2 put a credit (vc=1)# 62: Device 0 recv flit 2200002ffffdeadbeef0000000f# 62: Device 2 recv flit 2a00400ffffdeadbeef00000013# 62: [s1- t] addr=deadbeef00000017 dest=1# 63: Port 0 recv flit 2200002ffffdeadbeef00000013 (tail=0 dst=0 vc=1) # 63: Port 2 recv flit 3a00401ffffdeadbeef00000017 (tail=1 dst=2 vc=1) # 63: Port 2 get a credit (vc=1)# 63: [s0- t] addr=deadbeef0000000f dest=0# 64: Port 0 put a credit (vc=1)# 64: Port 2 put a credit (vc=1)# 64: Device 0 recv flit 2200002ffffdeadbeef00000010# 64: Device 2 recv flit 2a00400ffffdeadbeef00000014# 65: Port 0 recv flit 2200002ffffdeadbeef00000014 (tail=0 dst=0 vc=1) # 65: [s0- t] addr=deadbeef00000010 dest=0# 66: Port 0 put a credit (vc=1)# 66: Port 2 put a credit (vc=1)# 66: Device 0 recv flit 2200002ffffdeadbeef00000011# 66: Device 2 recv flit 2a00400ffffdeadbeef00000015# 67: Port 0 recv flit 2200002ffffdeadbeef00000015 (tail=0 dst=0 vc=1) # 67: [s0- t] addr=deadbeef00000011 dest=0# 68: Port 0 put a credit (vc=1)# 68: Port 2 put a credit (vc=1)# 68: Device 0 recv flit 2200002ffffdeadbeef00000012# 68: Device 2 recv flit 2a00400ffffdeadbeef00000016# 69: Port 0 recv flit 2200002ffffdeadbeef00000016 (tail=0 dst=0 vc=1) # 69: [s0- t] addr=deadbeef00000012 dest=0# 70: Port 0 put a credit (vc=1)# 70: Port 2 put a credit (vc=1)# 70: Device 0 recv flit 2200002ffffdeadbeef00000013# 70: Device 2 recv flit 3a00401ffffdeadbeef00000017# 71: Port 0 recv flit 3200003ffffdeadbeef00000017 (tail=1 dst=0 vc=1) # 71: [s0- t] addr=deadbeef00000013 dest=0# 72: Port 0 put a credit (vc=1)# 72: Device 0 recv flit 2200002ffffdeadbeef00000014# 73: [s0- t] addr=deadbeef00000014 dest=0# 74: Port 0 put a credit (vc=1)# 74: Device 0 recv flit 2200002ffffdeadbeef00000015# 75: [s0- t] addr=deadbeef00000015 dest=0# 76: Port 0 put a credit (vc=1)# 76: Device 0 recv flit 2200002ffffdeadbeef00000016# 77: [s0- t] addr=deadbeef00000016 dest=0# 78: Port 0 put a credit (vc=1)# 78: Device 0 recv flit 3200003ffffdeadbeef00000017# 79: [s0- t] addr=deadbeef00000017 dest=0# actual:deadbeef00000000 expected:deadbeef00000000# actual:deadbeef00000001 expected:deadbeef00000001# actual:deadbeef00000002 expected:deadbeef00000002# actual:deadbeef00000003 expected:deadbeef00000003# actual:deadbeef00000004 expected:deadbeef00000004# actual:deadbeef00000005 expected:deadbeef00000005# actual:deadbeef00000006 expected:deadbeef00000006# actual:deadbeef00000007 expected:deadbeef00000007# actual:deadbeef00000008 expected:deadbeef00000008# actual:deadbeef00000009 expected:deadbeef00000009# actual:deadbeef0000000a expected:deadbeef0000000a# actual:deadbeef0000000b expected:deadbeef0000000b# actual:deadbeef0000000c expected:deadbeef0000000c# actual:deadbeef0000000d expected:deadbeef0000000d# actual:deadbeef0000000e expected:deadbeef0000000e# actual:deadbeef0000000f expected:deadbeef0000000f# actual:deadbeef00000010 expected:deadbeef00000010# actual:deadbeef00000011 expected:deadbeef00000011# actual:deadbeef00000012 expected:deadbeef00000012# actual:deadbeef00000013 expected:deadbeef00000013# actual:deadbeef00000014 expected:deadbeef00000014# actual:deadbeef00000015 expected:deadbeef00000015# actual:deadbeef00000016 expected:deadbeef00000016# actual:deadbeef00000017 expected:deadbeef00000017# Pass 2# actual:deadbeef00000000 expected:deadbeef00000000# actual:deadbeef00000001 expected:deadbeef00000001# actual:deadbeef00000002 expected:deadbeef00000002# actual:deadbeef00000003 expected:deadbeef00000003# actual:deadbeef00000004 expected:deadbeef00000004# actual:deadbeef00000005 expected:deadbeef00000005# actual:deadbeef00000006 expected:deadbeef00000006# actual:deadbeef00000007 expected:deadbeef00000007# actual:deadbeef00000008 expected:deadbeef00000008# actual:deadbeef00000009 expected:deadbeef00000009# actual:deadbeef0000000a expected:deadbeef0000000a# actual:deadbeef0000000b expected:deadbeef0000000b# actual:deadbeef0000000c expected:deadbeef0000000c# actual:deadbeef0000000d expected:deadbeef0000000d# actual:deadbeef0000000e expected:deadbeef0000000e# actual:deadbeef0000000f expected:deadbeef0000000f# actual:deadbeef00000010 expected:deadbeef00000010# actual:deadbeef00000011 expected:deadbeef00000011# actual:deadbeef00000012 expected:deadbeef00000012# actual:deadbeef00000013 expected:deadbeef00000013# actual:deadbeef00000014 expected:deadbeef00000014# actual:deadbeef00000015 expected:deadbeef00000015# actual:deadbeef00000016 expected:deadbeef00000016# actual:deadbeef00000017 expected:deadbeef00000017# Pass 3# actual:deadbeef00000000 expected:deadbeef00000000# actual:deadbeef00000001 expected:deadbeef00000001# actual:deadbeef00000002 expected:deadbeef00000002# actual:deadbeef00000003 expected:deadbeef00000003# actual:deadbeef00000004 expected:deadbeef00000004# actual:deadbeef00000005 expected:deadbeef00000005# actual:deadbeef00000006 expected:deadbeef00000006# actual:deadbeef00000007 expected:deadbeef00000007# actual:deadbeef00000008 expected:deadbeef00000008# actual:deadbeef00000009 expected:deadbeef00000009# actual:deadbeef0000000a expected:deadbeef0000000a# actual:deadbeef0000000b expected:deadbeef0000000b# actual:deadbeef0000000c expected:deadbeef0000000c# actual:deadbeef0000000d expected:deadbeef0000000d# actual:deadbeef0000000e expected:deadbeef0000000e# actual:deadbeef0000000f expected:deadbeef0000000f# actual:deadbeef00000010 expected:deadbeef00000010# actual:deadbeef00000011 expected:deadbeef00000011# actual:deadbeef00000012 expected:deadbeef00000012# actual:deadbeef00000013 expected:deadbeef00000013# actual:deadbeef00000014 expected:deadbeef00000014# actual:deadbeef00000015 expected:deadbeef00000015# actual:deadbeef00000016 expected:deadbeef00000016# actual:deadbeef00000017 expected:deadbeef00000017# Pass 0# actual:deadbeef00000000 expected:deadbeef00000000# actual:deadbeef00000001 expected:deadbeef00000001# actual:deadbeef00000002 expected:deadbeef00000002# actual:deadbeef00000003 expected:deadbeef00000003# actual:deadbeef00000004 expected:deadbeef00000004# actual:deadbeef00000005 expected:deadbeef00000005# actual:deadbeef00000006 expected:deadbeef00000006# actual:deadbeef00000007 expected:deadbeef00000007# actual:deadbeef00000008 expected:deadbeef00000008# actual:deadbeef00000009 expected:deadbeef00000009# actual:deadbeef0000000a expected:deadbeef0000000a# actual:deadbeef0000000b expected:deadbeef0000000b# actual:deadbeef0000000c expected:deadbeef0000000c# actual:deadbeef0000000d expected:deadbeef0000000d# actual:deadbeef0000000e expected:deadbeef0000000e# actual:deadbeef0000000f expected:deadbeef0000000f# actual:deadbeef00000010 expected:deadbeef00000010# actual:deadbeef00000011 expected:deadbeef00000011# actual:deadbeef00000012 expected:deadbeef00000012# actual:deadbeef00000013 expected:deadbeef00000013# actual:deadbeef00000014 expected:deadbeef00000014# actual:deadbeef00000015 expected:deadbeef00000015# actual:deadbeef00000016 expected:deadbeef00000016# actual:deadbeef00000017 expected:deadbeef00000017# Pass 1# ** Note: $finish : testbench_sample_axi4stream.sv(74)# Time: 1110 ns Iteration: 0 Instance: /CONNECT_testbench_sample_axi4stream# End time: 10:36:57 on Jun 08,2022, Elapsed time: 0:00:00# Errors: 0, Warnings: 0Result (Intel FIFO IP)# ---- Performing Reset ----# 9: Device 0 send flit 2a00400ffffdeadbeef00000000# 9: [ENQ] data=2a00400ffffdeadbeef00000000# 9: [m0- t] addr=deadbeef00000000 dest=2# 10: Device 0 send flit 2a00400ffffdeadbeef00000001# 10: Device 1 send flit 2e00602ffffdeadbeef00000000# 10: [ENQ] data=2a00400ffffdeadbeef00000001# 10: [ENQ] data=2e00602ffffdeadbeef00000000# 10: [m0- t] addr=deadbeef00000001 dest=2# 10: [m1- t] addr=deadbeef00000000 dest=3# 11: Port 0 send flit 2a00400ffffdeadbeef00000000 (tail=0 dst=2 vc=1)# 11: Device 0 send flit 2a00400ffffdeadbeef00000002# 11: Device 1 send flit 2e00602ffffdeadbeef00000001# 11: Device 2 send flit 2200002ffffdeadbeef00000000# 11: [ENQ] data=2a00400ffffdeadbeef00000002# 11: [DEQ] data=2a00400ffffdeadbeef00000000# 11: [ENQ] data=2e00602ffffdeadbeef00000001# 11: [ENQ] data=2200002ffffdeadbeef00000000# 11: [m0- t] addr=deadbeef00000002 dest=2# 11: [m1- t] addr=deadbeef00000001 dest=3# 12: Port 0 send flit 2a00400ffffdeadbeef00000001 (tail=0 dst=2 vc=1)# 12: Port 1 send flit 2e00602ffffdeadbeef00000000 (tail=0 dst=3 vc=1)# 12: Port 0 get a credit (vc=1)# 12: Device 0 send flit 2a00400ffffdeadbeef00000003# 12: Device 1 send flit 2e00602ffffdeadbeef00000002# 12: Device 2 send flit 2200002ffffdeadbeef00000001# 12: Device 3 send flit 2600202ffffdeadbeef00000000# 12: [ENQ] data=2a00400ffffdeadbeef00000003# 12: [DEQ] data=2a00400ffffdeadbeef00000001# 12: [ENQ] data=2e00602ffffdeadbeef00000002# 12: [DEQ] data=2e00602ffffdeadbeef00000000# 12: [ENQ] data=2200002ffffdeadbeef00000001# 12: [ENQ] data=2600202ffffdeadbeef00000000# 12: [m0- t] addr=deadbeef00000003 dest=2# 12: [m1- t] addr=deadbeef00000002 dest=3# 13: Port 0 send flit 2a00400ffffdeadbeef00000002 (tail=0 dst=2 vc=1)# 13: Port 1 send flit 2e00602ffffdeadbeef00000001 (tail=0 dst=3 vc=1)# 13: Port 2 send flit 2200002ffffdeadbeef00000000 (tail=0 dst=0 vc=1)# 13: Port 0 get a credit (vc=1)# 13: Port 1 get a credit (vc=1)# 13: Device 0 send flit 2a00400ffffdeadbeef00000004# 13: Device 1 send flit 2e00602ffffdeadbeef00000003# 13: Device 2 send flit 2200002ffffdeadbeef00000002# 13: Device 3 send flit 2600202ffffdeadbeef00000001# 13: [ENQ] data=2a00400ffffdeadbeef00000004# 13: [DEQ] data=2a00400ffffdeadbeef00000002# 13: [ENQ] data=2e00602ffffdeadbeef00000003# 13: [DEQ] data=2e00602ffffdeadbeef00000001# 13: [ENQ] data=2200002ffffdeadbeef00000002# 13: [DEQ] data=2200002ffffdeadbeef00000000# 13: [ENQ] data=2600202ffffdeadbeef00000001# 13: [m0- t] addr=deadbeef00000004 dest=2# 13: [m1- t] addr=deadbeef00000003 dest=3# 14: Port 0 send flit 2a00400ffffdeadbeef00000003 (tail=0 dst=2 vc=1)# 14: Port 1 send flit 2e00602ffffdeadbeef00000002 (tail=0 dst=3 vc=1)# 14: Port 2 send flit 2200002ffffdeadbeef00000001 (tail=0 dst=0 vc=1)# 14: Port 3 send flit 2600202ffffdeadbeef00000000 (tail=0 dst=1 vc=1)# 14: Port 0 get a credit (vc=1)# 14: Port 2 get a credit (vc=1)# 14: Device 0 send flit 2a00400ffffdeadbeef00000005# 14: Device 1 send flit 2e00602ffffdeadbeef00000004# 14: Device 2 send flit 2200002ffffdeadbeef00000003# 14: Device 3 send flit 2600202ffffdeadbeef00000002# 14: [ENQ] data=2a00400ffffdeadbeef00000005# 14: [DEQ] data=2a00400ffffdeadbeef00000003# 14: [ENQ] data=2e00602ffffdeadbeef00000004# 14: [DEQ] data=2e00602ffffdeadbeef00000002# 14: [ENQ] data=2200002ffffdeadbeef00000003# 14: [DEQ] data=2200002ffffdeadbeef00000001# 14: [ENQ] data=2600202ffffdeadbeef00000002# 14: [DEQ] data=2600202ffffdeadbeef00000000# 14: [m0- t] addr=deadbeef00000005 dest=2# 14: [m1- t] addr=deadbeef00000004 dest=3# 15: Port 0 send flit 2a00400ffffdeadbeef00000004 (tail=0 dst=2 vc=1)# 15: Port 1 send flit 2e00602ffffdeadbeef00000003 (tail=0 dst=3 vc=1)# 15: Port 2 send flit 2200002ffffdeadbeef00000002 (tail=0 dst=0 vc=1)# 15: Port 3 send flit 2600202ffffdeadbeef00000001 (tail=0 dst=1 vc=1)# 15: Port 2 recv flit 2a00400ffffdeadbeef00000000 (tail=0 dst=2 vc=1) # 15: Port 3 recv flit 2e00602ffffdeadbeef00000000 (tail=0 dst=3 vc=1) # 15: Port 0 get a credit (vc=1)# 15: Port 1 get a credit (vc=1)# 15: Port 2 get a credit (vc=1)# 15: Port 3 get a credit (vc=1)# 15: Device 0 send flit 2a00400ffffdeadbeef00000006# 15: Device 1 send flit 2e00602ffffdeadbeef00000005# 15: Device 2 send flit 2200002ffffdeadbeef00000004# 15: Device 3 send flit 2600202ffffdeadbeef00000003# 15: [ENQ] data=2a00400ffffdeadbeef00000006# 15: [DEQ] data=2a00400ffffdeadbeef00000004# 15: [ENQ] data=2e00602ffffdeadbeef00000005# 15: [DEQ] data=2e00602ffffdeadbeef00000003# 15: [ENQ] data=2200002ffffdeadbeef00000004# 15: [DEQ] data=2200002ffffdeadbeef00000002# 15: [ENQ] data=2a00400ffffdeadbeef00000000# 15: [ENQ] data=2600202ffffdeadbeef00000003# 15: [DEQ] data=2600202ffffdeadbeef00000001# 15: [ENQ] data=2e00602ffffdeadbeef00000000# 15: [m0- t] addr=deadbeef00000006 dest=2# 15: [m1- t] addr=deadbeef00000005 dest=3# 16: Port 0 send flit 2a00400ffffdeadbeef00000005 (tail=0 dst=2 vc=1)# 16: Port 1 send flit 2e00602ffffdeadbeef00000004 (tail=0 dst=3 vc=1)# 16: Port 2 send flit 2200002ffffdeadbeef00000003 (tail=0 dst=0 vc=1)# 16: Port 3 send flit 2600202ffffdeadbeef00000002 (tail=0 dst=1 vc=1)# 16: Port 0 recv flit 2200002ffffdeadbeef00000000 (tail=0 dst=0 vc=1) # 16: Port 0 get a credit (vc=1)# 16: Port 1 get a credit (vc=1)# 16: Port 2 get a credit (vc=1)# 16: Port 3 get a credit (vc=1)# 16: Device 0 send flit 2a00400ffffdeadbeef00000007# 16: Device 1 send flit 2e00602ffffdeadbeef00000006# 16: Device 2 send flit 2200002ffffdeadbeef00000005# 16: Device 3 send flit 2600202ffffdeadbeef00000004# 16: [ENQ] data=2a00400ffffdeadbeef00000007# 16: [DEQ] data=2a00400ffffdeadbeef00000005# 16: [ENQ] data=2200002ffffdeadbeef00000000# 16: [ENQ] data=2e00602ffffdeadbeef00000006# 16: [DEQ] data=2e00602ffffdeadbeef00000004# 16: [ENQ] data=2200002ffffdeadbeef00000005# 16: [DEQ] data=2200002ffffdeadbeef00000003# 16: [ENQ] data=2600202ffffdeadbeef00000004# 16: [DEQ] data=2600202ffffdeadbeef00000002# 16: [m0- t] addr=deadbeef00000007 dest=2# 16: [m1- t] addr=deadbeef00000006 dest=3# 17: Port 0 send flit 2a00400ffffdeadbeef00000006 (tail=0 dst=2 vc=1)# 17: Port 1 send flit 2e00602ffffdeadbeef00000005 (tail=0 dst=3 vc=1)# 17: Port 2 send flit 2200002ffffdeadbeef00000004 (tail=0 dst=0 vc=1)# 17: Port 3 send flit 2600202ffffdeadbeef00000003 (tail=0 dst=1 vc=1)# 17: Port 0 recv flit 2200002ffffdeadbeef00000001 (tail=0 dst=0 vc=1) # 17: Port 3 recv flit 2e00602ffffdeadbeef00000001 (tail=0 dst=3 vc=1) # 17: Port 3 get a credit (vc=1)# 17: Port 2 put a credit (vc=1)# 17: Port 3 put a credit (vc=1)# 17: Device 0 send flit 2a00400ffffdeadbeef00000008# 17: Device 1 send flit 2e00602ffffdeadbeef00000007# 17: Device 2 send flit 2200002ffffdeadbeef00000006# 17: Device 3 send flit 2600202ffffdeadbeef00000005# 17: Device 2 recv flit 2a00400ffffdeadbeef00000000# 17: Device 3 recv flit 2e00602ffffdeadbeef00000000# 17: [ENQ] data=2a00400ffffdeadbeef00000008# 17: [DEQ] data=2a00400ffffdeadbeef00000006# 17: [ENQ] data=2200002ffffdeadbeef00000001# 17: [ENQ] data=2e00602ffffdeadbeef00000007# 17: [DEQ] data=2e00602ffffdeadbeef00000005# 17: [ENQ] data=2200002ffffdeadbeef00000006# 17: [DEQ] data=2200002ffffdeadbeef00000004# 17: [DEQ] data=2a00400ffffdeadbeef00000000# 17: [ENQ] data=2600202ffffdeadbeef00000005# 17: [DEQ] data=2600202ffffdeadbeef00000003# 17: [ENQ] data=2e00602ffffdeadbeef00000001# 17: [DEQ] data=2e00602ffffdeadbeef00000000# 17: [m0- t] addr=deadbeef00000008 dest=2# 17: [m1- t] addr=deadbeef00000007 dest=3# 18: Port 0 send flit 2a00400ffffdeadbeef00000007 (tail=0 dst=2 vc=1)# 18: Port 1 send flit 2e00602ffffdeadbeef00000006 (tail=0 dst=3 vc=1)# 18: Port 2 send flit 2200002ffffdeadbeef00000005 (tail=0 dst=0 vc=1)# 18: Port 3 send flit 2600202ffffdeadbeef00000004 (tail=0 dst=1 vc=1)# 18: Port 0 recv flit 2200002ffffdeadbeef00000002 (tail=0 dst=0 vc=1) # 18: Port 1 recv flit 2600202ffffdeadbeef00000000 (tail=0 dst=1 vc=1) # 18: Port 2 recv flit 2a00400ffffdeadbeef00000001 (tail=0 dst=2 vc=1) # 18: Port 3 recv flit 2e00602ffffdeadbeef00000002 (tail=0 dst=3 vc=1) # 18: Port 0 get a credit (vc=1)# 18: Port 1 get a credit (vc=1)# 18: Port 3 get a credit (vc=1)# 18: Port 0 put a credit (vc=1)# 18: Device 0 send flit 2a00400ffffdeadbeef00000009# 18: Device 1 send flit 2e00602ffffdeadbeef00000008# 18: Device 2 send flit 2200002ffffdeadbeef00000007# 18: Device 3 send flit 2600202ffffdeadbeef00000006# 18: Device 0 recv flit 2200002ffffdeadbeef00000000# 18: [ENQ] data=2a00400ffffdeadbeef00000009# 18: [DEQ] data=2a00400ffffdeadbeef00000007# 18: [ENQ] data=2200002ffffdeadbeef00000002# 18: [DEQ] data=2200002ffffdeadbeef00000000# 18: [ENQ] data=2e00602ffffdeadbeef00000008# 18: [DEQ] data=2e00602ffffdeadbeef00000006# 18: [ENQ] data=2600202ffffdeadbeef00000000# 18: [ENQ] data=2200002ffffdeadbeef00000007# 18: [DEQ] data=2200002ffffdeadbeef00000005# 18: [ENQ] data=2a00400ffffdeadbeef00000001# 18: [ENQ] data=2600202ffffdeadbeef00000006# 18: [DEQ] data=2600202ffffdeadbeef00000004# 18: [ENQ] data=2e00602ffffdeadbeef00000002# 18: [m0- t] addr=deadbeef00000009 dest=2# 18: [m1- t] addr=deadbeef00000008 dest=3# 19: Port 0 send flit 2a00400ffffdeadbeef00000008 (tail=0 dst=2 vc=1)# 19: Port 1 send flit 2e00602ffffdeadbeef00000007 (tail=0 dst=3 vc=1)# 19: Port 2 send flit 2200002ffffdeadbeef00000006 (tail=0 dst=0 vc=1)# 19: Port 3 send flit 2600202ffffdeadbeef00000005 (tail=0 dst=1 vc=1)# 19: Port 1 recv flit 2600202ffffdeadbeef00000001 (tail=0 dst=1 vc=1) # 19: Port 1 get a credit (vc=1)# 19: Port 2 get a credit (vc=1)# 19: Port 3 get a credit (vc=1)# 19: Port 3 put a credit (vc=1)# 19: Device 0 send flit 2a00400ffffdeadbeef0000000a# 19: Device 1 send flit 2e00602ffffdeadbeef00000009# 19: Device 2 send flit 2200002ffffdeadbeef00000008# 19: Device 3 send flit 2600202ffffdeadbeef00000007# 19: Device 3 recv flit 2e00602ffffdeadbeef00000001# 19: [ENQ] data=2a00400ffffdeadbeef0000000a# 19: [DEQ] data=2a00400ffffdeadbeef00000008# 19: [ENQ] data=2e00602ffffdeadbeef00000009# 19: [DEQ] data=2e00602ffffdeadbeef00000007# 19: [ENQ] data=2600202ffffdeadbeef00000001# 19: [ENQ] data=2200002ffffdeadbeef00000008# 19: [DEQ] data=2200002ffffdeadbeef00000006# 19: [ENQ] data=2600202ffffdeadbeef00000007# 19: [DEQ] data=2600202ffffdeadbeef00000005# 19: [DEQ] data=2e00602ffffdeadbeef00000001# 19: [m0- t] addr=deadbeef0000000a dest=2# 19: [m1- t] addr=deadbeef00000009 dest=3# 19: [s0- t] addr=deadbeef00000000 dest=0# 20: Port 0 send flit 2a00400ffffdeadbeef00000009 (tail=0 dst=2 vc=1)# 20: Port 1 send flit 2e00602ffffdeadbeef00000008 (tail=0 dst=3 vc=1)# 20: Port 2 send flit 2200002ffffdeadbeef00000007 (tail=0 dst=0 vc=1)# 20: Port 3 send flit 2600202ffffdeadbeef00000006 (tail=0 dst=1 vc=1)# 20: Port 3 recv flit 2e00602ffffdeadbeef00000003 (tail=0 dst=3 vc=1) # 20: Port 3 get a credit (vc=1)# 20: Port 0 put a credit (vc=1)# 20: Port 1 put a credit (vc=1)# 20: Port 2 put a credit (vc=1)# 20: Device 0 send flit 2a00400ffffdeadbeef0000000b# 20: Device 1 send flit 2e00602ffffdeadbeef0000000a# 20: Device 2 send flit 2200002ffffdeadbeef00000009# 20: Device 3 send flit 2600202ffffdeadbeef00000008# 20: Device 0 recv flit 2200002ffffdeadbeef00000001# 20: Device 1 recv flit 2600202ffffdeadbeef00000000# 20: Device 2 recv flit 2a00400ffffdeadbeef00000001# 20: [ENQ] data=2a00400ffffdeadbeef0000000b# 20: [DEQ] data=2a00400ffffdeadbeef00000009# 20: [DEQ] data=2200002ffffdeadbeef00000001# 20: [ENQ] data=2e00602ffffdeadbeef0000000a# 20: [DEQ] data=2e00602ffffdeadbeef00000008# 20: [DEQ] data=2600202ffffdeadbeef00000000# 20: [ENQ] data=2200002ffffdeadbeef00000009# 20: [DEQ] data=2200002ffffdeadbeef00000007# 20: [DEQ] data=2a00400ffffdeadbeef00000001# 20: [ENQ] data=2600202ffffdeadbeef00000008# 20: [DEQ] data=2600202ffffdeadbeef00000006# 20: [ENQ] data=2e00602ffffdeadbeef00000003# 20: [m0- t] addr=deadbeef0000000b dest=2# 20: [m1- t] addr=deadbeef0000000a dest=3# 21: Port 3 send flit 2600202ffffdeadbeef00000007 (tail=0 dst=1 vc=1)# 21: Port 0 recv flit 2200002ffffdeadbeef00000003 (tail=0 dst=0 vc=1) # 21: Port 1 recv flit 2600202ffffdeadbeef00000002 (tail=0 dst=1 vc=1) # 21: Port 2 recv flit 2a00400ffffdeadbeef00000002 (tail=0 dst=2 vc=1) # 21: Port 3 recv flit 2e00602ffffdeadbeef00000004 (tail=0 dst=3 vc=1) # 21: Port 0 get a credit (vc=1)# 21: Port 1 get a credit (vc=1)# 21: Port 3 get a credit (vc=1)# 21: Port 3 put a credit (vc=1)# 21: Device 0 send flit 2a00400ffffdeadbeef0000000c# 21: Device 1 send flit 2e00602ffffdeadbeef0000000b# 21: Device 2 send flit 2200002ffffdeadbeef0000000a# 21: Device 3 send flit 2600202ffffdeadbeef00000009# 21: Device 3 recv flit 2e00602ffffdeadbeef00000002# 21: [ENQ] data=2a00400ffffdeadbeef0000000c# 21: [ENQ] data=2200002ffffdeadbeef00000003# 21: [ENQ] data=2e00602ffffdeadbeef0000000b# 21: [ENQ] data=2600202ffffdeadbeef00000002# 21: [ENQ] data=2200002ffffdeadbeef0000000a# 21: [ENQ] data=2a00400ffffdeadbeef00000002# 21: [ENQ] data=2600202ffffdeadbeef00000009# 21: [DEQ] data=2600202ffffdeadbeef00000007# 21: [ENQ] data=2e00602ffffdeadbeef00000004# 21: [DEQ] data=2e00602ffffdeadbeef00000002# 21: [m0- t] addr=deadbeef0000000c dest=2# 21: [m1- t] addr=deadbeef0000000b dest=3# 21: [s0- t] addr=deadbeef00000001 dest=0# 21: [s1- t] addr=deadbeef00000000 dest=1# 22: Port 0 send flit 2a00400ffffdeadbeef0000000a (tail=0 dst=2 vc=1)# 22: Port 1 send flit 2e00602ffffdeadbeef00000009 (tail=0 dst=3 vc=1)# 22: Port 3 send flit 2600202ffffdeadbeef00000008 (tail=0 dst=1 vc=1)# 22: Port 1 recv flit 2600202ffffdeadbeef00000003 (tail=0 dst=1 vc=1) # 22: Port 1 get a credit (vc=1)# 22: Port 2 get a credit (vc=1)# 22: Port 3 get a credit (vc=1)# 22: Port 0 put a credit (vc=1)# 22: Port 1 put a credit (vc=1)# 22: Device 0 send flit 2a00400ffffdeadbeef0000000d# 22: Device 1 send flit 2e00602ffffdeadbeef0000000c# 22: Device 2 send flit 2200002ffffdeadbeef0000000b# 22: Device 3 send flit 2600202ffffdeadbeef0000000a# 22: Device 0 recv flit 2200002ffffdeadbeef00000002# 22: Device 1 recv flit 2600202ffffdeadbeef00000001# 22: [ENQ] data=2a00400ffffdeadbeef0000000d# 22: [DEQ] data=2a00400ffffdeadbeef0000000a# 22: [DEQ] data=2200002ffffdeadbeef00000002# 22: [ENQ] data=2e00602ffffdeadbeef0000000c# 22: [DEQ] data=2e00602ffffdeadbeef00000009# 22: [ENQ] data=2600202ffffdeadbeef00000003# 22: [DEQ] data=2600202ffffdeadbeef00000001# 22: [ENQ] data=2200002ffffdeadbeef0000000b# 22: [ENQ] data=2600202ffffdeadbeef0000000a# 22: [DEQ] data=2600202ffffdeadbeef00000008# 22: [m0- t] addr=deadbeef0000000d dest=2# 22: [m1- t] addr=deadbeef0000000c dest=3# 23: Port 1 send flit 2e00602ffffdeadbeef0000000a (tail=0 dst=3 vc=1)# 23: Port 2 send flit 2200002ffffdeadbeef00000008 (tail=0 dst=0 vc=1)# 23: Port 3 send flit 2600202ffffdeadbeef00000009 (tail=0 dst=1 vc=1)# 23: Port 3 recv flit 2e00602ffffdeadbeef00000005 (tail=0 dst=3 vc=1) # 23: Port 2 put a credit (vc=1)# 23: Port 3 put a credit (vc=1)# 23: Device 0 send flit 2a00400ffffdeadbeef0000000e# 23: Device 1 send flit 2e00602ffffdeadbeef0000000d# 23: Device 3 send flit 2600202ffffdeadbeef0000000b# 23: Device 2 recv flit 2a00400ffffdeadbeef00000002# 23: Device 3 recv flit 2e00602ffffdeadbeef00000003# 23: [ENQ] data=2a00400ffffdeadbeef0000000e# 23: [ENQ] data=2e00602ffffdeadbeef0000000d# 23: [DEQ] data=2e00602ffffdeadbeef0000000a# 23: [DEQ] data=2200002ffffdeadbeef00000008# 23: [DEQ] data=2a00400ffffdeadbeef00000002# 23: [ENQ] data=2600202ffffdeadbeef0000000b# 23: [DEQ] data=2600202ffffdeadbeef00000009# 23: [ENQ] data=2e00602ffffdeadbeef00000005# 23: [DEQ] data=2e00602ffffdeadbeef00000003# 23: [m0- t] addr=deadbeef0000000e dest=2# 23: [m1- t] addr=deadbeef0000000d dest=3# 23: [s0- t] addr=deadbeef00000002 dest=0# 23: [s1- t] addr=deadbeef00000001 dest=1# 24: Port 3 send flit 2600202ffffdeadbeef0000000a (tail=0 dst=1 vc=1)# 24: Port 0 recv flit 2200002ffffdeadbeef00000004 (tail=0 dst=0 vc=1) # 24: Port 1 recv flit 2600202ffffdeadbeef00000004 (tail=0 dst=1 vc=1) # 24: Port 2 recv flit 2a00400ffffdeadbeef00000003 (tail=0 dst=2 vc=1) # 24: Port 3 recv flit 2e00602ffffdeadbeef00000006 (tail=0 dst=3 vc=1) # 24: Port 0 get a credit (vc=1)# 24: Port 1 get a credit (vc=1)# 24: Port 3 get a credit (vc=1)# 24: Port 0 put a credit (vc=1)# 24: Port 1 put a credit (vc=1)# 24: Device 1 send flit 2e00602ffffdeadbeef0000000e# 24: Device 2 send flit 2200002ffffdeadbeef0000000c# 24: Device 3 send flit 2600202ffffdeadbeef0000000c# 24: Device 0 recv flit 2200002ffffdeadbeef00000003# 24: Device 1 recv flit 2600202ffffdeadbeef00000002# 24: [ENQ] data=2200002ffffdeadbeef00000004# 24: [DEQ] data=2200002ffffdeadbeef00000003# 24: [ENQ] data=2e00602ffffdeadbeef0000000e# 24: [ENQ] data=2600202ffffdeadbeef00000004# 24: [DEQ] data=2600202ffffdeadbeef00000002# 24: [ENQ] data=2200002ffffdeadbeef0000000c# 24: [ENQ] data=2a00400ffffdeadbeef00000003# 24: [ENQ] data=2600202ffffdeadbeef0000000c# 24: [DEQ] data=2600202ffffdeadbeef0000000a# 24: [ENQ] data=2e00602ffffdeadbeef00000006# 24: [m1- t] addr=deadbeef0000000e dest=3# 25: Port 0 send flit 2a00400ffffdeadbeef0000000b (tail=0 dst=2 vc=1)# 25: Port 1 send flit 2e00602ffffdeadbeef0000000b (tail=0 dst=3 vc=1)# 25: Port 3 send flit 2600202ffffdeadbeef0000000b (tail=0 dst=1 vc=1)# 25: Port 1 recv flit 2600202ffffdeadbeef00000005 (tail=0 dst=1 vc=1) # 25: Port 1 get a credit (vc=1)# 25: Port 2 get a credit (vc=1)# 25: Port 3 get a credit (vc=1)# 25: Port 3 put a credit (vc=1)# 25: Device 3 send flit 2600202ffffdeadbeef0000000d# 25: Device 3 recv flit 2e00602ffffdeadbeef00000004# 25: [DEQ] data=2a00400ffffdeadbeef0000000b# 25: [DEQ] data=2e00602ffffdeadbeef0000000b# 25: [ENQ] data=2600202ffffdeadbeef00000005# 25: [ENQ] data=2600202ffffdeadbeef0000000d# 25: [DEQ] data=2600202ffffdeadbeef0000000b# 25: [DEQ] data=2e00602ffffdeadbeef00000004# 25: [s0- t] addr=deadbeef00000003 dest=0# 25: [s1- t] addr=deadbeef00000002 dest=1# 26: Port 1 send flit 2e00602ffffdeadbeef0000000c (tail=0 dst=3 vc=1)# 26: Port 2 send flit 2200002ffffdeadbeef00000009 (tail=0 dst=0 vc=1)# 26: Port 3 send flit 2600202ffffdeadbeef0000000c (tail=0 dst=1 vc=1)# 26: Port 3 recv flit 2e00602ffffdeadbeef00000007 (tail=0 dst=3 vc=1) # 26: Port 0 put a credit (vc=1)# 26: Port 1 put a credit (vc=1)# 26: Port 2 put a credit (vc=1)# 26: Device 0 send flit 2a00400ffffdeadbeef0000000f# 26: Device 1 send flit 2e00602ffffdeadbeef0000000f# 26: Device 3 send flit 2600202ffffdeadbeef0000000e# 26: Device 0 recv flit 2200002ffffdeadbeef00000004# 26: Device 1 recv flit 2600202ffffdeadbeef00000003# 26: Device 2 recv flit 2a00400ffffdeadbeef00000003# 26: [ENQ] data=2a00400ffffdeadbeef0000000f# 26: [DEQ] data=2200002ffffdeadbeef00000004# 26: [ENQ] data=2e00602ffffdeadbeef0000000f# 26: [DEQ] data=2e00602ffffdeadbeef0000000c# 26: [DEQ] data=2600202ffffdeadbeef00000003# 26: [DEQ] data=2200002ffffdeadbeef00000009# 26: [DEQ] data=2a00400ffffdeadbeef00000003# 26: [ENQ] data=2600202ffffdeadbeef0000000e# 26: [DEQ] data=2600202ffffdeadbeef0000000c# 26: [ENQ] data=2e00602ffffdeadbeef00000007# 26: [m0- t] addr=deadbeef0000000f dest=2# 26: [m1- t] addr=deadbeef0000000f dest=3# 27: Port 3 send flit 2600202ffffdeadbeef0000000d (tail=0 dst=1 vc=1)# 27: Port 0 recv flit 2200002ffffdeadbeef00000005 (tail=0 dst=0 vc=1) # 27: Port 1 recv flit 2600202ffffdeadbeef00000006 (tail=0 dst=1 vc=1) # 27: Port 2 recv flit 2a00400ffffdeadbeef00000004 (tail=0 dst=2 vc=1) # 27: Port 3 recv flit 2e00602ffffdeadbeef00000008 (tail=0 dst=3 vc=1) # 27: Port 0 get a credit (vc=1)# 27: Port 1 get a credit (vc=1)# 27: Port 3 get a credit (vc=1)# 27: Port 3 put a credit (vc=1)# 27: Device 1 send flit 2e00602ffffdeadbeef00000010# 27: Device 2 send flit 2200002ffffdeadbeef0000000d# 27: Device 3 send flit 2600202ffffdeadbeef0000000f# 27: Device 3 recv flit 2e00602ffffdeadbeef00000005# 27: [ENQ] data=2200002ffffdeadbeef00000005# 27: [ENQ] data=2e00602ffffdeadbeef00000010# 27: [ENQ] data=2600202ffffdeadbeef00000006# 27: [ENQ] data=2200002ffffdeadbeef0000000d# 27: [ENQ] data=2a00400ffffdeadbeef00000004# 27: [ENQ] data=2600202ffffdeadbeef0000000f# 27: [DEQ] data=2600202ffffdeadbeef0000000d# 27: [ENQ] data=2e00602ffffdeadbeef00000008# 27: [DEQ] data=2e00602ffffdeadbeef00000005# 27: [m1- t] addr=deadbeef00000010 dest=3# 27: [s0- t] addr=deadbeef00000004 dest=0# 27: [s1- t] addr=deadbeef00000003 dest=1# 28: Port 0 send flit 2a00400ffffdeadbeef0000000c (tail=0 dst=2 vc=1)# 28: Port 1 send flit 2e00602ffffdeadbeef0000000d (tail=0 dst=3 vc=1)# 28: Port 3 send flit 2600202ffffdeadbeef0000000e (tail=0 dst=1 vc=1)# 28: Port 1 recv flit 2600202ffffdeadbeef00000007 (tail=0 dst=1 vc=1) # 28: Port 1 get a credit (vc=1)# 28: Port 2 get a credit (vc=1)# 28: Port 3 get a credit (vc=1)# 28: Port 1 put a credit (vc=1)# 28: Device 3 send flit 2600202ffffdeadbeef00000010# 28: Device 1 recv flit 2600202ffffdeadbeef00000004# 28: [DEQ] data=2a00400ffffdeadbeef0000000c# 28: [DEQ] data=2e00602ffffdeadbeef0000000d# 28: [ENQ] data=2600202ffffdeadbeef00000007# 28: [DEQ] data=2600202ffffdeadbeef00000004# 28: [ENQ] data=2600202ffffdeadbeef00000010# 28: [DEQ] data=2600202ffffdeadbeef0000000e# 29: Port 1 send flit 2e00602ffffdeadbeef0000000e (tail=0 dst=3 vc=1)# 29: Port 2 send flit 2200002ffffdeadbeef0000000a (tail=0 dst=0 vc=1)# 29: Port 3 send flit 2600202ffffdeadbeef0000000f (tail=0 dst=1 vc=1)# 29: Port 3 recv flit 2e00602ffffdeadbeef00000009 (tail=0 dst=3 vc=1) # 29: Port 0 put a credit (vc=1)# 29: Port 2 put a credit (vc=1)# 29: Port 3 put a credit (vc=1)# 29: Device 0 send flit 2a00400ffffdeadbeef00000010# 29: Device 1 send flit 2e00602ffffdeadbeef00000011# 29: Device 3 send flit 2600202ffffdeadbeef00000011# 29: Device 0 recv flit 2200002ffffdeadbeef00000005# 29: Device 2 recv flit 2a00400ffffdeadbeef00000004# 29: Device 3 recv flit 2e00602ffffdeadbeef00000006# 29: [ENQ] data=2a00400ffffdeadbeef00000010# 29: [DEQ] data=2200002ffffdeadbeef00000005# 29: [ENQ] data=2e00602ffffdeadbeef00000011# 29: [DEQ] data=2e00602ffffdeadbeef0000000e# 29: [DEQ] data=2200002ffffdeadbeef0000000a# 29: [DEQ] data=2a00400ffffdeadbeef00000004# 29: [ENQ] data=2600202ffffdeadbeef00000011# 29: [DEQ] data=2600202ffffdeadbeef0000000f# 29: [ENQ] data=2e00602ffffdeadbeef00000009# 29: [DEQ] data=2e00602ffffdeadbeef00000006# 29: [m0- t] addr=deadbeef00000010 dest=2# 29: [m1- t] addr=deadbeef00000011 dest=3# 29: [s1- t] addr=deadbeef00000004 dest=1# 30: Port 0 recv flit 2200002ffffdeadbeef00000006 (tail=0 dst=0 vc=1) # 30: Port 1 recv flit 2600202ffffdeadbeef00000008 (tail=0 dst=1 vc=1) # 30: Port 2 recv flit 2a00400ffffdeadbeef00000005 (tail=0 dst=2 vc=1) # 30: Port 3 recv flit 2e00602ffffdeadbeef0000000a (tail=0 dst=3 vc=1) # 30: Port 0 get a credit (vc=1)# 30: Port 1 get a credit (vc=1)# 30: Port 3 get a credit (vc=1)# 30: Port 1 put a credit (vc=1)# 30: Device 1 send flit 2e00602ffffdeadbeef00000012# 30: Device 2 send flit 2200002ffffdeadbeef0000000e# 30: Device 3 send flit 2600202ffffdeadbeef00000012# 30: Device 1 recv flit 2600202ffffdeadbeef00000005# 30: [ENQ] data=2200002ffffdeadbeef00000006# 30: [ENQ] data=2e00602ffffdeadbeef00000012# 30: [ENQ] data=2600202ffffdeadbeef00000008# 30: [DEQ] data=2600202ffffdeadbeef00000005# 30: [ENQ] data=2200002ffffdeadbeef0000000e# 30: [ENQ] data=2a00400ffffdeadbeef00000005# 30: [ENQ] data=2600202ffffdeadbeef00000012# 30: [ENQ] data=2e00602ffffdeadbeef0000000a# 30: [m1- t] addr=deadbeef00000012 dest=3# 30: [s0- t] addr=deadbeef00000005 dest=0# 31: Port 0 send flit 2a00400ffffdeadbeef0000000d (tail=0 dst=2 vc=1)# 31: Port 1 send flit 2e00602ffffdeadbeef0000000f (tail=0 dst=3 vc=1)# 31: Port 3 send flit 2600202ffffdeadbeef00000010 (tail=0 dst=1 vc=1)# 31: Port 1 recv flit 2600202ffffdeadbeef00000009 (tail=0 dst=1 vc=1) # 31: Port 1 get a credit (vc=1)# 31: Port 2 get a credit (vc=1)# 31: Port 3 get a credit (vc=1)# 31: Port 3 put a credit (vc=1)# 31: Device 3 send flit 2600202ffffdeadbeef00000013# 31: Device 3 recv flit 2e00602ffffdeadbeef00000007# 31: [DEQ] data=2a00400ffffdeadbeef0000000d# 31: [DEQ] data=2e00602ffffdeadbeef0000000f# 31: [ENQ] data=2600202ffffdeadbeef00000009# 31: [ENQ] data=2600202ffffdeadbeef00000013# 31: [DEQ] data=2600202ffffdeadbeef00000010# 31: [DEQ] data=2e00602ffffdeadbeef00000007# 31: [s1- t] addr=deadbeef00000005 dest=1# 32: Port 1 send flit 2e00602ffffdeadbeef00000010 (tail=0 dst=3 vc=1)# 32: Port 2 send flit 2200002ffffdeadbeef0000000b (tail=0 dst=0 vc=1)# 32: Port 3 send flit 2600202ffffdeadbeef00000011 (tail=0 dst=1 vc=1)# 32: Port 3 recv flit 2e00602ffffdeadbeef0000000b (tail=0 dst=3 vc=1) # 32: Port 0 put a credit (vc=1)# 32: Port 1 put a credit (vc=1)# 32: Port 2 put a credit (vc=1)# 32: Device 0 send flit 2a00400ffffdeadbeef00000011# 32: Device 1 send flit 2e00602ffffdeadbeef00000013# 32: Device 3 send flit 2600202ffffdeadbeef00000014# 32: Device 0 recv flit 2200002ffffdeadbeef00000006# 32: Device 1 recv flit 2600202ffffdeadbeef00000006# 32: Device 2 recv flit 2a00400ffffdeadbeef00000005# 32: [ENQ] data=2a00400ffffdeadbeef00000011# 32: [DEQ] data=2200002ffffdeadbeef00000006# 32: [ENQ] data=2e00602ffffdeadbeef00000013# 32: [DEQ] data=2e00602ffffdeadbeef00000010# 32: [DEQ] data=2600202ffffdeadbeef00000006# 32: [DEQ] data=2200002ffffdeadbeef0000000b# 32: [DEQ] data=2a00400ffffdeadbeef00000005# 32: [ENQ] data=2600202ffffdeadbeef00000014# 32: [DEQ] data=2600202ffffdeadbeef00000011# 32: [ENQ] data=2e00602ffffdeadbeef0000000b# 32: [m0- t] addr=deadbeef00000011 dest=2# 32: [m1- t] addr=deadbeef00000013 dest=3# 33: Port 0 recv flit 2200002ffffdeadbeef00000007 (tail=0 dst=0 vc=1) # 33: Port 1 recv flit 2600202ffffdeadbeef0000000a (tail=0 dst=1 vc=1) # 33: Port 2 recv flit 2a00400ffffdeadbeef00000006 (tail=0 dst=2 vc=1) # 33: Port 0 get a credit (vc=1)# 33: Port 1 get a credit (vc=1)# 33: Port 3 get a credit (vc=1)# 33: Port 3 put a credit (vc=1)# 33: Device 1 send flit 2e00602ffffdeadbeef00000014# 33: Device 2 send flit 2200002ffffdeadbeef0000000f# 33: Device 3 send flit 2600202ffffdeadbeef00000015# 33: Device 3 recv flit 2e00602ffffdeadbeef00000008# 33: [ENQ] data=2200002ffffdeadbeef00000007# 33: [ENQ] data=2e00602ffffdeadbeef00000014# 33: [ENQ] data=2600202ffffdeadbeef0000000a# 33: [ENQ] data=2200002ffffdeadbeef0000000f# 33: [ENQ] data=2a00400ffffdeadbeef00000006# 33: [ENQ] data=2600202ffffdeadbeef00000015# 33: [DEQ] data=2e00602ffffdeadbeef00000008# 33: [m1- t] addr=deadbeef00000014 dest=3# 33: [s0- t] addr=deadbeef00000006 dest=0# 33: [s1- t] addr=deadbeef00000006 dest=1# 34: Port 0 send flit 2a00400ffffdeadbeef0000000e (tail=0 dst=2 vc=1)# 34: Port 1 send flit 2e00602ffffdeadbeef00000011 (tail=0 dst=3 vc=1)# 34: Port 3 send flit 2600202ffffdeadbeef00000012 (tail=0 dst=1 vc=1)# 34: Port 3 recv flit 2e00602ffffdeadbeef0000000c (tail=0 dst=3 vc=1) # 34: Port 1 get a credit (vc=1)# 34: Port 2 get a credit (vc=1)# 34: Port 3 get a credit (vc=1)# 34: Port 1 put a credit (vc=1)# 34: Device 1 recv flit 2600202ffffdeadbeef00000007# 34: [DEQ] data=2a00400ffffdeadbeef0000000e# 34: [DEQ] data=2e00602ffffdeadbeef00000011# 34: [DEQ] data=2600202ffffdeadbeef00000007# 34: [DEQ] data=2600202ffffdeadbeef00000012# 34: [ENQ] data=2e00602ffffdeadbeef0000000c# 35: Port 1 send flit 2e00602ffffdeadbeef00000012 (tail=0 dst=3 vc=1)# 35: Port 2 send flit 2200002ffffdeadbeef0000000c (tail=0 dst=0 vc=1)# 35: Port 3 send flit 2600202ffffdeadbeef00000013 (tail=0 dst=1 vc=1)# 35: Port 1 recv flit 2600202ffffdeadbeef0000000b (tail=0 dst=1 vc=1) # 35: Port 0 put a credit (vc=1)# 35: Port 2 put a credit (vc=1)# 35: Port 3 put a credit (vc=1)# 35: Device 0 send flit 2a00400ffffdeadbeef00000012# 35: Device 1 send flit 2e00602ffffdeadbeef00000015# 35: Device 3 send flit 2600202ffffdeadbeef00000016# 35: Device 0 recv flit 2200002ffffdeadbeef00000007# 35: Device 2 recv flit 2a00400ffffdeadbeef00000006# 35: Device 3 recv flit 2e00602ffffdeadbeef00000009# 35: [ENQ] data=2a00400ffffdeadbeef00000012# 35: [DEQ] data=2200002ffffdeadbeef00000007# 35: [ENQ] data=2e00602ffffdeadbeef00000015# 35: [DEQ] data=2e00602ffffdeadbeef00000012# 35: [ENQ] data=2600202ffffdeadbeef0000000b# 35: [DEQ] data=2200002ffffdeadbeef0000000c# 35: [DEQ] data=2a00400ffffdeadbeef00000006# 35: [ENQ] data=2600202ffffdeadbeef00000016# 35: [DEQ] data=2600202ffffdeadbeef00000013# 35: [DEQ] data=2e00602ffffdeadbeef00000009# 35: [m0- t] addr=deadbeef00000012 dest=2# 35: [m1- t] addr=deadbeef00000015 dest=3# 35: [s1- t] addr=deadbeef00000007 dest=1# 36: Port 2 recv flit 2a00400ffffdeadbeef00000007 (tail=0 dst=2 vc=1) # 36: Port 3 recv flit 2e00602ffffdeadbeef0000000d (tail=0 dst=3 vc=1) # 36: Port 0 get a credit (vc=1)# 36: Port 1 get a credit (vc=1)# 36: Port 3 get a credit (vc=1)# 36: Port 1 put a credit (vc=1)# 36: Device 1 send flit 2e00602ffffdeadbeef00000016# 36: Device 2 send flit 2200002ffffdeadbeef00000010# 36: Device 3 send flit 3600203ffffdeadbeef00000017# 36: Device 1 recv flit 2600202ffffdeadbeef00000008# 36: [ENQ] data=2e00602ffffdeadbeef00000016# 36: [DEQ] data=2600202ffffdeadbeef00000008# 36: [ENQ] data=2200002ffffdeadbeef00000010# 36: [ENQ] data=2a00400ffffdeadbeef00000007# 36: [ENQ] data=3600203ffffdeadbeef00000017# 36: [ENQ] data=2e00602ffffdeadbeef0000000d# 36: [m1- t] addr=deadbeef00000016 dest=3# 36: [s0- t] addr=deadbeef00000007 dest=0# 37: Port 0 send flit 2a00400ffffdeadbeef0000000f (tail=0 dst=2 vc=1)# 37: Port 1 send flit 2e00602ffffdeadbeef00000013 (tail=0 dst=3 vc=1)# 37: Port 3 send flit 2600202ffffdeadbeef00000014 (tail=0 dst=1 vc=1)# 37: Port 0 recv flit 2200002ffffdeadbeef00000008 (tail=0 dst=0 vc=1) # 37: Port 1 recv flit 2600202ffffdeadbeef0000000c (tail=0 dst=1 vc=1) # 37: Port 1 get a credit (vc=1)# 37: Port 2 get a credit (vc=1)# 37: Port 3 get a credit (vc=1)# 37: Port 3 put a credit (vc=1)# 37: Device 3 recv flit 2e00602ffffdeadbeef0000000a# 37: [DEQ] data=2a00400ffffdeadbeef0000000f# 37: [ENQ] data=2200002ffffdeadbeef00000008# 37: [DEQ] data=2e00602ffffdeadbeef00000013# 37: [ENQ] data=2600202ffffdeadbeef0000000c# 37: [DEQ] data=2600202ffffdeadbeef00000014# 37: [DEQ] data=2e00602ffffdeadbeef0000000a# 37: [s1- t] addr=deadbeef00000008 dest=1# 38: Port 1 send flit 2e00602ffffdeadbeef00000014 (tail=0 dst=3 vc=1)# 38: Port 2 send flit 2200002ffffdeadbeef0000000d (tail=0 dst=0 vc=1)# 38: Port 3 send flit 2600202ffffdeadbeef00000015 (tail=0 dst=1 vc=1)# 38: Port 3 recv flit 2e00602ffffdeadbeef0000000e (tail=0 dst=3 vc=1) # 38: Port 1 put a credit (vc=1)# 38: Port 2 put a credit (vc=1)# 38: Device 0 send flit 2a00400ffffdeadbeef00000013# 38: Device 1 send flit 3e00603ffffdeadbeef00000017# 38: Device 1 recv flit 2600202ffffdeadbeef00000009# 38: Device 2 recv flit 2a00400ffffdeadbeef00000007# 38: [ENQ] data=2a00400ffffdeadbeef00000013# 38: [ENQ] data=3e00603ffffdeadbeef00000017# 38: [DEQ] data=2e00602ffffdeadbeef00000014# 38: [DEQ] data=2600202ffffdeadbeef00000009# 38: [DEQ] data=2200002ffffdeadbeef0000000d# 38: [DEQ] data=2a00400ffffdeadbeef00000007# 38: [DEQ] data=2600202ffffdeadbeef00000015# 38: [ENQ] data=2e00602ffffdeadbeef0000000e# 38: [m0- t] addr=deadbeef00000013 dest=2# 38: [m1- t] addr=deadbeef00000017 dest=3# 39: Port 1 recv flit 2600202ffffdeadbeef0000000d (tail=0 dst=1 vc=1) # 39: Port 2 recv flit 2a00400ffffdeadbeef00000008 (tail=0 dst=2 vc=1) # 39: Port 0 get a credit (vc=1)# 39: Port 1 get a credit (vc=1)# 39: Port 3 get a credit (vc=1)# 39: Port 0 put a credit (vc=1)# 39: Port 3 put a credit (vc=1)# 39: Device 2 send flit 2200002ffffdeadbeef00000011# 39: Device 0 recv flit 2200002ffffdeadbeef00000008# 39: Device 3 recv flit 2e00602ffffdeadbeef0000000b# 39: [DEQ] data=2200002ffffdeadbeef00000008# 39: [ENQ] data=2600202ffffdeadbeef0000000d# 39: [ENQ] data=2200002ffffdeadbeef00000011# 39: [ENQ] data=2a00400ffffdeadbeef00000008# 39: [DEQ] data=2e00602ffffdeadbeef0000000b# 39: [s1- t] addr=deadbeef00000009 dest=1# 40: Port 0 send flit 2a00400ffffdeadbeef00000010 (tail=0 dst=2 vc=1)# 40: Port 1 send flit 2e00602ffffdeadbeef00000015 (tail=0 dst=3 vc=1)# 40: Port 3 send flit 2600202ffffdeadbeef00000016 (tail=0 dst=1 vc=1)# 40: Port 3 recv flit 2e00602ffffdeadbeef0000000f (tail=0 dst=3 vc=1) # 40: Port 1 get a credit (vc=1)# 40: Port 2 get a credit (vc=1)# 40: Port 3 get a credit (vc=1)# 40: Port 1 put a credit (vc=1)# 40: Device 1 recv flit 2600202ffffdeadbeef0000000a# 40: [DEQ] data=2a00400ffffdeadbeef00000010# 40: [DEQ] data=2e00602ffffdeadbeef00000015# 40: [DEQ] data=2600202ffffdeadbeef0000000a# 40: [DEQ] data=2600202ffffdeadbeef00000016# 40: [ENQ] data=2e00602ffffdeadbeef0000000f# 40: [s0- t] addr=deadbeef00000008 dest=0# 41: Port 1 send flit 2e00602ffffdeadbeef00000016 (tail=0 dst=3 vc=1)# 41: Port 2 send flit 2200002ffffdeadbeef0000000e (tail=0 dst=0 vc=1)# 41: Port 3 send flit 3600203ffffdeadbeef00000017 (tail=1 dst=1 vc=1)# 41: Port 0 recv flit 2200002ffffdeadbeef00000009 (tail=0 dst=0 vc=1) # 41: Port 1 recv flit 2600202ffffdeadbeef0000000e (tail=0 dst=1 vc=1) # 41: Port 2 put a credit (vc=1)# 41: Port 3 put a credit (vc=1)# 41: Device 0 send flit 2a00400ffffdeadbeef00000014# 41: Device 2 recv flit 2a00400ffffdeadbeef00000008# 41: Device 3 recv flit 2e00602ffffdeadbeef0000000c# 41: [ENQ] data=2a00400ffffdeadbeef00000014# 41: [ENQ] data=2200002ffffdeadbeef00000009# 41: [DEQ] data=2e00602ffffdeadbeef00000016# 41: [ENQ] data=2600202ffffdeadbeef0000000e# 41: [DEQ] data=2200002ffffdeadbeef0000000e# 41: [DEQ] data=2a00400ffffdeadbeef00000008# 41: [DEQ] data=3600203ffffdeadbeef00000017# 41: [DEQ] data=2e00602ffffdeadbeef0000000c# 41: [m0- t] addr=deadbeef00000014 dest=2# 41: [s1- t] addr=deadbeef0000000a dest=1# 42: Port 2 recv flit 2a00400ffffdeadbeef00000009 (tail=0 dst=2 vc=1) # 42: Port 3 recv flit 2e00602ffffdeadbeef00000010 (tail=0 dst=3 vc=1) # 42: Port 0 get a credit (vc=1)# 42: Port 1 get a credit (vc=1)# 42: Port 3 get a credit (vc=1)# 42: Port 1 put a credit (vc=1)# 42: Device 2 send flit 2200002ffffdeadbeef00000012# 42: Device 1 recv flit 2600202ffffdeadbeef0000000b# 42: [DEQ] data=2600202ffffdeadbeef0000000b# 42: [ENQ] data=2200002ffffdeadbeef00000012# 42: [ENQ] data=2a00400ffffdeadbeef00000009# 42: [ENQ] data=2e00602ffffdeadbeef00000010# 43: Port 0 send flit 2a00400ffffdeadbeef00000011 (tail=0 dst=2 vc=1)# 43: Port 1 send flit 3e00603ffffdeadbeef00000017 (tail=1 dst=3 vc=1)# 43: Port 1 recv flit 2600202ffffdeadbeef0000000f (tail=0 dst=1 vc=1) # 43: Port 1 get a credit (vc=1)# 43: Port 3 get a credit (vc=1)# 43: Port 0 put a credit (vc=1)# 43: Port 3 put a credit (vc=1)# 43: Device 0 recv flit 2200002ffffdeadbeef00000009# 43: Device 3 recv flit 2e00602ffffdeadbeef0000000d# 43: [DEQ] data=2a00400ffffdeadbeef00000011# 43: [DEQ] data=2200002ffffdeadbeef00000009# 43: [DEQ] data=3e00603ffffdeadbeef00000017# 43: [ENQ] data=2600202ffffdeadbeef0000000f# 43: [DEQ] data=2e00602ffffdeadbeef0000000d# 43: [s1- t] addr=deadbeef0000000b dest=1# 44: Port 3 recv flit 2e00602ffffdeadbeef00000011 (tail=0 dst=3 vc=1) # 44: Port 1 put a credit (vc=1)# 44: Port 2 put a credit (vc=1)# 44: Device 0 send flit 2a00400ffffdeadbeef00000015# 44: Device 1 recv flit 2600202ffffdeadbeef0000000c# 44: Device 2 recv flit 2a00400ffffdeadbeef00000009# 44: [ENQ] data=2a00400ffffdeadbeef00000015# 44: [DEQ] data=2600202ffffdeadbeef0000000c# 44: [DEQ] data=2a00400ffffdeadbeef00000009# 44: [ENQ] data=2e00602ffffdeadbeef00000011# 44: [m0- t] addr=deadbeef00000015 dest=2# 44: [s0- t] addr=deadbeef00000009 dest=0# 45: Port 0 recv flit 2200002ffffdeadbeef0000000a (tail=0 dst=0 vc=1) # 45: Port 1 recv flit 2600202ffffdeadbeef00000010 (tail=0 dst=1 vc=1) # 45: Port 2 recv flit 2a00400ffffdeadbeef0000000a (tail=0 dst=2 vc=1) # 45: Port 0 get a credit (vc=1)# 45: Port 1 get a credit (vc=1)# 45: Port 3 get a credit (vc=1)# 45: Port 3 put a credit (vc=1)# 45: Device 3 recv flit 2e00602ffffdeadbeef0000000e# 45: [ENQ] data=2200002ffffdeadbeef0000000a# 45: [ENQ] data=2600202ffffdeadbeef00000010# 45: [ENQ] data=2a00400ffffdeadbeef0000000a# 45: [DEQ] data=2e00602ffffdeadbeef0000000e# 45: [s1- t] addr=deadbeef0000000c dest=1# 46: Port 0 send flit 2a00400ffffdeadbeef00000012 (tail=0 dst=2 vc=1)# 46: Port 3 recv flit 2e00602ffffdeadbeef00000012 (tail=0 dst=3 vc=1) # 46: Port 1 get a credit (vc=1)# 46: Port 2 get a credit (vc=1)# 46: Port 3 get a credit (vc=1)# 46: Port 1 put a credit (vc=1)# 46: Device 1 recv flit 2600202ffffdeadbeef0000000d# 46: [DEQ] data=2a00400ffffdeadbeef00000012# 46: [DEQ] data=2600202ffffdeadbeef0000000d# 46: [ENQ] data=2e00602ffffdeadbeef00000012# 47: Port 2 send flit 2200002ffffdeadbeef0000000f (tail=0 dst=0 vc=1)# 47: Port 1 recv flit 2600202ffffdeadbeef00000011 (tail=0 dst=1 vc=1) # 47: Port 0 put a credit (vc=1)# 47: Port 2 put a credit (vc=1)# 47: Port 3 put a credit (vc=1)# 47: Device 0 send flit 2a00400ffffdeadbeef00000016# 47: Device 0 recv flit 2200002ffffdeadbeef0000000a# 47: Device 2 recv flit 2a00400ffffdeadbeef0000000a# 47: Device 3 recv flit 2e00602ffffdeadbeef0000000f# 47: [ENQ] data=2a00400ffffdeadbeef00000016# 47: [DEQ] data=2200002ffffdeadbeef0000000a# 47: [ENQ] data=2600202ffffdeadbeef00000011# 47: [DEQ] data=2200002ffffdeadbeef0000000f# 47: [DEQ] data=2a00400ffffdeadbeef0000000a# 47: [DEQ] data=2e00602ffffdeadbeef0000000f# 47: [m0- t] addr=deadbeef00000016 dest=2# 47: [s1- t] addr=deadbeef0000000d dest=1# 48: Port 2 recv flit 2a00400ffffdeadbeef0000000b (tail=0 dst=2 vc=1) # 48: Port 3 recv flit 2e00602ffffdeadbeef00000013 (tail=0 dst=3 vc=1) # 48: Port 0 get a credit (vc=1)# 48: Port 1 get a credit (vc=1)# 48: Port 1 put a credit (vc=1)# 48: Device 2 send flit 2200002ffffdeadbeef00000013# 48: Device 1 recv flit 2600202ffffdeadbeef0000000e# 48: [DEQ] data=2600202ffffdeadbeef0000000e# 48: [ENQ] data=2200002ffffdeadbeef00000013# 48: [ENQ] data=2a00400ffffdeadbeef0000000b# 48: [ENQ] data=2e00602ffffdeadbeef00000013# 48: [s0- t] addr=deadbeef0000000a dest=0# 49: Port 0 send flit 2a00400ffffdeadbeef00000013 (tail=0 dst=2 vc=1)# 49: Port 1 recv flit 2600202ffffdeadbeef00000012 (tail=0 dst=1 vc=1) # 49: Port 3 put a credit (vc=1)# 49: Device 3 recv flit 2e00602ffffdeadbeef00000010# 49: [DEQ] data=2a00400ffffdeadbeef00000013# 49: [ENQ] data=2600202ffffdeadbeef00000012# 49: [DEQ] data=2e00602ffffdeadbeef00000010# 49: [s1- t] addr=deadbeef0000000e dest=1# 50: Port 2 recv flit 2a00400ffffdeadbeef0000000c (tail=0 dst=2 vc=1) # 50: Port 3 recv flit 2e00602ffffdeadbeef00000014 (tail=0 dst=3 vc=1) # 50: Port 0 get a credit (vc=1)# 50: Port 1 put a credit (vc=1)# 50: Port 2 put a credit (vc=1)# 50: Device 0 send flit 3a00401ffffdeadbeef00000017# 50: Device 1 recv flit 2600202ffffdeadbeef0000000f# 50: Device 2 recv flit 2a00400ffffdeadbeef0000000b# 50: [ENQ] data=3a00401ffffdeadbeef00000017# 50: [DEQ] data=2600202ffffdeadbeef0000000f# 50: [ENQ] data=2a00400ffffdeadbeef0000000c# 50: [DEQ] data=2a00400ffffdeadbeef0000000b# 50: [ENQ] data=2e00602ffffdeadbeef00000014# 50: [m0- t] addr=deadbeef00000017 dest=2# 51: Port 0 send flit 2a00400ffffdeadbeef00000014 (tail=0 dst=2 vc=1)# 51: Port 1 recv flit 2600202ffffdeadbeef00000013 (tail=0 dst=1 vc=1) # 51: Port 2 recv flit 2a00400ffffdeadbeef0000000d (tail=0 dst=2 vc=1) # 51: Port 0 get a credit (vc=1)# 51: Port 3 put a credit (vc=1)# 51: Device 3 recv flit 2e00602ffffdeadbeef00000011# 51: [DEQ] data=2a00400ffffdeadbeef00000014# 51: [ENQ] data=2600202ffffdeadbeef00000013# 51: [ENQ] data=2a00400ffffdeadbeef0000000d# 51: [DEQ] data=2e00602ffffdeadbeef00000011# 51: [s1- t] addr=deadbeef0000000f dest=1# 52: Port 0 send flit 2a00400ffffdeadbeef00000015 (tail=0 dst=2 vc=1)# 52: Port 2 recv flit 2a00400ffffdeadbeef0000000e (tail=0 dst=2 vc=1) # 52: Port 3 recv flit 2e00602ffffdeadbeef00000015 (tail=0 dst=3 vc=1) # 52: Port 0 get a credit (vc=1)# 52: Port 2 get a credit (vc=1)# 52: Port 1 put a credit (vc=1)# 52: Port 2 put a credit (vc=1)# 52: Device 1 recv flit 2600202ffffdeadbeef00000010# 52: Device 2 recv flit 2a00400ffffdeadbeef0000000c# 52: [DEQ] data=2a00400ffffdeadbeef00000015# 52: [DEQ] data=2600202ffffdeadbeef00000010# 52: [ENQ] data=2a00400ffffdeadbeef0000000e# 52: [DEQ] data=2a00400ffffdeadbeef0000000c# 52: [ENQ] data=2e00602ffffdeadbeef00000015# 53: Port 0 send flit 2a00400ffffdeadbeef00000016 (tail=0 dst=2 vc=1)# 53: Port 2 send flit 2200002ffffdeadbeef00000010 (tail=0 dst=0 vc=1)# 53: Port 0 recv flit 2200002ffffdeadbeef0000000b (tail=0 dst=0 vc=1) # 53: Port 1 recv flit 2600202ffffdeadbeef00000014 (tail=0 dst=1 vc=1) # 53: Port 2 recv flit 2a00400ffffdeadbeef0000000f (tail=0 dst=2 vc=1) # 53: Port 0 get a credit (vc=1)# 53: Port 3 put a credit (vc=1)# 53: Device 3 recv flit 2e00602ffffdeadbeef00000012# 53: [DEQ] data=2a00400ffffdeadbeef00000016# 53: [ENQ] data=2200002ffffdeadbeef0000000b# 53: [ENQ] data=2600202ffffdeadbeef00000014# 53: [DEQ] data=2200002ffffdeadbeef00000010# 53: [ENQ] data=2a00400ffffdeadbeef0000000f# 53: [DEQ] data=2e00602ffffdeadbeef00000012# 53: [s1- t] addr=deadbeef00000010 dest=1# 54: Port 0 send flit 3a00401ffffdeadbeef00000017 (tail=1 dst=2 vc=1)# 54: Port 2 recv flit 2a00400ffffdeadbeef00000010 (tail=0 dst=2 vc=1) # 54: Port 3 recv flit 2e00602ffffdeadbeef00000016 (tail=0 dst=3 vc=1) # 54: Port 0 get a credit (vc=1)# 54: Port 1 put a credit (vc=1)# 54: Port 2 put a credit (vc=1)# 54: Device 2 send flit 2200002ffffdeadbeef00000014# 54: Device 1 recv flit 2600202ffffdeadbeef00000011# 54: Device 2 recv flit 2a00400ffffdeadbeef0000000d# 54: [DEQ] data=3a00401ffffdeadbeef00000017# 54: [DEQ] data=2600202ffffdeadbeef00000011# 54: [ENQ] data=2200002ffffdeadbeef00000014# 54: [ENQ] data=2a00400ffffdeadbeef00000010# 54: [DEQ] data=2a00400ffffdeadbeef0000000d# 54: [ENQ] data=2e00602ffffdeadbeef00000016# 55: Port 1 recv flit 2600202ffffdeadbeef00000015 (tail=0 dst=1 vc=1) # 55: Port 2 recv flit 2a00400ffffdeadbeef00000011 (tail=0 dst=2 vc=1) # 55: Port 0 get a credit (vc=1)# 55: Port 0 put a credit (vc=1)# 55: Port 3 put a credit (vc=1)# 55: Device 0 recv flit 2200002ffffdeadbeef0000000b# 55: Device 3 recv flit 2e00602ffffdeadbeef00000013# 55: [DEQ] data=2200002ffffdeadbeef0000000b# 55: [ENQ] data=2600202ffffdeadbeef00000015# 55: [ENQ] data=2a00400ffffdeadbeef00000011# 55: [DEQ] data=2e00602ffffdeadbeef00000013# 55: [s1- t] addr=deadbeef00000011 dest=1# 56: Port 3 recv flit 3e00603ffffdeadbeef00000017 (tail=1 dst=3 vc=1) # 56: Port 0 get a credit (vc=1)# 56: Port 2 get a credit (vc=1)# 56: Port 1 put a credit (vc=1)# 56: Port 2 put a credit (vc=1)# 56: Device 1 recv flit 2600202ffffdeadbeef00000012# 56: Device 2 recv flit 2a00400ffffdeadbeef0000000e# 56: [DEQ] data=2600202ffffdeadbeef00000012# 56: [DEQ] data=2a00400ffffdeadbeef0000000e# 56: [ENQ] data=3e00603ffffdeadbeef00000017# 56: [s0- t] addr=deadbeef0000000b dest=0# 57: Port 2 send flit 2200002ffffdeadbeef00000011 (tail=0 dst=0 vc=1)# 57: Port 0 recv flit 2200002ffffdeadbeef0000000c (tail=0 dst=0 vc=1) # 57: Port 1 recv flit 2600202ffffdeadbeef00000016 (tail=0 dst=1 vc=1) # 57: Port 2 recv flit 2a00400ffffdeadbeef00000012 (tail=0 dst=2 vc=1) # 57: Port 0 get a credit (vc=1)# 57: Port 2 get a credit (vc=1)# 57: Port 3 put a credit (vc=1)# 57: Device 3 recv flit 2e00602ffffdeadbeef00000014# 57: [ENQ] data=2200002ffffdeadbeef0000000c# 57: [ENQ] data=2600202ffffdeadbeef00000016# 57: [DEQ] data=2200002ffffdeadbeef00000011# 57: [ENQ] data=2a00400ffffdeadbeef00000012# 57: [DEQ] data=2e00602ffffdeadbeef00000014# 57: [s1- t] addr=deadbeef00000012 dest=1# 58: Port 2 send flit 2200002ffffdeadbeef00000012 (tail=0 dst=0 vc=1)# 58: Port 2 get a credit (vc=1)# 58: Port 1 put a credit (vc=1)# 58: Port 2 put a credit (vc=1)# 58: Device 2 send flit 2200002ffffdeadbeef00000015# 58: Device 1 recv flit 2600202ffffdeadbeef00000013# 58: Device 2 recv flit 2a00400ffffdeadbeef0000000f# 58: [DEQ] data=2600202ffffdeadbeef00000013# 58: [ENQ] data=2200002ffffdeadbeef00000015# 58: [DEQ] data=2200002ffffdeadbeef00000012# 58: [DEQ] data=2a00400ffffdeadbeef0000000f# 59: Port 2 send flit 2200002ffffdeadbeef00000013 (tail=0 dst=0 vc=1)# 59: Port 1 recv flit 3600203ffffdeadbeef00000017 (tail=1 dst=1 vc=1) # 59: Port 2 recv flit 2a00400ffffdeadbeef00000013 (tail=0 dst=2 vc=1) # 59: Port 0 put a credit (vc=1)# 59: Port 3 put a credit (vc=1)# 59: Device 2 send flit 2200002ffffdeadbeef00000016# 59: Device 0 recv flit 2200002ffffdeadbeef0000000c# 59: Device 3 recv flit 2e00602ffffdeadbeef00000015# 59: [DEQ] data=2200002ffffdeadbeef0000000c# 59: [ENQ] data=3600203ffffdeadbeef00000017# 59: [ENQ] data=2200002ffffdeadbeef00000016# 59: [DEQ] data=2200002ffffdeadbeef00000013# 59: [ENQ] data=2a00400ffffdeadbeef00000013# 59: [DEQ] data=2e00602ffffdeadbeef00000015# 59: [s1- t] addr=deadbeef00000013 dest=1# 60: Port 2 get a credit (vc=1)# 60: Port 1 put a credit (vc=1)# 60: Port 2 put a credit (vc=1)# 60: Device 2 send flit 3200003ffffdeadbeef00000017# 60: Device 1 recv flit 2600202ffffdeadbeef00000014# 60: Device 2 recv flit 2a00400ffffdeadbeef00000010# 60: [DEQ] data=2600202ffffdeadbeef00000014# 60: [ENQ] data=3200003ffffdeadbeef00000017# 60: [DEQ] data=2a00400ffffdeadbeef00000010# 60: [s0- t] addr=deadbeef0000000c dest=0# 61: Port 2 send flit 2200002ffffdeadbeef00000014 (tail=0 dst=0 vc=1)# 61: Port 0 recv flit 2200002ffffdeadbeef0000000d (tail=0 dst=0 vc=1) # 61: Port 2 recv flit 2a00400ffffdeadbeef00000014 (tail=0 dst=2 vc=1) # 61: Port 2 get a credit (vc=1)# 61: Port 3 put a credit (vc=1)# 61: Device 3 recv flit 2e00602ffffdeadbeef00000016# 61: [ENQ] data=2200002ffffdeadbeef0000000d# 61: [DEQ] data=2200002ffffdeadbeef00000014# 61: [ENQ] data=2a00400ffffdeadbeef00000014# 61: [DEQ] data=2e00602ffffdeadbeef00000016# 61: [s1- t] addr=deadbeef00000014 dest=1# 62: Port 2 send flit 2200002ffffdeadbeef00000015 (tail=0 dst=0 vc=1)# 62: Port 0 recv flit 2200002ffffdeadbeef0000000e (tail=0 dst=0 vc=1) # 62: Port 2 get a credit (vc=1)# 62: Port 1 put a credit (vc=1)# 62: Port 2 put a credit (vc=1)# 62: Device 1 recv flit 2600202ffffdeadbeef00000015# 62: Device 2 recv flit 2a00400ffffdeadbeef00000011# 62: [ENQ] data=2200002ffffdeadbeef0000000e# 62: [DEQ] data=2600202ffffdeadbeef00000015# 62: [DEQ] data=2200002ffffdeadbeef00000015# 62: [DEQ] data=2a00400ffffdeadbeef00000011# 63: Port 2 send flit 2200002ffffdeadbeef00000016 (tail=0 dst=0 vc=1)# 63: Port 0 recv flit 2200002ffffdeadbeef0000000f (tail=0 dst=0 vc=1) # 63: Port 2 recv flit 2a00400ffffdeadbeef00000015 (tail=0 dst=2 vc=1) # 63: Port 2 get a credit (vc=1)# 63: Port 0 put a credit (vc=1)# 63: Port 3 put a credit (vc=1)# 63: Device 0 recv flit 2200002ffffdeadbeef0000000d# 63: Device 3 recv flit 3e00603ffffdeadbeef00000017# 63: [ENQ] data=2200002ffffdeadbeef0000000f# 63: [DEQ] data=2200002ffffdeadbeef0000000d# 63: [DEQ] data=2200002ffffdeadbeef00000016# 63: [ENQ] data=2a00400ffffdeadbeef00000015# 63: [DEQ] data=3e00603ffffdeadbeef00000017# 63: [s1- t] addr=deadbeef00000015 dest=1# 64: Port 2 send flit 3200003ffffdeadbeef00000017 (tail=1 dst=0 vc=1)# 64: Port 0 recv flit 2200002ffffdeadbeef00000010 (tail=0 dst=0 vc=1) # 64: Port 2 get a credit (vc=1)# 64: Port 1 put a credit (vc=1)# 64: Port 2 put a credit (vc=1)# 64: Device 1 recv flit 2600202ffffdeadbeef00000016# 64: Device 2 recv flit 2a00400ffffdeadbeef00000012# 64: [ENQ] data=2200002ffffdeadbeef00000010# 64: [DEQ] data=2600202ffffdeadbeef00000016# 64: [DEQ] data=3200003ffffdeadbeef00000017# 64: [DEQ] data=2a00400ffffdeadbeef00000012# 64: [s0- t] addr=deadbeef0000000d dest=0# 65: Port 0 recv flit 2200002ffffdeadbeef00000011 (tail=0 dst=0 vc=1) # 65: Port 2 recv flit 2a00400ffffdeadbeef00000016 (tail=0 dst=2 vc=1) # 65: Port 2 get a credit (vc=1)# 65: Port 0 put a credit (vc=1)# 65: Device 0 recv flit 2200002ffffdeadbeef0000000e# 65: [ENQ] data=2200002ffffdeadbeef00000011# 65: [DEQ] data=2200002ffffdeadbeef0000000e# 65: [ENQ] data=2a00400ffffdeadbeef00000016# 65: [s1- t] addr=deadbeef00000016 dest=1# 66: Port 0 recv flit 2200002ffffdeadbeef00000012 (tail=0 dst=0 vc=1) # 66: Port 2 get a credit (vc=1)# 66: Port 1 put a credit (vc=1)# 66: Port 2 put a credit (vc=1)# 66: Device 1 recv flit 3600203ffffdeadbeef00000017# 66: Device 2 recv flit 2a00400ffffdeadbeef00000013# 66: [ENQ] data=2200002ffffdeadbeef00000012# 66: [DEQ] data=3600203ffffdeadbeef00000017# 66: [DEQ] data=2a00400ffffdeadbeef00000013# 66: [s0- t] addr=deadbeef0000000e dest=0# 67: Port 2 recv flit 3a00401ffffdeadbeef00000017 (tail=1 dst=2 vc=1) # 67: Port 2 get a credit (vc=1)# 67: Port 0 put a credit (vc=1)# 67: Device 0 recv flit 2200002ffffdeadbeef0000000f# 67: [DEQ] data=2200002ffffdeadbeef0000000f# 67: [ENQ] data=3a00401ffffdeadbeef00000017# 67: [s1- t] addr=deadbeef00000017 dest=1# 68: Port 0 recv flit 2200002ffffdeadbeef00000013 (tail=0 dst=0 vc=1) # 68: Port 2 put a credit (vc=1)# 68: Device 2 recv flit 2a00400ffffdeadbeef00000014# 68: [ENQ] data=2200002ffffdeadbeef00000013# 68: [DEQ] data=2a00400ffffdeadbeef00000014# 68: [s0- t] addr=deadbeef0000000f dest=0# 69: Port 0 put a credit (vc=1)# 69: Device 0 recv flit 2200002ffffdeadbeef00000010# 69: [DEQ] data=2200002ffffdeadbeef00000010# 70: Port 0 recv flit 2200002ffffdeadbeef00000014 (tail=0 dst=0 vc=1) # 70: Port 2 put a credit (vc=1)# 70: Device 2 recv flit 2a00400ffffdeadbeef00000015# 70: [ENQ] data=2200002ffffdeadbeef00000014# 70: [DEQ] data=2a00400ffffdeadbeef00000015# 70: [s0- t] addr=deadbeef00000010 dest=0# 71: Port 0 put a credit (vc=1)# 71: Device 0 recv flit 2200002ffffdeadbeef00000011# 71: [DEQ] data=2200002ffffdeadbeef00000011# 72: Port 0 recv flit 2200002ffffdeadbeef00000015 (tail=0 dst=0 vc=1) # 72: Port 2 put a credit (vc=1)# 72: Device 2 recv flit 2a00400ffffdeadbeef00000016# 72: [ENQ] data=2200002ffffdeadbeef00000015# 72: [DEQ] data=2a00400ffffdeadbeef00000016# 72: [s0- t] addr=deadbeef00000011 dest=0# 73: Port 0 put a credit (vc=1)# 73: Device 0 recv flit 2200002ffffdeadbeef00000012# 73: [DEQ] data=2200002ffffdeadbeef00000012# 74: Port 0 recv flit 2200002ffffdeadbeef00000016 (tail=0 dst=0 vc=1) # 74: Port 2 put a credit (vc=1)# 74: Device 2 recv flit 3a00401ffffdeadbeef00000017# 74: [ENQ] data=2200002ffffdeadbeef00000016# 74: [DEQ] data=3a00401ffffdeadbeef00000017# 74: [s0- t] addr=deadbeef00000012 dest=0# 75: Port 0 put a credit (vc=1)# 75: Device 0 recv flit 2200002ffffdeadbeef00000013# 75: [DEQ] data=2200002ffffdeadbeef00000013# 76: Port 0 recv flit 3200003ffffdeadbeef00000017 (tail=1 dst=0 vc=1) # 76: [ENQ] data=3200003ffffdeadbeef00000017# 76: [s0- t] addr=deadbeef00000013 dest=0# 77: Port 0 put a credit (vc=1)# 77: Device 0 recv flit 2200002ffffdeadbeef00000014# 77: [DEQ] data=2200002ffffdeadbeef00000014# 78: [s0- t] addr=deadbeef00000014 dest=0# 79: Port 0 put a credit (vc=1)# 79: Device 0 recv flit 2200002ffffdeadbeef00000015# 79: [DEQ] data=2200002ffffdeadbeef00000015# 80: [s0- t] addr=deadbeef00000015 dest=0# 81: Port 0 put a credit (vc=1)# 81: Device 0 recv flit 2200002ffffdeadbeef00000016# 81: [DEQ] data=2200002ffffdeadbeef00000016# 82: [s0- t] addr=deadbeef00000016 dest=0# 83: Port 0 put a credit (vc=1)# 83: Device 0 recv flit 3200003ffffdeadbeef00000017# 83: [DEQ] data=3200003ffffdeadbeef00000017# 84: [s0- t] addr=deadbeef00000017 dest=0# actual:deadbeef00000000 expected:deadbeef00000000# actual:deadbeef00000001 expected:deadbeef00000001# actual:deadbeef00000002 expected:deadbeef00000002# actual:deadbeef00000003 expected:deadbeef00000003# actual:deadbeef00000004 expected:deadbeef00000004# actual:deadbeef00000005 expected:deadbeef00000005# actual:deadbeef00000006 expected:deadbeef00000006# actual:deadbeef00000007 expected:deadbeef00000007# actual:deadbeef00000008 expected:deadbeef00000008# actual:deadbeef00000009 expected:deadbeef00000009# actual:deadbeef0000000a expected:deadbeef0000000a# actual:deadbeef0000000b expected:deadbeef0000000b# actual:deadbeef0000000c expected:deadbeef0000000c# actual:deadbeef0000000d expected:deadbeef0000000d# actual:deadbeef0000000e expected:deadbeef0000000e# actual:deadbeef0000000f expected:deadbeef0000000f# actual:deadbeef00000010 expected:deadbeef00000010# actual:deadbeef00000011 expected:deadbeef00000011# actual:deadbeef00000012 expected:deadbeef00000012# actual:deadbeef00000013 expected:deadbeef00000013# actual:deadbeef00000014 expected:deadbeef00000014# actual:deadbeef00000015 expected:deadbeef00000015# actual:deadbeef00000016 expected:deadbeef00000016# actual:deadbeef00000017 expected:deadbeef00000017# Pass 2# actual:deadbeef00000000 expected:deadbeef00000000# actual:deadbeef00000001 expected:deadbeef00000001# actual:deadbeef00000002 expected:deadbeef00000002# actual:deadbeef00000003 expected:deadbeef00000003# actual:deadbeef00000004 expected:deadbeef00000004# actual:deadbeef00000005 expected:deadbeef00000005# actual:deadbeef00000006 expected:deadbeef00000006# actual:deadbeef00000007 expected:deadbeef00000007# actual:deadbeef00000008 expected:deadbeef00000008# actual:deadbeef00000009 expected:deadbeef00000009# actual:deadbeef0000000a expected:deadbeef0000000a# actual:deadbeef0000000b expected:deadbeef0000000b# actual:deadbeef0000000c expected:deadbeef0000000c# actual:deadbeef0000000d expected:deadbeef0000000d# actual:deadbeef0000000e expected:deadbeef0000000e# actual:deadbeef0000000f expected:deadbeef0000000f# actual:deadbeef00000010 expected:deadbeef00000010# actual:deadbeef00000011 expected:deadbeef00000011# actual:deadbeef00000012 expected:deadbeef00000012# actual:deadbeef00000013 expected:deadbeef00000013# actual:deadbeef00000014 expected:deadbeef00000014# actual:deadbeef00000015 expected:deadbeef00000015# actual:deadbeef00000016 expected:deadbeef00000016# actual:deadbeef00000017 expected:deadbeef00000017# Pass 3# actual:deadbeef00000000 expected:deadbeef00000000# actual:deadbeef00000001 expected:deadbeef00000001# actual:deadbeef00000002 expected:deadbeef00000002# actual:deadbeef00000003 expected:deadbeef00000003# actual:deadbeef00000004 expected:deadbeef00000004# actual:deadbeef00000005 expected:deadbeef00000005# actual:deadbeef00000006 expected:deadbeef00000006# actual:deadbeef00000007 expected:deadbeef00000007# actual:deadbeef00000008 expected:deadbeef00000008# actual:deadbeef00000009 expected:deadbeef00000009# actual:deadbeef0000000a expected:deadbeef0000000a# actual:deadbeef0000000b expected:deadbeef0000000b# actual:deadbeef0000000c expected:deadbeef0000000c# actual:deadbeef0000000d expected:deadbeef0000000d# actual:deadbeef0000000e expected:deadbeef0000000e# actual:deadbeef0000000f expected:deadbeef0000000f# actual:deadbeef00000010 expected:deadbeef00000010# actual:deadbeef00000011 expected:deadbeef00000011# actual:deadbeef00000012 expected:deadbeef00000012# actual:deadbeef00000013 expected:deadbeef00000013# actual:deadbeef00000014 expected:deadbeef00000014# actual:deadbeef00000015 expected:deadbeef00000015# actual:deadbeef00000016 expected:deadbeef00000016# actual:deadbeef00000017 expected:deadbeef00000017# Pass 0# actual:deadbeef00000000 expected:deadbeef00000000# actual:deadbeef00000001 expected:deadbeef00000001# actual:deadbeef00000002 expected:deadbeef00000002# actual:deadbeef00000003 expected:deadbeef00000003# actual:deadbeef00000004 expected:deadbeef00000004# actual:deadbeef00000005 expected:deadbeef00000005# actual:deadbeef00000006 expected:deadbeef00000006# actual:deadbeef00000007 expected:deadbeef00000007# actual:deadbeef00000008 expected:deadbeef00000008# actual:deadbeef00000009 expected:deadbeef00000009# actual:deadbeef0000000a expected:deadbeef0000000a# actual:deadbeef0000000b expected:deadbeef0000000b# actual:deadbeef0000000c expected:deadbeef0000000c# actual:deadbeef0000000d expected:deadbeef0000000d# actual:deadbeef0000000e expected:deadbeef0000000e# actual:deadbeef0000000f expected:deadbeef0000000f# actual:deadbeef00000010 expected:deadbeef00000010# actual:deadbeef00000011 expected:deadbeef00000011# actual:deadbeef00000012 expected:deadbeef00000012# actual:deadbeef00000013 expected:deadbeef00000013# actual:deadbeef00000014 expected:deadbeef00000014# actual:deadbeef00000015 expected:deadbeef00000015# actual:deadbeef00000016 expected:deadbeef00000016# actual:deadbeef00000017 expected:deadbeef00000017# Pass 1# ** Note: $finish : testbench_sample_axi4stream.sv(74)# Time: 1110 ns Iteration: 0 Instance: /CONNECT_testbench_sample_axi4stream# End time: 10:47:19 on Jun 08,2022, Elapsed time: 0:00:01# Errors: 0, Warnings: 0" }, { "title": "《内存一致性与缓存一致性》笔记（三）：TSO与x86内存模型", "url": "/posts/MCCC3/", "categories": "读书笔记, 内存一致性与缓存一致性", "tags": "", "date": "2022-06-06 03:03:00 +0000", "snippet": "TSO（Total Store Ordering）是一个被广泛使用的内存模型，在SPARC中首次提出，并在x86架构中使用，RISC-V也提供了TSO扩展，即RVTSO。为什么需要TSO/X86长期以来，处理器内核使用write buffer来保存已提交的store指令，直到内存系统可以处理这些store请求。当store指令提交时，store请求进入write buffer，而当需要写入的...", "content": "TSO（Total Store Ordering）是一个被广泛使用的内存模型，在SPARC中首次提出，并在x86架构中使用，RISC-V也提供了TSO扩展，即RVTSO。为什么需要TSO/X86长期以来，处理器内核使用write buffer来保存已提交的store指令，直到内存系统可以处理这些store请求。当store指令提交时，store请求进入write buffer，而当需要写入的缓存行在内存系统中可以保证缓存一致性时，store请求就退出write buffer。对单核处理器来说，如果在write buffer中存在对地址A的store，那么碰到对地址A的load指令时，可以直接返回即将store的值，或者也可以选择停顿load指令。问题发生在有多个这样的处理器的时候，如下图所示。r1和r2可以同时为0吗？两个处理器核可以按照如下顺序执行代码： C1执行S1，但将store NEW的请求放入write buffer中。 C2执行S2，但将store NEW的请求放入write buffer中。 接下来，两个核心分别执行L1和L2，load的值都为0。 最后，两个核心的write buffer将NEW值写入内存。 执行完毕后，r1和r2同时为0，违反了SC。可能的解决方法有直接禁用write buffer，但会影响性能，或者使用更激进的、推测性的SC实现，但增加了复杂性，且可能浪费检查内存一致性违规与处理错误情况的功耗。SPARC和x86的选择时放弃SC，支持一种全新的内存模型，以更好地支持基于FIFO的write buffer。TSO/X86的基本概念SC要求在以下所有四种连续执行指令的组合中保留load和store的程序顺序： Load –&gt; Load Load –&gt; Store Store –&gt; Store Store –&gt; Load TSO包含前三个约束，但不包括第四个。需要注意的是，上面的第三条约束要求write buffer必须是FIFO，以保证store-store的顺序。这种改动对大多数程序来说影响不大，如下图中在上一章中出现过的例子，在TSO模型中仍然可以正确执行。r2是否总是会被更新为NEW？TSO模型下一种可能的执行情况下面是另一个比较有意思的例子，注意每个处理器核仍必须按照程序顺序观察到自己所发出的store请求的效果，因此r1和r3都会被更新为NEW值。r1或r3是否可能为0？上面的例子在TSO模型下的执行情况形式语言的定义 所有的核按照程序顺序来把load和store指令插入&lt;m顺序，且顺序与内存地址无关，可能有以下四种情况： 若L(a) &lt;p L(b)，则L(a) &lt;m L(b) 若L(a) &lt;p S(b)，则L(a) &lt;m S(b) 若S(a) &lt;p S(b)，则S(a) &lt;m S(b) 使用FIFO write buffer 每一次load都会从之前最后一次相同地址的store中获取值，或从write buffer中对相同地址的store请求通过旁路获取值，即L(a)的值为MAX_&lt;m {S(a) | S(a) &lt;m L(a) or S(a) &lt;p L(a)}，其中MAX_&lt;m/&lt;p表示内存/程序顺序中最后出现的那一项，且程序顺序优先（即write buffer绕过了内存系统的其余部分）。 定义FENCE： 若L(a) &lt;p FENCE，则L(a) &lt;m FENCE 若S(a) &lt;p FENCE，则S(a) &lt;m FENCE 若FENCE &lt;p FENCE，则FENCE &lt;m FENCE 若FENCE &lt;p L(a)，则FENCE &lt;m L(a) 若FENCE &lt;p S(a)，则FENCE &lt;m S(a) 不过由于TSO只放宽了对store-load的顺序约束，FENCE相关的定义也可以是： 若S(a) &lt;p FENCE，则S(a) &lt;m FENCE 若FENCE &lt;p L(a)，则FENCE &lt;m L(a) 我们选择第一种，也就是让TSO的FENCE作为所有内存请求的分界，这样设计比较简单，不影响正确性，且和之后更宽松的内存模型中的FENCE保持一致。下表总结了TSO的顺序规则，和SC的表有两个最重要的区别：Op1是store、OP2是load的情况对应了B，表示bypassing；额外包含了FENCE，而在SC中不需要FENCE。TSO顺序规则（X表示强制顺序，B则表示如果访问同一个地址，需要bypassing）实现TSO/X86有意思的是，人们普遍认为x86内存模型等同于TSO，然而Intel和AMD从来没有保证这一点，但是Sewell等人在x86-TSO: A rigorous and usable programmer’s model for x86 multiprocessors提出了x86-TSO模型，该模型有两种形式，如下图所示，作者证明两者是等价的。一方面，x86-TSO和x86的非官方的规则保持一致，且通过测试；另一方面，在Intel和AMD的平台上没有发现违反x86-TSO模型的情况。注意这仍然不能完全保证x86-TSO和x86的内存模型完全一致。两种TSO的实现对于SC来说，上图a中利用开关的实现可以被b中一个缓存一致的内存系统替代，在TSO中也同理，唯一的区别就是相比SC在每个处理器核上多了一个基于FIFO的write buffer。另外，在多线程中，TSO的write buffer在逻辑上对每个线程都是私有的，因此在一个多线程内核中，一个线程上下文不应该从另一个线程的write buffer中通过旁路获取数据，这种逻辑上的分离可以通过每个线程实现单独的write buffer实现，或者更常见的是通过共享write buffer，但每个条目都由线程id进行标记，只有id匹配时才可以bypass。 测验问题4：在一个有多线程核心的TSO系统中，线程可以绕过write buffer的值，而不用管是哪个线程写入的值。 答：错误。一个线程可以bypass自己写入的数据，但是直到store请求被添加到内存顺序时才会被其他线程观测到。实现原子指令TSO中的原子RMW指令和SC的很相似，最关键的区别就是TSO允许load的bypass，且store可能被放入write buffer中。为了理解原子RMW的实现，我们认为RMW是load之后马上接着一个store。按照TSO的顺序规则，load不能bypass更早的load。我们可能会认为load可以bypass更早的store，但其实不行，因为RMW指令中load和store需要原子完成，那么store也需要bypass更早的store，但是这在TSO中是不允许的，因此对原子RMW来说load也不能bypass更早的store。因此，从实现角度来看，原子RMW需要在之前的store退出write buffer之后才可以执行。同时，为了确保load之后可以马上store，load时需要将缓存一致性状态更新为读写状态（如M状态），而不是一般的只读状态（如S状态）。最后，为了保证原子性，在load和store之间不能更新缓存一致性状态。我们也可以实现更激进的优化，比如说，如果实现了（a）write buffer中的每个请求都在缓存中有读写权限（如处于M状态），并且在RMW提交之前保持读写权限，或者（b）像MIPS R10000一样进行推测性load检查，就不需要在执行RMW前先清空write buffer（此处以及下文中的清空都是指完成write buffer中的所有请求，而非直接清空）。实现FENCE我们可以通过添加FENCE指令来保证store-load的顺序。由于TSO只允许store-load的重排，因此需要使用FENCE的场合不多，FENCE的实现方式也不是很重要。一种简单的实现是，当FENCE被执行时，清空write buffer，并在FENCE提交之前不再执行后面的load指令。比较SC与TSO我们可以发现，从执行和实现的角度来说，SC都是TSO的一个子集。更一般来说，如果所有符合X模型的执行顺序也符合Y模型的执行顺序，那么Y模型比X模型更宽松（更弱），或者X模型比Y模型更严格。不过也有可能两个内存模型时不可比的，X模型允许Y模型中不允许的顺序，同时Y模型也允许一些X模型中不允许的顺序。好的内存一致性模型是什么样的？ 可编程性：一个好的模型应该使编写多线程程序相对比较容易，对大多数用户都是直观的。 性能：一个好的模型应该有助于在功耗、成本等约束条件下实现高性能，并提供广泛的设计选择。 可移植性：一个好的模型应该被广泛使用，或至少提供向后兼容的能力，或提供模型之间转换的能力。 精确性：一个好的模型应该是被精确定义的，通常用数学来进行定义。 SC和TSO怎么样？ 可编程性：SC是最直观的，TSO相对来说也比较直观。 性能：对简单的处理器内核来说，TSO可以比SC有更好的性能，但也可以进行各种优化以缩小两者之间的差距。 可移植性：SC被广泛理解，TSO被广泛采用。 精确：我们已经给出了SC和TSO的精确定义。 " }, { "title": "CONNECT Note (4) - AXI4 Interface on Credit-based Flow Control Network", "url": "/posts/CONNECT4/", "categories": "CALCM, CONNECT", "tags": "", "date": "2022-06-05 00:03:00 +0000", "snippet": "Source code: link.Flit FIFOIn credit-based flow control network, a FIFO is needed to store multiple requests and responses and a credit counter is used to reflect the availability of buffer in a cl...", "content": "Source code: link.Flit FIFOIn credit-based flow control network, a FIFO is needed to store multiple requests and responses and a credit counter is used to reflect the availability of buffer in a client. In FlitFIFO.sv, we design a simple FIFO with valid-ready interface, based on chisel3 queue data structure (source).module BasicFIFO #(parameter DEPTH = `FLIT_BUFFER_DEPTH, parameter DATA_WIDTH = `FLIT_WIDTH) ( input CLK, input RST_N, // Input input [DATA_WIDTH - 1 : 0] enq_data, input enq_valid, output enq_ready, // Output output [DATA_WIDTH - 1 : 0] deq_data, output deq_valid, input deq_ready ); logic [DATA_WIDTH - 1 : 0] buffer [0 : DEPTH]; logic [$clog2(DEPTH) - 1 : 0] enq_ptr; logic [$clog2(DEPTH) - 1 : 0] deq_ptr; logic ptr_match, empty, full, maybe_full; logic enq_fire, deq_fire, do_enq, do_deq; assign ptr_match = (enq_ptr == deq_ptr); assign empty = ptr_match &amp;&amp; !maybe_full; assign full = ptr_match &amp;&amp; maybe_full; always_ff @(posedge CLK) begin if (!RST_N) begin enq_ptr &lt;= 0; deq_ptr &lt;= 0; maybe_full &lt;= 0; end else begin if (do_enq) enq_ptr &lt;= enq_ptr + 1; if (do_deq) deq_ptr &lt;= deq_ptr + 1; if (do_enq != do_deq) maybe_full &lt;= do_enq; end end always_ff @(posedge CLK) begin if (!RST_N) begin for (int i = 0; i &lt; DEPTH; i++) begin buffer[i] &lt;= 0; end end else begin if (do_enq) buffer[enq_ptr] &lt;= enq_data; end end assign enq_fire = enq_valid &amp;&amp; enq_ready; assign deq_fire = deq_valid &amp;&amp; deq_ready; logic [DATA_WIDTH - 1 : 0] deq_data_; logic deq_valid_; assign deq_data = deq_data_; assign deq_valid = deq_valid_; always_comb begin do_enq = enq_fire; do_deq = deq_fire; deq_data_ = buffer[deq_ptr]; deq_valid_ = !empty; // Flow if (empty) begin if (deq_ready) begin do_enq = 0; end do_deq = 0; deq_data_ = enq_data; end if (enq_valid) begin deq_valid_ = 1; end end logic enq_ready_; assign enq_ready = enq_ready_; always_comb begin enq_ready_ = !full; // Pipe if (deq_ready) begin enq_ready_ = 1; end endendmoduleA wrapper is also created to connect the network with the AXI interface and include the FIFO as follows.+--------+ +------------+ +-------------+ +---------+| Device | &lt;--AXI4--&gt; | AXI4Bridge | &lt;--flits--&gt; | InPortFIFO | &lt;--flits--&gt;| |+--------+ +------------+ +-------------+ | | ^ | Network | | +-------------+ | | +-----------flits--&gt; | OutPortFIFO | &lt;--flits--&gt;| | +-------------+ +---------+TestSee testbench_sample_axi4.sv for test bench.Results# ---- Performing Reset ----# 9: Port 0 send flit 1c2000000006d000000000002# 9: [m0-aw] addr=00000002# 10: Port 0 send flit 1c60000ffdeadbeefdeadbeef# 10: Port 1 send flit 1e4000200006d000000000003# 10: Port 0 get a credit# 10: [m0- w] data=deadbeefdeadbeef# 10: [m1-ar] addr=00000003# 11: Port 0 send flit 1c60000ffdeadbeefdeadbef0# 11: Port 0 get a credit# 11: [m0- w] data=deadbeefdeadbef0# 12: Port 0 send flit 1c60000ffdeadbeefdeadbef1# 12: Port 2 recv flit 1c2000000006d000000000002# 12: Port 0 get a credit# 12: Port 1 get a credit# 12: Port 2 put a credit# 12: [m0- w] data=deadbeefdeadbef1# 13: Port 0 send flit 1c60000ffdeadbeefdeadbef2# 13: Port 0 get a credit# 13: [m0- w] data=deadbeefdeadbef2# 14: Port 0 send flit 1c60001ffdeadbeefdeadbef3# 14: Port 2 recv flit 1c60000ffdeadbeefdeadbeef# 14: Port 3 recv flit 1e4000200006d000000000003# 14: Port 0 get a credit# 14: Port 3 put a credit# 14: [m0- w] data=deadbeefdeadbef3# 14: [s0-aw] addr=00000002# 15: Port 2 recv flit 1c60000ffdeadbeefdeadbef0# 15: Port 0 get a credit# 15: Port 2 put a credit# 16: Port 2 recv flit 1c60000ffdeadbeefdeadbef1# 16: [s0- w] data=deadbeefdeadbeef# 16: [s1-ar] addr=00000003# 17: Port 3 send flit 1ac000000deadbeefdeadbeef# 17: Port 2 recv flit 1c60000ffdeadbeefdeadbef2# 17: Port 2 put a credit# 17: [s1- r] data=deadbeefdeadbeef# 18: Port 3 send flit 1ac000000deadbeefdeadbef0# 18: Port 2 recv flit 1c60001ffdeadbeefdeadbef3# 18: Port 3 get a credit# 18: [s0- w] data=deadbeefdeadbef0# 18: [s1- r] data=deadbeefdeadbef0# 19: Port 3 send flit 1ac000000deadbeefdeadbef1# 19: Port 3 get a credit# 19: Port 2 put a credit# 19: [s1- r] data=deadbeefdeadbef1# 20: Port 3 send flit 1ac000100deadbeefdeadbef2# 20: Port 1 recv flit 1ac000000deadbeefdeadbeef# 20: Port 3 get a credit# 20: Port 1 put a credit# 20: [s0- w] data=deadbeefdeadbef1# 20: [s1- r] data=deadbeefdeadbef2# 21: Port 1 recv flit 1ac000000deadbeefdeadbef0# 21: Port 3 get a credit# 21: Port 2 put a credit# 21: [m1- r] data=deadbeefdeadbeef# 22: Port 1 recv flit 1ac000000deadbeefdeadbef1# 22: Port 1 put a credit# 22: [s0- w] data=deadbeefdeadbef2# 23: Port 1 recv flit 1ac000100deadbeefdeadbef2# 23: Port 2 put a credit# 23: [m1- r] data=deadbeefdeadbef0# 24: Port 1 put a credit# 24: [s0- w] data=deadbeefdeadbef3# 25: Port 2 send flit 18a0000000000000000000000# 25: [m1- r] data=deadbeefdeadbef1# 25: [s0- b]# 26: Port 2 get a credit# 26: Port 1 put a credit# 27: [m1- r] data=deadbeefdeadbef2# 28: Port 0 recv flit 18a0000000000000000000000# 28: Port 0 put a credit# 29: [m0- b]# actual:deadbeefdeadbeef expected:deadbeefdeadbeef# # actual:deadbeefdeadbef0 expected:deadbeefdeadbef0# # actual:deadbeefdeadbef1 expected:deadbeefdeadbef1# # actual:deadbeefdeadbef2 expected:deadbeefdeadbef2# # Pass# actual:deadbeefdeadbeef expected:deadbeefdeadbeef# # actual:deadbeefdeadbef0 expected:deadbeefdeadbef0# # actual:deadbeefdeadbef1 expected:deadbeefdeadbef1# # actual:deadbeefdeadbef2 expected:deadbeefdeadbef2# # Pass" }, { "title": "CONNECT Note (3) - AXI4 Interface", "url": "/posts/CONNECT3/", "categories": "CALCM, CONNECT", "tags": "", "date": "2022-06-02 03:07:00 +0000", "snippet": "Source code: link.AXI Requests &amp; Responses In FlitsSignals in 5 channels are encapsulated in flit data, with following parameters.parameter ADDR_WIDTH = 32;parameter DATA_WIDTH = 64;paramet...", "content": "Source code: link.AXI Requests &amp; Responses In FlitsSignals in 5 channels are encapsulated in flit data, with following parameters.parameter ADDR_WIDTH = 32;parameter DATA_WIDTH = 64;parameter STRB_WIDTH = DATA_WIDTH / 8;parameter ID_WIDTH = 8;parameter USER_WIDTH = 8;LSBs of AxID indicates the sender. LSBs of AxADDR indicates the target receiver.connect_parameters.v is modified according to the given configurations.`define NUM_USER_SEND_PORTS 4`define NUM_USER_RECV_PORTS 4`define NUM_VCS 2`define FLIT_DATA_WIDTH 92`define FLIT_WIDTH (`FLIT_DATA_WIDTH + 5)AW/AR Channelchannel - [91 : 89] # aw = 3'b001, ar = 3'b010axuser - [88 : 81]axid - [80 : 73]------ padding ------axlen - [60 : 53]axsize - [52 : 50]axburst - [49 : 48]axlock - [47 : 47]axcache - [46 : 43]axprot - [42 : 40]axqos - [39 : 36]axregion - [35 : 32]axaddr - [31 : 0]Updated on 06/10, we need to unify AXI4-Stream and AXI4 id/user field.channel - [91 : 89] # r = 3'b110------ padding ------axlen - [76 : 69]axsize - [68 : 66]axburst - [65 : 64]axlock - [63 : 63]axcache - [62 : 59]axprot - [58 : 56]axqos - [55 : 52]axregion - [51 : 48]axaddr - [47 : 16]axuser - [15 : 8]axid - [ 7 : 0]W Channelchannel - [91 : 89] # w = 3'b011wuser - [88 : 81]wid - [80 : 73]wlast - [72 : 72]wstrb - [71 : 64]wdata - [63 : 0]Updated on 06/10, we need to unify AXI4-Stream and AXI4 id/user field.channel - [91 : 89] # r = 3'b110wlast - [88 : 88]wstrb - [87 : 80]wdata - [79 : 16]wuser - [15 : 8]wid - [ 7 : 0]B Channelchannel - [91 : 89] # b = 3'b101buser - [88 : 81]bid - [80 : 73]------ padding ------bresp - [65 : 64]------ padding ------Updated on 06/10, we need to unify AXI4-Stream and AXI4 id/user field.channel - [91 : 89] # r = 3'b110------ padding ------bresp - [81 : 80]------ padding ------buser - [15 : 8]bid - [ 7 : 0]R Channelchannel - [91 : 89] # r = 3'b110ruser - [88 : 81]rid - [80 : 73]rlast - [72 : 72]------ padding ------rresp - [65 : 64]rdata - [63 : 0]Updated on 06/10, we need to unify AXI4-Stream and AXI4 id/user field.channel - [91 : 89] # r = 3'b110rlast - [88 : 88]------ padding ------rresp - [81 : 80]rdata - [79 : 16]ruser - [15 : 8]rid - [ 7 : 0]Bridge DesignA bridge is used to convert a pair of send &amp; recv port to a standard AXI protocol interface, either as a master device or a slave device. For example, for a network with 4 endpoints, we can have 2 master devices and 2 slave devices.Peek flow control is used in current version.FSM of Master Bridgealways_comb begin case (state) IDLE: begin if (aw_fire) next_state = WDATA; else if (ar_fire) next_state = RDATA1; else next_state = state; end WDATA: begin if (w_fire &amp;&amp; axi.wlast) next_state = WRESP1; else next_state = state; end WRESP1: begin if (get_flit[`FLIT_WIDTH - 1]) // flit is valid next_state = WRESP2; else next_state = state; end WRESP2: begin if (b_fire) next_state = IDLE; else next_state = state; end RDATA1: begin if (get_flit[`FLIT_WIDTH - 1]) // flit is valid next_state = RDATA2; else next_state = state; end RDATA2: begin if (r_fire) begin if (axi.rlast) next_state = IDLE; else next_state = RDATA1; end else next_state = state; end default: next_state = state; endcaseendFSM of Slave Bridgealways_comb begin case (state) IDLE: begin if (get_flit[`FLIT_WIDTH - 1]) begin // flit is valid if (get_flit[`FLIT_DATA_WIDTH - 1 : `FLIT_DATA_WIDTH - 3] == CHANNEL_AW) next_state = WADDR; else if (get_flit[`FLIT_DATA_WIDTH - 1 : `FLIT_DATA_WIDTH - 3] == CHANNEL_AR) next_state = RADDR; else next_state = state; end else next_state = state; end WADDR: begin if (aw_fire) next_state = WDATA1; else next_state = state; end WDATA1: begin if (get_flit[`FLIT_WIDTH - 1]) // flit is valid next_state = WDATA2; else next_state = state; end WDATA2: begin if (w_fire) begin if (axi.wlast) next_state = WRESP; else next_state = WDATA1; end else next_state = state; end WRESP: begin if (b_fire) next_state = IDLE; else next_state = state; end RADDR: begin if (ar_fire) next_state = RDATA; else next_state = state; end RDATA: begin if (r_fire) begin if (axi.rlast) next_state = IDLE; else next_state = RDATA; end else next_state = state; end default: next_state = state; endcaseendTestWe create a master device and a slave device to send or receive data. See AXI4Device.sv for example device modules and testbench_sample_peek_axi4.sv for test bench.Results# ---- Performing Reset ----# 9: Port 0 send flit 1c2000000006d000000000002# 9: [m0-aw] addr=00000002# 10: Port 0 send flit 1c60000ffdeadbeefdeadbeef# 10: Port 1 send flit 1e4000200006d000000000003# 10: [m0- w] data=deadbeefdeadbeef# 10: [m1-ar] addr=00000003# 11: Port 0 send flit 1c60000ffdeadbeefdeadbef0# 11: [m0- w] data=deadbeefdeadbef0# 12: Port 0 send flit 1c60000ffdeadbeefdeadbef1# 12: Port 2 recv flit 1c2000000006d000000000002# 12: [m0- w] data=deadbeefdeadbef1# 13: Port 0 send flit 1c60000ffdeadbeefdeadbef2# 13: [m0- w] data=deadbeefdeadbef2# 14: Port 0 send flit 1c60001ffdeadbeefdeadbef3# 14: Port 3 recv flit 1e4000200006d000000000003# 14: [m0- w] data=deadbeefdeadbef3# 14: [s0-aw] addr=00000002# 15: Port 2 recv flit 1c60000ffdeadbeefdeadbeef# 16: [s0- w] data=deadbeefdeadbeef# 16: [s1-ar] addr=00000003# 17: Port 3 send flit 1ac000000deadbeefdeadbeef# 17: Port 2 recv flit 1c60000ffdeadbeefdeadbef0# 17: [s1- r] data=deadbeefdeadbeef# 18: Port 3 send flit 1ac000000deadbeefdeadbef0# 18: [s0- w] data=deadbeefdeadbef0# 18: [s1- r] data=deadbeefdeadbef0# 19: Port 3 send flit 1ac000000deadbeefdeadbef1# 19: Port 2 recv flit 1c60000ffdeadbeefdeadbef1# 19: [s1- r] data=deadbeefdeadbef1# 20: Port 3 send flit 1ac000100deadbeefdeadbef2# 20: Port 1 recv flit 1ac000000deadbeefdeadbeef# 20: [s0- w] data=deadbeefdeadbef1# 20: [s1- r] data=deadbeefdeadbef2# 21: Port 2 recv flit 1c60000ffdeadbeefdeadbef2# 21: [m1- r] data=deadbeefdeadbeef# 22: Port 1 recv flit 1ac000000deadbeefdeadbef0# 22: [s0- w] data=deadbeefdeadbef2# 23: Port 2 recv flit 1c60001ffdeadbeefdeadbef3# 23: [m1- r] data=deadbeefdeadbef0# 24: Port 1 recv flit 1ac000000deadbeefdeadbef1# 24: [s0- w] data=deadbeefdeadbef3# 25: Port 2 send flit 18a0000000000000000000000# 25: [m1- r] data=deadbeefdeadbef1# 25: [s0- b]# 26: Port 1 recv flit 1ac000100deadbeefdeadbef2# 27: [m1- r] data=deadbeefdeadbef2# 28: Port 0 recv flit 18a0000000000000000000000# 29: [m0- b]# actual:deadbeefdeadbeef expected:deadbeefdeadbeef# # actual:deadbeefdeadbef0 expected:deadbeefdeadbef0# # actual:deadbeefdeadbef1 expected:deadbeefdeadbef1# # actual:deadbeefdeadbef2 expected:deadbeefdeadbef2# # Pass# actual:deadbeefdeadbeef expected:deadbeefdeadbeef# # actual:deadbeefdeadbef0 expected:deadbeefdeadbef0# # actual:deadbeefdeadbef1 expected:deadbeefdeadbef1# # actual:deadbeefdeadbef2 expected:deadbeefdeadbef2# # Pass" }, { "title": "CONNECT Note (2) - Setup", "url": "/posts/CONNECT2/", "categories": "CALCM, CONNECT", "tags": "", "date": "2022-06-01 01:27:00 +0000", "snippet": "Run CONNECT testbenchThank Siddharth for providing the notes! Clone the repository from Crossroads GitHub To build a basic double ring network with 4 ports, 256 bit flits, 2 virtual cha...", "content": "Run CONNECT testbenchThank Siddharth for providing the notes! Clone the repository from Crossroads GitHub To build a basic double ring network with 4 ports, 256 bit flits, 2 virtual channels, and peek flow control (not credit-based, although we will want credit based later on), do python gen_network.py -t double_ring -w 256 -n 4 -v 2 -d 8 --router_type=vc --peek_flow_control --use_virtual_links --gen_rtl cd into build (it will make that directory for you when you do --gen_rtl). This contains a bunch of .v files that make up the generated network. Create a new file inside build called connect_parameters.v and fill it like so (NUM_USER_SEND_PORTS/NUM_USER_RECV_PORTS: how many ports did you give (the -n param); NUM_VCS: how many virtual channels (the -v param); FLIT_DATA_WIDTH: width of the network (the -w param) `define NUM_USER_SEND_PORTS 4`define NUM_USER_RECV_PORTS 4`define NUM_VCS 2`define FLIT_DATA_WIDTH 256 Inside build, make a file called run_modelsim.sh and paste the script from below inside it, then do chmod +x run_modelsim.sh (A Short Intro to ModelSim Verilog Simulator) vlib workvlog *.sv *.vvsim -c -do \"run -all\" CONNECT_testbench_sample_peek Inside build, ./run_modelsim.sh and see modelsim compile and run and eventually two flits injected into the network and two flits ejected from it Top Module &amp; InterfaceThe following module can be seen as the top module under testbench with peek flow control.module mkNetworkRealSimple(NetworkSimple); String name = \"Simple Network\"; // Vector of input and output interfaces to connect network clients Vector#(NumUserSendPorts, InPortSimple) send_ports_ifaces; Vector#(NumUserRecvPorts, OutPortSimple) recv_ports_ifaces; Vector#(NumUserRecvPorts, RecvPortInfo) recv_ports_info_ifaces; function get_rt( Integer i ); `ifdef USE_VOQ_ROUTER return mkVOQRouterSimple(i); `elsif USE_IQ_ROUTER return mkIQRouterSimple(i); `else return mkRouterSimple(i); `endif endfunction function get_port_info_ifc( Integer id ); let recv_port_info_ifc = interface RecvPortInfo method UserRecvPortID_t getRecvPortID; return fromInteger(id);\t endmethod endinterface; return recv_port_info_ifc; endfunction // Declare router and traffic source interfaces Vector#(NumRouters, RouterSimple) routers &lt;- genWithM( get_rt ); Vector#(NumLinks, ConnectPorts) links; interface send_ports = send_ports_ifaces; interface recv_ports = recv_ports_ifaces; interface recv_ports_info = recv_ports_info_ifaces; endmoduleThe interface of this module is in NetworkTypes.bsv.interface NetworkSimple; interface Vector#(NumUserSendPorts, InPortSimple) send_ports; interface Vector#(NumUserRecvPorts, OutPortSimple) recv_ports; interface Vector#(NumUserRecvPorts, RecvPortInfo) recv_ports_info; // Used by clients for obtaining response addressendinterfaceInternal definitions are in NetworkExternalTypes.bsv.////////////////////////////////////////////////// Simpler InPort and OutPort interfaces// - Routers only exchange notFull signals, instead of credits// Implemented by routers and traffic sources //////////////////////////////////////////////// interface InPortSimple; (* always_ready *) method Action putFlit(Maybe#(Flit_t) flit_in); (* always_ready *) method ActionValue#(Vector#(NumVCs, Bool)) getNonFullVCs;endinterfaceinterface OutPortSimple; (* always_ready *) method ActionValue#(Maybe#(Flit_t)) getFlit(); (* always_ready *) method Action putNonFullVCs(Vector#(NumVCs, Bool) nonFullVCs);endinterface// Used by clients to obtain response addressinterface RecvPortInfo; (* always_ready *) method UserRecvPortID_t getRecvPortID;endinterfaceDefinition of Flit_t.typedef struct { Bool is_tail; // only required for multi-flit packets UserRecvPortID_t dst; VC_t vc; FlitData_t data; // payload of flit} Flit_t deriving(Bits, Eq);Implement AXI InterfaceWe develop AXI4 standard protocol interface with SystemVerilog. Global constants are defined in AXI4Package.sv and encapsulated in axi4_pkg package. The interface is defined in AXI4Interface.sv, called axi_interface, where all the signals are defined, including those that are not used yet (e.g., axcache or axprot). The modules (AXI4MasterToInPortSimple and AXI4SlaveToOutPortSimple) to bridge AXI4 interface and CONNECT network interfaces are defined in AXI4Bridge.sv, and the logics are handled by a simple FSM. A wrapper (NetworkIdealSimpleAXI4Wrapper) is defined outside mkNetworkSimple module and can be used by a test bench (testbench_sample_peek_axi4.sv). Results# @ 8: Injecting flit (data = 0000000000001234) into send port 0# @ 11: Injecting flit (data = 0000000000002345) into send port 0# @ 12: Ejecting flit (data = 0000000000001234) at receive port 1# @ 16: Ejecting flit (data = 0000000000002345) at receive port 2Notes The width of flit data needs to be consistent with AXI data width. Meanwhile we simply use axaddr to find the receive port, but can be extended to a more complicated encoding. At present we only implement the write logic, i.e., writing some data to a send port and to be received at a receive port. The AXI request and response themselves can/should be transferred through the network? Next step can be connect a CPU core to one endpoint of the network, and memory to another endpoint? If it works, connect multiple cores? Reference BSV by example, a good intro book to Bluespec. Piccolo, a 3-stage in-order CPU core developed with Bluespec. " }, { "title": "《内存一致性与缓存一致性》笔记（二）：顺序一致性", "url": "/posts/MCCC2/", "categories": "读书笔记, 内存一致性与缓存一致性", "tags": "", "date": "2022-05-29 19:23:00 +0000", "snippet": "共享内存行为的问题访问共享内存时的问题上面的例子很直观地说明了共享内存访问行为的问题，C1有可能先执行S2然后再执行S1。从硬件的视角来看，处理器有以下三种方式对内存访问进行重新排序： Store-store重排：也就是上面的例子中出现的情况。 Load-load重排：这种重排似乎是安全的，但其实在上面的例子中，重排C2的load顺序和重排C1的store顺序其实导致...", "content": "共享内存行为的问题访问共享内存时的问题上面的例子很直观地说明了共享内存访问行为的问题，C1有可能先执行S2然后再执行S1。从硬件的视角来看，处理器有以下三种方式对内存访问进行重新排序： Store-store重排：也就是上面的例子中出现的情况。 Load-load重排：这种重排似乎是安全的，但其实在上面的例子中，重排C2的load顺序和重排C1的store顺序其实导致的结果是一样的。 Load-store与store-load重排：这种重排可能会导致很多出乎意料的行为，如下图所示。 r1和r2可以同时是0吗？顺序一致性的基本概念顺序一致性（sequential consistency，SC）是最直观的内存一致性模型，具体来说就是顺序一致性表示执行的结果与操作按照程序规定的顺序执行是一样的，也就是说从单个处理器的视角来看，内存访问的顺序和程序顺序是一致的。Table 3.1中的例子在SC模型下的执行顺序我们用&lt;m表示内存顺序，&lt;p表示程序顺序，如op1 &lt;m op2表示内存顺序中op1出现在op2之前，op1 &lt;p op2表示=程序顺序中op1出现在op2之前。在SC中，op1 &lt;p op2可以推出op1 &lt;m op2。下图进一步说明了上面例子的执行情况。上述例子中4种不同的执行情况形式语言的定义我们尝试用形式语言来更精确地定义SC。L(a)和S(a)分别代表对地址a的load和store，&lt;p和&lt;m分别表示程序顺序和内存顺序，且程序顺序是对于单个处理器核的，而内存顺序是全局的。一次SC的执行需要满足以下条件： 所有的核按照程序顺序来把load和store指令插入&lt;m顺序，且顺序与内存地址无关，可能有以下四种情况： 若L(a) &lt;p L(b)，则L(a) &lt;m L(b) 若L(a) &lt;p S(b)，则L(a) &lt;m S(b) 若S(a) &lt;p S(b)，则S(a) &lt;m S(b) 若S(a) &lt;p L(b)，则S(a) &lt;m L(b) 每一次load都会从之前最后一次相同地址的store中获取值，即L(a)的值为MAX_&lt;m {S(a) | S(a) &lt;m L(a)}，其中MAX_&lt;m表示内存顺序中最后出现的那一项。 原子RMW（read-modify-write）指令会在之后深入讨论，这种指令进一步限制了可能的执行情况。我们在下图中总结了SC的排序要求，这张表规定了哪些程序顺序是由一致性模型强制执行的。SC顺序规则一个SC的实现只允许SC的执行，这是SC实现的“安全”属性，同时SC的实现还应该有一定的“有效性”，例如应该保证让程序可以往下执行，而不会卡在一个地方。简单的SC实现SC有两种简单的实现方式，可以让我们更好的理解这种内存模型。 多任务单核处理器：在单个顺序核上执行所有线程来实现多线程的切换。线程T1执行一段时间后，进行上下文切换，开始执行T2，上下文切换时必须保证完成所有之前的内存访问指令，因此保证了所有SC的规则。 Switch：如下图所示，每个核按程序顺序向switch提交内存操作，switch负责接收内存请求并访问内存。 使用switch的简单SC实现这两种实现证明了SC的可行性，但其实这样实现的性能并没有随着核数的增加而提升，性能瓶颈导致一些人错误地得出结论：SC并没有达到真正的并行执行，但其实不然，我们接下来就会看到这一点。有缓存一致性的基本SC实现缓存一致性有利于SC的实现，可以完全并行地执行不冲突的加载和存储。冲突的定义是，两个操作访问同一个地址，并且其中至少有一个是store。我们实现下面的简单缓存一致性： 使用状态modified (M)来表示一个核可以写入和读取的L1块。 使用状态shared (S)来表示一个或多个核只能读取的L1块。 GetM和GetS分别表示获取M和S中块的一致性请求。 我们之前一直把缓存一致性当作黑盒，现在我们稍微打开内存系统的黑盒，每个核都连接到自己的L1缓存，根据内存访问指令来更新自己的缓存状态，如下图所示。用缓存一致性实现SC实现缓存一致性情况下的SC并发执行有缓存一致性的优化SC实现大多数实际的内核实现都比我们的基本SC实现要复杂得多，且同时实现了缓存一致性。通常会采用预取、推测执行和多线程等技术以提高性能，优化内存访问延迟，我们现在讨论这些技术如何影响SC的实现。需要注意的是，只要不违反SC的最终结果，我们就可以进行任何形式的优化。非绑定预取对区块B的非绑定预取是对一致性内存系统的请求，以改变B在一个或多个cache中的一致性状态，这个请求通常可以是由软件、处理器核或者cache发出的，通过GetS或GetM等请求以允许M或S状态下的load以及M状态下的load或store。需要注意的是，非绑定预取仅限在缓存中进行，也就是说内存一致性仍然是可以保证的。推测执行load和store指令可能位于错误的分支预测路径上，这些load和store可以仿照非绑定预取的方式来执行，因而不会对SC的正确性产生影响。对于load指令来说，如果不成功，只会导致非绑定的GetS预取，而不会影响寄存器的状态，如果成功，就返回load的值到寄存器。对于store指令也是类似的，可能提前发出一个非绑定的GetM预取，但是在指令提交之前，不会更新缓存中的内容。 测验问题1：在一个保持顺序一致性的系统中，一个核心必须按程序顺序发出一致性请求。 答：错误，因为一个处理器核可能按照任意顺序发出缓存一致性请求。动态调度处理器核可以动态调度指令以实现乱序执行，实现更高的性能。在多核处理器的背景下，这可能导致内存一致性的问题。考虑两条load指令L1和L2，处理器核可能会在L1之前推测性地执行L2，尽管这种重排对其他内核是不可见的，但仍然违反了SC。有两种技术来检查这种情况。第一种方法是在推测性地执行L2、但是提交L2之前，处理器检查推测访问的缓存块是否已经离开了缓存。只要缓存块仍然在缓存中，值就不会变化。为了执行这个检查，处理器跟踪L2 load的地址，并将evict的缓存块和传入的一致性请求进行对比，一个传入的GetM表明另一个处理器可以观察到L2的miss，而这个GetM表明推测性的load执行错误，需要冲刷推测路径。第二种方法是在处理器准备提交load指令时replay每个推测性的load。如果提交时load的值不等于之前推测load的值，那么预测就不正确。动态调度内核中的非绑定预取一个动态调度的内核可能会遇到不按程序顺序load和store的情况，如程序顺序是load A，store B，store C，但可能不按照这个顺序进行非绑定预取，如GetM C，然后并行执行GetS A和GetM B，但这不会影响SC，SC只要求一个处理器的load和store从外界看上去按照程序顺序访问其L1缓存。简而言之，SC（或其他任何内存一致性模型）决定了load和store真正提交的顺序，但不决定缓存一致性活动的顺序。 测验问题2：内存一致性模型规定了缓存一致性请求的合法顺序。 答：错误。SC内存模型下的原子操作在微架构中实现原子指令本身不难，但是过于简化的设计可能会导致性能不佳，例如实现原子指令的一个正确但简单的方法是让处理器锁定内存系统（即防止其他内核发出内存访问）并对内存进行读取、修改和写入操作。这种实现虽然正确而直观，但却牺牲了性能。更激进的实现是利用了SC可以保证所有请求按顺序发出。因此，一个原子RMW操作可以通过让一个核心在缓存中获得M状态的缓存块来实现，然后在缓存中进行load和store，不需要任何一致性消息或者总线锁定，只需要在此过程中阻塞其他处理器传来的一致性请求，并在store真正完成之后再响应其他处理器即可，这个过程不会有死锁的风险。 测验问题3：为了执行一个原子的read-modify-write（如test-and-set），一个核必须始终与其他核进行通信。 答：错误。一个更加优化的实现可以在不违反原子性的情况下，允许在load和store之间有更多的时间。考虑这样的情况，一个缓存块当前处于S状态，RMW的load可以立即进行推测执行，与此同时缓存控制器发出一致性请求，将块的状态更新为M状态，更新完成后就可以执行store。这个过程不一定能保证原子性，因此为了检查原子性，处理器核必须检查load的缓存块是否在load和store之间从缓存中被evict，类似于前一节中检测错误的推测性load指令的过程。案例研究：MIPS R10000在执行过程中，处理器核按程序顺序向地址队列发出（推测性）load和store。load从上一个相同地址的store指令中获得推测性的值，如果没有，则从数据缓存中获得值。load和store按程序顺序提交，然后删除其地址队列条目。提交store时，缓存块必须在M状态下，并且缓存块与ROB必须被原子地更新。重要的是，如果发生了缓存块eviction（可能是由于一致性请求或者仅仅是为其他的数据腾出空间），且evict的缓存块地址对应了地址队列中的某个load请求，那么这个load请求和后面所有的指令都要被冲刷，并重新执行。因此，当load最终提交时，load的缓存块在执行和提交之间一直在缓存中，因此这个期间load的值不会被改变。由于store在提交时实际写入缓存，R10000在逻辑上将load和store按程序顺序提交给内存系统，从而实现了SC。" }, { "title": "《内存一致性与缓存一致性》笔记（一）：基础", "url": "/posts/MCCC1/", "categories": "读书笔记, 内存一致性与缓存一致性", "tags": "", "date": "2022-05-28 16:00:00 +0000", "snippet": "《内存一致性与缓存一致性（第二版）》一书（英文名为A Primer on Memory Consistency and Cache Coherence）隶属于Synthesis Lectures on Computer Architecture系列，面向于学习过基础的计算机体系结构但是想进一步深入了解内存一致性与缓存一致性的读者。内存一致性与缓存一致性（CMU 18-447 Spring 2...", "content": "《内存一致性与缓存一致性（第二版）》一书（英文名为A Primer on Memory Consistency and Cache Coherence）隶属于Synthesis Lectures on Computer Architecture系列，面向于学习过基础的计算机体系结构但是想进一步深入了解内存一致性与缓存一致性的读者。内存一致性与缓存一致性（CMU 18-447 Spring 2022 L24 Cache Coherence）内存一致性（又称内存一致性模型或内存模型）首先来看一个直观的例子。一所大学在网上发布其课程表，假设计算机体系结构课程最初被安排在152教室。在开课的前一天，教务处决定将该课移到252教室，并发送了一封电子邮件，要求网站管理员更新在线课程表，几分钟后，教务处向所有这门课的学生发送了一条短信，让他们查看新更新的课程表。不难想象，如果网站管理员太忙，无法立即发布更新信息，那么勤奋的学生收到短信后，立即查看在线课程表，但仍然观察到152教室的（旧）课程表 。尽管在线课程表最终被更新为252教室，而且教务处确实以正确的顺序进行了“写入”操作，但勤奋的学生还是以不同的顺序观察到了课程表，因此去错了上课的教室。内存一致性模型以load和store的方式定义了正确的共享内存行为。访问内存不确定的行为可能发生在各种共享内存的硬件中，如乱序处理器内核、write buffer、预取、多级缓存中，因此我们有必要确定内存访问行为的正确性。在本书的后面几章会依次介绍顺序一致性、TSO模型、以及“松弛”或“弱”内存一致性模型。缓存一致性和之前一样，我们来看一个直观的例子。一个学生检查了在线课程表，发现计算机结构课程在152教室（读取数据），并将此信息复制到手机中（缓存数据）。随后，教务处决定将课程移到252室，更新了在线课程表（写入数据），并通过短信通知学生。注意此时学生手机里的信息现在已经过时了，发生了缓存不一致的情况。如果这时学生按照手机里的信息前往152教室，就会发现走错教室了。对过时数据的访问是通过缓存一致性协议来避免的，缓存一致性协议在同步发生的时间和方式上存在差异，有两类主要的一致性协议。第一类方法中，一致性协议确保写入的内容同步传播到各个缓存中；第二类方法中，一致性协议将写入的内容异步传播到缓存中，且仍然保证一致性。小测验我们时常觉得自己已经非常了解内存一致性和缓存一致性了，以下的一些判断题可以帮助读者判断是否充分了解了相关的概念。 在一个保持顺序一致性的系统中，一个核心必须按程序顺序发出一致性请求。 内存一致性模型规定了缓存一致性请求的合法顺序。 为了执行一个原子的read-modify-write（如test-and-set），一个核必须始终与其他核进行通信。 在一个有多线程核心的TSO系统中，线程可以绕过write buffer的值，而不用管是哪个线程写入的值。 相对于高级语言的一致性模型（如Java），程序员编写适当的同步代码，不需要考虑架构的内存一致性模型。 在MSI总线嗅探协议中，一个缓存行只能处于三种一致性状态中的一种。 总线嗅探缓存一致性协议要求内核必须在总线上通信。 GPU不支持硬件缓存一致性，因此无法实现内存一致性模型。 基本系统模型我们考虑的是具有多个处理器内核的系统，所有处理器都可以对共享内存进行load和store，且每个内核都有自己的私有数据缓存，可能还有一个共享的最后一级缓存（LLC）。不过我们介绍缓存时，都指私有数据缓存而非LLC。基本系统模型缓存不一致的情况缓存不一致的例子上图说明了一种缓存不一致的情况，C1和C2都缓存了A的值，假设A的初始值为42，C1往缓存中写入A=43，但并没有同步到C2，因此C2陷入了无限循环。缓存一致性接口一致性接口处理器通过一致性接口（如上图）与一致性协议进行交互，包括： read-request：接收一个内存地址并返回一个值。 write-request：接收一个内存地址和一个将要写入的值，并返回一个确认信号。 可以按照缓存一致性接口的性质将缓存一致性协议分为两类： 内存一致性无关的缓存一致性：一次写入在返回之前就对其他所有内核可见，由于内存写入是同步传播的，这类协议提供了一个与原子内存系统相同的接口。这种协议可以很好地将缓存完全抽象化，处理器流水线只需执行内存一致性模型规定的顺序即可。 内存一致性相关的缓存一致性：内存写入是异步传播的，因此写入可以在被所有处理器看到之前返回，允许处理器观察非最新的值，但仍然需要遵守内存一致性规定的顺序。 （内存一致性无关的）缓存一致性不变性我们通过单写多读（single-writer-multiple-reader, SWMR）不变性来定义一致性，如下图所示，同一时刻要么有一个核心在写，要么有一个或多个核心在读，两者不会同时发生。单读多写的例子与此同时，我们需要保证一个内存位置在一个时间片开始时的值与该内存位置在其上一个时间片结束时的值相同，称为data-value不变性。" }, { "title": "CONNECT Note (1) - Paper", "url": "/posts/CONNECT1/", "categories": "CALCM, CONNECT", "tags": "", "date": "2022-05-26 15:32:00 +0000", "snippet": "原文：CONNECT: Re-Examining Conventional Wisdom for Designing NoCs in the Context of FPGAs.CONNECT是一个NoC生成器，面向FPGA生成任意多节点拓扑结构的可综合RTL设计，这里有一个online demo。CONNECT的意义在于，传统的NoC设计都为ASIC优化，但是面向FPGA的NoC设计需要考虑...", "content": "原文：CONNECT: Re-Examining Conventional Wisdom for Designing NoCs in the Context of FPGAs.CONNECT是一个NoC生成器，面向FPGA生成任意多节点拓扑结构的可综合RTL设计，这里有一个online demo。CONNECT的意义在于，传统的NoC设计都为ASIC优化，但是面向FPGA的NoC设计需要考虑到以下几点： 相比逻辑和存储资源相比，FPGA的导线资源相对丰富。 大量中等大小的缓冲区形式的片上存储资源较少。 深度流水线的性能回报较少。 极其灵活的应用特定的现场可重构性。 CONNECT NoC架构体现了一套以FPGA为动力的设计原则，这些原则独特地影响了关键的NoC设计决策，如拓扑结构、链路宽度、路由器流水线深度、网络缓冲区大小和流量控制。适应FPGA的要求布线资源丰富FPGA的布线资源非擦和功能丰富，特别是相对于其他资源而言。传统NoC设计中，路由器通常被视为内部密集连接的组件，通过狭窄的通道相互连接，并复用大量信息。相反，FPGA的NoC应该是尽可能使通道尽可能宽，流量控制机制也可以使用更宽的接口，间接减少路由器的存储需求，CONNECT NoC架构还允许一个路由器的逻辑直接进入另一个路由器，以更有效地实现缓冲区的流量控制。存储资源不足FPGA提供BRAM和Distributed RAM两种存储形式，大部分可用的存储容量都以BRAM提供，且不能被进一步细分。对NoC来说，大量的缓冲区的容量通常逗比Distributed RAM大得多，但是又比BRAM容量小得多。因此，为FPGA调整的NoC应该对增加缓冲区大小的优化有较高的门槛，具体来说，CONNECT NoC架构完全避免使用BRAM，而是将Distributed RAM用于其数据包缓冲区，且缓冲区大小和配置考虑到了Distributed RAM的具体尺寸和大小，以更有效地利用LUT资源。低时钟频率时钟频率也是FPGA和ASIC最重要的差异之一，因而在FPGA上设计深度流水线以提高频率的回报很差。然而，FPGA在低频率下的性能损失可以通过增加数据通路和链路宽度来有效弥补，且CONNECT NoC架构中的浅流水线具有降低网络延迟的额外好处，并大大减少了路由器消耗的flip-flop数量。可重构性鉴于FPGA的灵活性，一个有效的NoC设计有可能被要求与各种不同的应用相匹配，因此也可以适配于一些特定应用。具体来说，CONNECT NoC架构并不是一个单一的IP，而是一个RTL设计生成器，可以生成专门适应一些应用的NoC实例，甚至是特定的逐次运行的工作负载。CONNECT NoC生成器完全是参数化的，网络遵循相同的简单标准通用接口。CONNECT NoC架构CONNECT NoC旨在成为更大的基于FPGA系统的一部分，因此CONNECT NoC需要在两个相互冲突的目标之间取得平衡：提供足够的网络性能和尽量减少FPGA上资源的使用。基于这样的设计理念，CONNECT和传统NoC的设计完全不同，具体有以下两点： 单级流水线：和大多数典型的3-5级流水线相比，CONNECT采用了单级流水线（就是没有流水线？），降低了硬件成本且减少了延迟，有机会实现更简单的流量控制和更高效的缓冲期使用。 紧密耦合的路由器：CONNECT在路由器中尝试使用更宽的接口来最大化布线资源的使用，导致耦合更紧密，包括在额外的线路上携带flit控制信息（而非在传统的设计中使用单独的header flit），这也允许路由器直接查看下游接受路由器的缓冲区占用信息。 CONNECT路由器架构路由器设计采用Bluespec System Verilog实现，保持灵活的可参数化设计，支持 Variable number of input and output ports Variable number of virtual channels (VCs) Variable flit width Variable flit buffer depth Two flow control mechanisms Flexible user-specified routing Four allocation algorithmsCONNECT路由器架构 数据包路由：路由是由查找表实现的，为网络中的每个可能目的地提供输出端口，这样的设计提供了灵活性，尽管对于大型网络可能需要大量条目，但CONNECT通过利用Distributed RAM以一种有效的方式来实现，占用的LUT数量通常少于10。 Flit缓冲区：Flit缓冲区是按输入组织的，通过在每个单读单写的Distributed RAM实现多个逻辑FIFO（每个VC一个FIFO）来实现缓冲区的设计。 缓冲区分配：CONNECT支持4中分离式输入-输出分配算法，分配模块由两个子模块组成，分别负责输入和输出的仲裁，产生符合条件的输入和可用输出的有效匹配。 设计特点 拓扑结构不可知：支持任意拓扑结构，且连接相同数量终端的CONNECT网络是可以互换的。 虚拟通道：支持多个VC，确保无死锁，也可以通过减少队列头部阻塞的影响来提高网络性能。 虚拟链接：当启用这个功能时，NoC保证多bit数据包的连续传输，保证了一旦一个数据包开始传送，就会在其他数据包传送之前完成，可能会轻微增加硬件成本，但是可以大大减少接收端的重组缓冲和逻辑设计要求。 Peek流量控制：可以很随意地翻译成“偷看”流量控制，传统的基于信用的流量控制通常被设计为可以容忍由于多周期链路延迟和路由器深流水线设计造成的长延迟，且支持在同一组线路上复用不同VC的流量控制信息以减少线路数量。在Peek流量控制中，路由器直接将缓冲区占用信息暴露给上游路由器，目前CONNECT使用单bit的peek流量控制，很类似于开关信号流量控制。 " }, { "title": "《片上网络》笔记（六）：路由器微架构", "url": "/posts/NOC6/", "categories": "读书笔记, 片上网络", "tags": "", "date": "2022-05-26 00:32:00 +0000", "snippet": "路由器的设计必须在严格的面积和功率限制下满足延迟和吞吐量的要求，这也是在多核系统扩展时面临的一个主要挑战。对吞吐量要求较低时，可以使用非常简单的路由器，其面积和功率开销较低，但是当NoC的延迟和吞吐量要求提高时，就会对路由器设计提出挑战。路由器的微架构决定了关键路径延迟，影响每一跳以及整个网络的延迟。路由的实现、流量控制和实际的路由器流水线会影响到使用缓冲区和链接的效率，从而影响到整个网络的...", "content": "路由器的设计必须在严格的面积和功率限制下满足延迟和吞吐量的要求，这也是在多核系统扩展时面临的一个主要挑战。对吞吐量要求较低时，可以使用非常简单的路由器，其面积和功率开销较低，但是当NoC的延迟和吞吐量要求提高时，就会对路由器设计提出挑战。路由器的微架构决定了关键路径延迟，影响每一跳以及整个网络的延迟。路由的实现、流量控制和实际的路由器流水线会影响到使用缓冲区和链接的效率，从而影响到整个网络的吞吐量。路由器的微架构也会影响网络功耗（包括动态和静态）和面积占用。虚拟通道路由器微架构基于信用的VC路由器微架构上图展示了基于信用的VC路由器微架构，以解释典型路由器的工作原理。在这个例子中，我们假设有一个2-D Mesh结构，每个路由器有5个输入和输出端口，分别对应4个相邻方向以及一个本地处理器端口，每个输入端口有4个VC，每个VC都有自己的缓冲队列，队列深度为4 flits。构成路由器的主要部件是输入缓冲、路由计算逻辑、虚拟通道分配器、开关分配器和crossbar开关。如果不使用源节点路由，路由计算器将计算（或查找）当前数据包的输出端口，分配器决定哪些flits被选择进入下一个阶段，并穿越crossbar，最后crossbar负责将flits从输入端口物理地移动到输出端口。缓冲区和虚拟通道当数据包或flits不能立即转发到输出时，会被存放在缓冲区中，可以在输入端口和输出端口上进行缓冲。当交换机的分配速率大于通道的传输速率时，就需要输出缓冲。缓冲区组织结构缓冲区与VC组织结构缓冲区组织结构会很大程度上影响网络吞吐量，具体的设计有：单个固定长度的队列、多个固定长度的队列（每个队列被称为虚拟通道，复用并共享物理通道或带宽）、多个可变长度的队列（多个VC共享一个大的缓冲区，但电路更复杂，且需要为每个VC保留一定大小以避免死锁）、最小数量的虚拟通道、最小缓冲区数量。输入VC状态每个VC都和其中flit的以下状态有关： Global (G): Idle/Routing/waiting for output VC/waiting for credits in output VC/Active. Active VCs can perform switch allocation. Route (R): Output port for the packet. This field is used for switch allocation. The output port is populated after route computation by the head flit. In designs with lookahead routing or source routing, the head flit arrives at the current router with the output port already designated. Output VC (O): Output VC (i.e., VC at downstream router) for this packet. This is populated after VC allocation by the head flit, and used by all subsequent flits in the packet. Credit Count (C): Number of credits (i.e., flit buffers at downstream router) in output VC O at output port R. This field is used by body and tail flits. Pointers (P): Pointers to head and tail flits. This is required if buffers are implemented as a shared pool of multiple variable-length queues, as described above. 开关设计路由器的crossbar开关是路由器数据通路的核心部分，将数据从输入端口移动到输出端口。Crossbar设计下面给出了一个基本的描述Crossbar开关的Verilog模块，如上图所示，大多数低频路由器使用这种crossbar。module xbar (clk, reset, in0, in1, in2, in3, in4, out0, out1, out2 out3, out4, colsel0, colsel1,colsel2, colsel3, colsel4); input clk; input reset; input ['CHANNELWIDTH:0] in0, in1, in2, in3, in4; output ['CHANNELWIDTH:0] out0, out1, out2, out3, out4; input [2:0] colsel0, colsel1, colsel2, colsel3 colsel4; reg [2:0] colsel0reg, colsel1reg, colsel2reg, colsel3reg, colsel4reg; bitxbar bx0(in0[0],in1[0],in2[0],in3[0],in4[0],out0[0],out1[0],out2[0] out3[0],out4[0],colsel0reg,colsel1reg,colsel2reg,colsel3reg,colsel4reg,1'bx); bitxbar bx1(in0[1],in1[1],in2[1],in3[1],in4[1],out0[1],out1[1],out2[1],out3[1],out4[1],colsel0reg,colsel1reg,colsel2reg,colsel3reg,colsel4reg,1'bx); bitxbar bx2(in0[2],in1[2],in2[2],in3[2],in4[2],out0[2],out1[2],out2[2],out3[2],out4[2],colsel0reg,colsel1reg,colsel2reg,colsel3reg,colsel4reg,1'bx); bitxbar bx3(in0[3],in1[3],in2[3],in3[3],in4[3],out0[3],out1[3],out2[3],out3[3],out4[3],colsel0reg,colsel1reg,colsel2reg,colsel3reg,colsel4reg,1'bx);endmodulemodule bitxbar (i0,i1,i2,i3,i4,o0,o1,o2,o3,o4,sel0,sel1,sel2,sel3,sel4,inv); input i0,i1,i2,i3,i4; output o0,o1,o2,o3,o4; input [2:0] sel0, sel1, sel2, sel3, sel4; input inv; buf b0(i00, i0); // buffer for driving in0 to the 5 muxes // ... buf b4(i40, i4); mux5_1 m0(i00, i10, i20, i30, i40, o0, sel0, inv); // ... mux5_1 m4(i00, i10, i20, i30, i40, o4, sel4, inv);endmodule当频率要求更高时，会采用下图所示的设计，但无论采用哪种设计，面积和功率都会以O((pw)^2)的复杂度增长，其中p是crossbar端口的数量，w是端口宽度（单位bit）。一个5x5的crossbar开关分配器与仲裁器一个分配器将N个请求与M个资源相匹配，而一个仲裁器将N个请求与1个资源相匹配。对路由器来说，资源是VC和crossbar开关的端口。在多个VC的路由器中，我们需要一个VC分配器（VA）以及一个交换机分配器（SA），前者将VC分配给数据包或flit，或者将crossbar端口分配给VC。一个能提供高匹配概率的分配器或仲裁器可以使更多包顺利获得VC并通过crossbar，提高网络吞吐量。在大多数NoC中，路由器的分配逻辑决定了周期时间，因此分配器和仲裁器必须是快速的，并且是流水线的，以便在高时钟频率下工作。循环（Round-robin）仲裁器循环仲裁器矩阵仲裁器矩阵仲裁器，其中wij表示优先位，当wij为1时，请求i的优先级比请求j更高分离式分配器为了降低分配器的复杂度，并使其流水化，分配器可以被构建为多个仲裁器的组合。在实践中尝试了不同的仲裁器，其中循环仲裁器由于其实现简单而最受欢迎。一个分离式的3:4分配器（3个请求者、4个资源）流水线路由器流水线 (BW: Buffer Write, RC: Route Computation, VA: Virtual Channel Allocation, SA: Switch Allocation, ST: Switch Traversal, LT: Link Traversal)上图a显示了一个基本的VC路由器的逻辑流水线阶段。head flit到达一个输入端口时，首先根据其输入的VC在BW级进行解码并放入缓冲区，接下来在RC级路由逻辑进行计算以确定包的输出端口，然后head flit在VA级进行仲裁，找到输出端口的VC（即下一个路由器输入端口的VC），成功分配VC后，head flit进入SA级，对switch的输入和输出端口进行仲裁，随后进入ST级，遍历crossbar，最后在LT级，该flit被传送到下一个节点。body和tail flit遵循类似的流水线，但不会经过RC和VA级，而是直接继承header flit分配的路由和VC，tail flit在离开路由器时，会移除head flit保留的VC。实现与优化最长关键路径延迟的流水线级决定了时钟频率，通常当VC较多或crossbar较复杂时，VA或SA级的延迟最长。增加流水线级数会增加消息在每一跳的延迟，以及影响所需的最小缓冲区大小和吞吐量的缓冲区周转时间，因此有必要对其进行优化。提前路由（lookahead routing）流水线移除了RC级，数据包的路由是由提前一跳确定的，并在head flit中编码，如上图b。低负载旁路（low-load bypass）流水线在轻度负载的路由器中移除了BW和SA级，如上图c。推测性VA移除了VA级，flit在BW之后推测性地进入SA级，对switch端口进行仲裁，并试图获得一个空闲的VC，如上图d。提前旁路（lookahead bypass）移除了BW和SA级，如上图e。具体的优化技巧相对而言比较复杂，可以参考原文。" }, { "title": "Pigasus笔记：Zhipeng Zhao的PhD Thesis", "url": "/posts/Pigasus1/", "categories": "CALCM, Pigasus", "tags": "", "date": "2022-05-25 21:07:00 +0000", "snippet": "原文：Pigasus: Efficient Handling of Input-Dependent Streaming on FPGAs.FPGA普遍应用于各种实时流媒体应用，如信号处理、机器学习推理、视频处理等，其特点是FPGA加速的部分是与输入无关的，因而加速的性能也和输入无关，具有固定的行为和性能。相比之下，网络入侵检测和预防（IDS/IPS）则是与输入相关的，性能取决于输入。之前有大...", "content": "原文：Pigasus: Efficient Handling of Input-Dependent Streaming on FPGAs.FPGA普遍应用于各种实时流媒体应用，如信号处理、机器学习推理、视频处理等，其特点是FPGA加速的部分是与输入无关的，因而加速的性能也和输入无关，具有固定的行为和性能。相比之下，网络入侵检测和预防（IDS/IPS）则是与输入相关的，性能取决于输入。之前有大量工作使用FPGA进行IDS/IPS，但大多数情况下都遵循传统的静态和固定性能的FPGA设计，效率低且成本高，这也就是Pigasus需要解决的问题。Pigasus主要提出的观点有：（1）对常见情况进行优化，并在系统的不同层次上处理不常见的情况，用最少资源获得最大性能（这就是计算机体系结构本科课程的第一课）；（2）在编译时和运行时针对不同输入的情况进行调整，适应不同的工作负载。背后的五个关键概念是： FPGA优先的架构：FPGA作为主要的处理单元，直接连接到100 Gbps网络端口，执行大部分数据处理，只有少数数据包（即不常见的情况）交给CPU处理。 基于快慢分离路径SRAM的TCP重组：快速路径处理有序的数据包（常见情况），慢速路径处理乱序的数据包（不常见情况），可以使用一个内存密集的数据结构来跟踪乱序数据包而不影响整体性能。 分层模式匹配：通过分层过滤器来分离不同速率下的需求，如前端的小型过滤器和输入速率一致，针对所有规则检查所有数据包（常见情况），而后端的复杂过滤器只针对部分匹配的规则检查可疑的数据包（不常见情况）。 分散的面向服务的流设计：通过分解系统，对单个流服务进行参数化，并将它们连接到一个共同的通信抽象系统上，随着部署环境的变化，用户可以在编译时轻松选择最有效的设计。 动态溢出机制：在运行时按需引入更多的计算能力，启动备用的内核（如CPU上实现的内核）以支持工作负荷的临时增加。 这五个关键概念将在接下来的章节中详细展开。Pigasus概述DatapathPigasus架构上图展示了Pigasus架构的主要组成部分，包括FPGA上的parser、reassembler、MSPM以及CPU上的full matcher，以下是工作流程。 数据包预处理：每个数据包首先经过一个100Gbps的以太网核心（Ethernet core），得到原始的以太网帧（Ethernet frames），并被暂时存储在数据包缓冲区（Packet Buffer），header被单独发送到parser（提取TCP/IP header中的metadata），然后将header转发到reassembler。 Reassembler：Reassembler按顺序对TCP数据包进行排序，以便对其进行连续扫描（即识别跨越多个数据包的匹配），不过UDP数据包不需要重组。 Data mover：Parser和reassembler只对header和metadata进行处理，而MSPM对完整的数据包进行处理，data mover从缓冲区获得原始数据包，转发到MSPM。 Multi-string pattern matcher：MSPM负责（1）根据10k条规则的header匹配检查每个数据包，（2）根据10k条规则的快速模式（fast pattern）来检查每个数据包的每个索引（every index of every packet），（3）检查部分匹配规则的非快速字符串模式（non-fast string patterns），这一部分在Snort的full matcher中，但在Pigasus放在了FPGA上。 DMA引擎：对于每个数据包，MSPM输出该数据包部分匹配的规则ID（rule ID）的集合，如果是空集，那么数据包就会被释放到网络中，否则就会被转发到DMA引擎，将数据包传输到CPU进行完全匹配。为了节省PCIe带宽，DMA引擎在FPGA侧的DRAM（检查数据包缓冲区，check packet buffer）中保留一份发送给full matcher的数据包，允许full matcher用一个（数据包ID，决策）的tuple作为响应，而不是在处理后通过PCIe复制整个数据包。 Full matcher：每个数据包都带有metadata，包括MSPM确定为部分匹配的规则ID，然后full matcher检索完整的规则并检查是否完全匹配，然后做出决策（转发或放弃）并写入一个环形缓冲区，由FPGA侧的DMA引擎轮询。 内存资源管理BRAM仅保留给需要延迟敏感或吞吐量要求高的内存访问的模块，也就是reassembler和MSPM。具体来说，reassembler为每个数据包进行流量表的查找和更新，因此需要低延迟的内存访问以提高数据包的速率。MSPM需要高吞吐量，因为每个数据包的每个索引都必须在100Gbps的速率下与多个表（如ShiftOR表、Hash表、不同的header表等）进行检查。为了节省BRAM，其他有状态的模块如数据包缓冲区、DMA引擎分别被分配到eSRAM和DRAM中，这些功能对带宽和延迟的要求不那么严格。即使把BRAM全部给reassembler和MSPM，容量仍然是非常有限的，比如对MSPM使用传统的基于NFA的搜索算法需要23 MB，超过了Intel Stratix 10 MX 2100 FPGA上16 MB的BRAM容量，在后续的章节中会详述如何在不影响性能的情况下高效使用有线容量的BRAM。评价环境 实现：Intel Stratix 10 MX FPGA（16 MB BRAM + 10 MB eSRAM + 8 GB DRAM）+ Intel i9-9960X 16核 @ 3.1 GHz。 流量发生器：DPDK Pktgen与Moongen安装在一台单独的机器上，装有100 Gbps的Mellanox ConnectX-5 EN网络适配器。 Traces与规则集：使用公开的Snort Registered Rulset (snapshot-29141) 以及来自Stratosphere的traces来测试Snort和Pigasus。 测试吞吐量：（1）无损（Zero Loss）吞吐量是通过逐渐增加数据包生成器的传输速率来测量的，直到系统第一次开始丢弃数据包；（2）平均吞吐量为trace中数据包的累积大小（单位为bits）与处理trace所需的总时间的比值。 测试延迟：使用DPDK Pktgen的内置延迟测量程序来测量延迟。 性能与成本具体的结论和分析可以参考原文。FPGA优先架构本章介绍了在端到端系统层面做出的设计决定，即FPGA优先架构，可以为常见的流量定制设计，同时实现高性能和低资源消耗。传统的FPGA-as-offload方案大多数现有的工作都采用了上图中的方案，CPU仍然是主要处理单元，FPGA作为加速器处理输入无关的单一任务，如MSPM。在FPGA优先的方案中，如下图所示，FPGA可以直接访问输入数据，并直接处理常见情况。最重要的区别在于，在这种架构中，FPGA本身也是控制单元，负责管理流量状态。FPGA优先架构这种架构的好处在于可以通过只在FPGA上加速常用情况以平衡两个相互冲突的要求：高性能和低资源利用率。处理输入依赖行为的一个直接方法是为最坏的情况做准备，这样一个单一的设计可以满足所有情况下的要求，然而这种面向最坏情况的设计是不可行的，与普通情况下的设计相比，这样设计提供的性能优势很少，而且在内存方面的资源成本很高。因此，大多数工作都用FPGA加速单一任务，但如下图所示，根据Amdahl’s law，很容易看出仅仅通过加速单个任务来提升性能很难。Fraction of CPU time spent performing each task in Snort 3.0 with Hyperscan接下来的问题是，哪些任务应该留在CPU上？一般的原则是：（1）任务应该是对性能要求不高的，在CPU上运行时间很短和/或不经常被触发，（2）任务在处理流水线的末端，避免FPGA和CPU之间的多次数据移动，（3）任务在FPGA上的实现成本很高。根据这样的原则，full match可以放在CPU上执行。基于快慢分离路径SRAM的TCP重组上述架构要求在FPGA上支持TCP重组，本章介绍了TCP重组的要求和设计空间，以及基于快慢分离路径SRAM的设计。设计要求与设计空间重组是指在存在数据包碎片、丢失和乱序传输的情况下重建TCP字节流的过程。重组对于IDS/IPS是必要的，因为MSPM和full matcher必须检测可能跨越多个数据包的模式（字符串或正则表达式），如下图所示。Reassembler的关键设计目标是在FPGA的内存限制下，以100 Gbps的速度运行，对数十万个流量进行重新排序。TCP重组的例子硬件设计通常倾向于固定长度、恒定时间和一般确定性的数据结构，大多数TCP重组设计也是如此，如采用固定的64 KB数据包缓冲区并使用指针来跟踪每个流量的乱序状态等，通过使用静态缓冲区，实现近乎恒定的时间来将乱序数据包放入内存，资源消耗也是有限制的，然而200 Gbps（读写各100 Gbps）的带宽很难用DDR4 3200来满足，需要有新的方式来缓存数据包。一个潜在的方法是保持相同的数据结构，但开发一个具有保证双向200Gbps带宽的逻辑单地址空间存储器。我们可能可以将多个通道（每个通道都有自己的地址空间）的DRAM或甚至高带宽内存（HBM）聚集起来以获得足够的带宽，并在前面使用SRAM缓存来提供刚性的200Gbps读写带宽，以解决DRAM技术的非确定性。然而，多通道DRAM和HBM可能并不适用于很多FPGA，而且在提供有保证的性能的同时，在读写访问、不同通道和SRAM/DRAM之间进行平衡是非常 复杂的。Pigasus最后采用了SRAM的方法，但简单地采用静态分配固定大小的缓冲区的方法是不行的，因为这种设计既浪费了空间也限制了乱序流量的情况。对软件工程师来说，可以用链表来解决，内存可以按需分配，但从硬件的角度来说，链表的插入时间是可变的，可能导致流水线停滞，降低吞吐量。不过，通过仔细设计重组流水线，分为快速路径（处理常见的顺序数据包）和慢速路径（处理不常见的乱序数据包）可以实现比较好的效果。基于快慢分离路径SRAM的设计重组流程Reassembly流程如上图所示，具体过程如下： 从free list中获取一个未使用的数据包ID，跟踪数据包缓冲区中的空闲slot。 初始处理单元把数据包存储到2 KB的固定slot中，由eSRAM实现的数据包缓冲区中的数据包ID索引，注意数据包缓冲区是由所有流量共享的。同时，提取出来的metadata被发送到流量表。 流量表和乱序链表把分类后的数据包metadata发送给data mover。 Data mover从数据包缓冲区中根据数据包ID获取实际数据。 Data mover将数据包ID释放到free list中进行回收，并把数据包转发给MSPM阶段。 重组功能的简单例子上图展示了我们处理不同数据包顺序的情况。流量表是一个在BRAM中实现的哈希表，将流量五元组标识符映射到一个包含（1）顺序数据包的下一个期望seq序号、（2）链表header节点的指针的表。注意乱序链表是在一个单独的BRAM中实现的，空间由所有的流量动态共享。Pigasus使用三个执行引擎来管理重组状态。每个引擎处理不同类别的输入数据包的metadata：快速路径为已建立的流量处理有序的数据包，插入引擎为新流量处理SYN数据包，乱序引擎为现有流量处理乱序数据包。由于Pigasus是在硬件中实现的，这些引擎可以同时运行在不同的数据包metadata上，而不会互相停顿，但必须注意在获取重组流量表中的共享状态时不要发生冲突。下图展示了重组过程的流水线。重组流水线 快速路径：首先在流量表中查找该流量的条目，如果不存在就将metadata放到插入引擎的队列中，如果存在，那么若metadata与流量表中下一个预期seq序号不匹配，或流量表中指针非空，快速通道就将metadata推送到乱序引擎的队列中，其余情况就将流量表中的预期seq序号更新为流量中后续数据包的seq序号，并将当前数据包送给MSPM。这一过程的时间是恒定的。 乱序引擎：乱序引擎的时间不恒定，对于每个刚刚进入队列的数据包，乱序引擎分配一个新的节点，代表数据包的开始和结束序号，遍历该流量的链表，并在适当的位置插入新分配的节点，还有一些具体的实现细节可以参考原文。 插入引擎：向流量表插入新的流量条目，时间也不恒定。 流量表与乱序引擎流量表的实现也比较复杂，快速路径、插入引擎和乱序引擎需要在流量表上进行同步，上图展示了流量表的数据结构，提供了很高的内存密度。具体的实现细节可以参考原文。分层模式匹配本章介绍了如何为常见情况进行优化，在FPGA上只用3.3 MB的BRAM实现MSPM并达到100 Gbps的性能。通过分层过滤器来分离不同速率下的需求，如前端的小型过滤器和输入速率一致，针对所有规则检查所有数据包（常见情况），而后端的复杂过滤器只针对部分匹配的规则检查可疑的数据包（不常见情况）。MSPM一个Snort规则包括三类模式：header匹配、精确字符串匹配的集合、一组正则表达式。如果所有模式都被识别，一个数据包就会触发规则。在Snort中，MSPM负责检查header匹配和一个高度选择性的精确字符串匹配，称为快速模式。只有两个同时匹配的字符串才会转发到完全匹配阶段，称为非快速字符串模式。Pigasus的MSPM检查快速模式、header和非快速模式，减少了完全匹配对CPU的负荷。现有的设计有经典的基于状态机的FPGA设计和基于哈希表的软件设计（Hyperscan）。 基于状态机的FPGA设计：分为DFA和NFA，类似于编译原理课程中的概念，用FPGA实现，但最多只能适用于几百条规则，对内存和逻辑资源的要求很高。 Snort 3.0 + hyperscan软件设计：如下图所示。首先检查header匹配，然后检查快速字符串匹配模式，这一过程中会利用SIMD进行优化，相比状态机方法大幅提速。具体的实现细节可以参考原文或Snort 3.0的实现。 MSPM in Snort 3.0. Every String Matcher selected by the Port Group Module is evaluated sequentially.分层MSPMPigasus的MSPM，共计消耗3.3 MB BRAM如上图所示，Pigasus首先从字符串匹配开始，然后再进行header匹配（端口分组）。快速模式字符串匹配（Fast Pattern String Matching, FPSM）类似Hyperscan，Pigasus也有一个过滤阶段，数据包会穿越两个并行的过滤器：一个Shift-or匹配器和一组快速模式长度的哈希表，并行检查Shift-or和32x8的哈希表，哈希表只存储一位的值，表示一个给定的 (index, length) 元组是否导致匹配，但是不存储规则ID。之后的规则归纳模块从过滤器的256项输出中选择非0的规则匹配，缩小到8项，如下图所示。具体来说，规则归纳逻辑是一个二分仲裁树，每个仲裁器前都有一个存储单个条目的缓存。规则归纳逻辑（Rule reduction logic）在这个过程之后，只需要为17 KB的规则表创建8个副本，对之前的输出在规则表中查找相应的规则ID，如果没有触发瑞和规则就直接转发到以太网，否则就进入下一阶段的过滤器。header匹配（HM）Header匹配设计在这个阶段，我们使用header数据来确定FPSM阶段产生的匹配是否与相应的规则的端口分组一致，上图展示了header匹配的设计，每个规则单元用一个规则ID来匹配数据包的 (protocol, src port, dest port)，规则ID用来索引Rule2PG表，该表最多返回4个端口分组ID（因为一个规则最多属于4个端口分组）。端口分组ID用来选择相关的PG单元，每个PG单元内查找一个特定的表。最后，所有的并行查询结果被合并，以产生最终的匹配结果。非快速模式字符串匹配（NFPSM）NFPSM中的规则匹配指纹和FPSM一样，Pigasus采用一组哈希表来同时检查所有字符串，为匹配的字符串创建一个“指纹”，与规则指纹表中的指纹进行匹配，如上图所示。分散的面向服务的流设计之前已经有多个新的想法来提高系统不同层次的性能，同时使一组给定输入的资源消耗最小。虽然这些定制化对于高效率来说是至关重要的，但也会导致对输入的过度拟合。因此，Pigasus提出了一个分解和动态的架构，本章重点讨论分解问题。Pigasus 1.0是专门针对单一输入速率（100 Gbps）、特定FPGA（Intel Stratix 10 MX）和特定trace（Stratosphere）的一次性设计，而Pigasus 2.0是一个设计模板，允许用户便捷地重新构建一个分解的、可参数化的流服务库。如果需要扩展header匹配器，Pigasus 1.0需要扩展整个流水线，而Pigasus 2.0只需要扩展其中一小部分Pigasus最初的动机只是为了有效地处理不同的输入，但在这里反而提出了一个更普遍的问题：如何在不事先了解部署环境（包括输入、目标FPGA和目标性能）的情况下设计Pigasus，同时允许用户在编译时轻松地重新调整设计，以获得高效率。之前类似的工作包括分层模块化设计、IP开发、设计抽象化。Pigasus解决方案的三个关键部分包括分解、参数化和通信抽象。Pigasus 2.0架构，流服务通过一个共同的通信抽象进行逻辑连接，通信抽象的RTL实现是在编译时自动生成的分解将设计分解成小块后，用户可以按每个IP的粒度进行扩展，从而用更少的资源达到目标性能。如果分解得太少，我们就不太可能获得预期的效率，如果分解得太多，那么标准化接口的开销就会变得很明显，也会使用户难以理解。为了平衡，遵循以下三个原则： 扩大规模的最小单位：每个流媒体服务都应该是可以扩展的最小单位，如只想扩展header匹配的性能，扩展整个MSPM太大，分解到某一个子表则太小，根据这一原则将MSPM分为3个独立的服务，即FPSM、HM和NFPSM。 封装对延迟敏感的部分：不应该破坏任何对延迟敏感的部分，如eSRAM有固定的响应周期，使用eSRAM的data mover应该和数据包缓冲区以及重组逻辑封装为Pigasus 2.0中的一个更大的重组服务。 对外暴露相同的标准接口：Pigasus采用了Avalon流接口，可以很容易适应其他标准，如AXI接口等，下面的代码给出了一个服务的接口，每个服务包括数据包通道、metadata通道、用户数据通道。 module example_service ( // Input packet data input logic [511:0] in_pkt_data, input logic in_pkt_valid, input logic in_pkt_startofpacket, input logic in_pkt_endofpacket, input logic [5:0] in_pkt_empty, output logic in_pkt_ready, // Input metadata input metadata_t in_meta_data, input logic in_meta_valid, output logic in_meta_ready, // Input user data input logic [511:0] in_usr_data, input logic in_usr_valid, input logic in_usr_startofpacket, input logic in_usr_endofpacket, input logic [5:0] in_usr_empty, output logic in_usr_ready, // Output direction is omitted for simplicity)参数化通过SystemVerilog参数和基于Python的Jinja2模板的组合来实现的。例如，MSPM中的许多子模块（如哈希表、规则归纳等）都是在Jinja2模板中编码的，这些模板生成最终的SystemVerilog文件。原文给出了一些关键的可调参数及其影响。通讯抽象Pigasus提出了一个通信抽象，所有的服务都被逻辑连接到一个共同的抽象上，在高层次组合各种服务，编译器负责生成RTL，不损失效率的同时，提供了更好的可移植性、设计效率和可调试性。 不同的通道实现：通信抽象提供了服务间通信通道的不同物理实现，如FPGA内部（NoC）或FPGA之间（以太网或PCIe）的通信，用户只需要在Python级别选择适当的服务通道类型，不需要关心RTL实现细节。 负载平衡：通信抽象在内部提供了负载平衡服务，允许用户简单地创建副本，而不必关心负载分配和结果的合并。 调试：当数据在内核之间流动时可以收集统计数据，以供用户快速识别性能瓶颈或定位功能错误。 在多FPGA系统上使用共同通信抽象探索不同的流水线分割方式动态溢出机制在编译时准确地预测实际工作负载在运行时的行为是非常困难的。因此，在编译时做出的设计决定很可能是不完美的，可能配置不足从而损失了性能，或过度配置从而浪费了资源。动态溢出机制可以动态地按需调出备份流服务，以吸收流量的变化。Pigasus针对常见情况进行调整，分配足够的资源来提升常见情况下的性能，但也可能使设计在面对不断变化的工作负载时变得脆弱。处理流量突发性的典型方法是使用缓冲，但是在100 Gbps的情况下，流量中的小抖动（jitters）很容易溢出MB级别的片上缓冲空间，且由于实时处理（IPS模式）的延迟要求，我们不能无限期地缓冲流量。以下是一个例子来说明动态溢出机制，当NFPSM作为MSPM模块的最后一个模式匹配阶段处于高负荷时，动态地将溢出的流量路由到CPU。Pigasus设计了一个溢出路径，可以将流量路由到CPU，如下图所示。调度器不断地检查输入缓冲区的占用率来监控NFPSM的负载，如果占用率超过了运行时间可配置的阈值，调度器将把溢出的流量送到DMA引擎，并分配给CPU内核。注意两个细节：当溢出发生时，NFPSM和CPU都在工作，没有完全绕过NFPSM；未被NFPSM处理的溢出流量不会破坏系统的正确性，因为CPU将对数据包进行更完全的匹配。NFPSM CPU溢出设计Notes这篇笔记省略了相当多的实现细节和实验评价，但这些内容同样非常精彩，作者在原文的结论部分也给出了相当多的不足与未来工作值得学习，这篇笔记中并没有涉及，感兴趣的读者可以参考原文。另外，原文的附录给出了详细的环境配置、Pigasus构建、实验方法等内容可以参考。" }, { "title": "《片上网络》笔记（五）：流量控制", "url": "/posts/NOC5/", "categories": "读书笔记, 片上网络", "tags": "", "date": "2022-05-24 19:41:00 +0000", "snippet": "流量控制决定了各种资源（网络缓冲器和连接）的分配，如资源如何被分配给消息、分配的粒度、资源如何被不同的消息共享等。良好的流量控制程序可以在少量额外开销的代价下降低消息的延迟，通过有效共享资源来提高网络吞吐量，降低网络功耗。流量控制协议的实现复杂度取决于路由器微架构的复杂性以及路由器之间资源信息的通信所需的布线开销。消息、包、Flits和Phits当一个消息被注入网络时，它首先被分割成数据包（...", "content": "流量控制决定了各种资源（网络缓冲器和连接）的分配，如资源如何被分配给消息、分配的粒度、资源如何被不同的消息共享等。良好的流量控制程序可以在少量额外开销的代价下降低消息的延迟，通过有效共享资源来提高网络吞吐量，降低网络功耗。流量控制协议的实现复杂度取决于路由器微架构的复杂性以及路由器之间资源信息的通信所需的布线开销。消息、包、Flits和Phits当一个消息被注入网络时，它首先被分割成数据包（简称为包），然后被分割成固定长度的flits（flits是flow control units的简称）。例如，一个128字节的缓存行从共享者发送到请求者，将作为一个消息被发送，如果最大的包大小大于128字节，整个消息将被编码为一个单一的包。这个包会包括一个包含目标地址的head flit、body flits以及表示包结束的tail flit。Flits可以进一步细分为phits（即physical units），对应于物理信道宽度。下图展示了消息到包以及包到flits的分解。简而言之，消息是网络中的通信逻辑单位，而数据包是对网络有意义的物理单位。一个数据包包含目的地信息，而一个flit可能不包含，因此一个数据包的所有flits必须走相同的路线。消息、包于flits的组成：假设单个flit大小为16字节、缓存行为64字节，那么一个缓存行的包由5个flits组成，一个缓存一致性命令会是一个单独的flit由于丰富的片上布线资源，片上网络中的信道往往很宽，所以消息很可能由单个数据包组成。不过在片外网络中，信道宽度受到引脚带宽的限制，这种限制导致flits被分解成更小的块，称为phits。迄今为止，在片上网络中，由于片上信道较宽，一个flit往往由单个phit组成，是信息的最小细分单位。注意如上图所示，许多消息实际上是单flit的包。流量控制是根据资源分配的粒度来分类的，我们将在接下来的章节中讨论基于消息、数据包和flit粒度的技术。基于消息的流量控制我们从电路切换开始，这是一种在消息层面上工作的技术，粒度最粗。电路交换（Circuit Switching）从Core 0到Core 8电路交换的例子，同时Core 2被停顿（S：Setup Flit，A：Acknowledge flit，D：数据消息，T：Tail (deallocation) flit，每个D都表示一条消息，注意在第12和16周期时，源节点没有数据要发送）电路交换将资源（链接）预先分配给整个消息，setup的信息被发送到网络，整条链路都被保留，并发回acknowledge信号，随后就可以发送数据，完整的工作流程如上图所示。由于链路是预先保留的，每一跳都不需要缓冲器来保存等待分配的数据包，因此可以节省能耗，但在setup和实际信息传输之间，链路是空闲的，其他想要使用这些资源的消息会被阻断，导致带宽利用率很低。基于包的流量控制基于包的流量控制技术首先将消息分解成数据包，然后在链路上交错放置这些包，从而提高链路利用率。与电路交换不同的是，这种技术将需要每个节点在缓冲区中临时存储网络中的包。存储和转发（Store-and-forward）在这种技术中，每个节点都会等待，直到收到整个包，然后再将包的每个部分转发给下一个节点。因此，每一跳都会产生较长的延迟，不适合对延迟要求较高的NoC。此外，存储和转发流量控制需要在每个路由器上有足够大的缓冲区来缓冲整个数据包，对NoC的适应性较差。在下图中，我们使用这种技术来发送从0到8的包，每个包需要5跳，因此每一跳传输包的延迟是5个周期。存储和转发的例子虚拟直通（Virtual Cut-Through）为了减少数据包在每一跳所经历的延迟，虚拟直通允许在整个包被当前路由器收到之前，将包传输到下一个节点，减小了延迟。如下图所示，相比之前传输整个包需要25个周期，这种技术只需要9个周期就可以完成。然而，带宽和存储仍然以数据包大小为单位分配，只有在下一个下游的路由器有足够的存储空间来容纳整个数据包的情况下，数据包才能继续发送。对面积和功率都很紧张的NoC来说，当包的尺寸较大时（如64或128字节的缓存行），很难容纳支持 虚拟直通所需的大型缓冲区。在下图b中，整个包在从节点2到节点5的过程中被延迟了，即使节点5的缓冲区可以容纳5个flits中的2个。虚拟直通的例子基于Flit的流量控制相比前文中提到的技术，基于流量控制的机制对缓冲区大小的要求较低，有助于路由器满足芯片上严格的面积或功率限制。虫洞（Wormhole）与虚拟直通一样，虫洞流量控制也会对流量进行切割，允许flits在当前路由器接收到整个包之前，将流量转移到下一个路由器。然而，与存储和转发或虚拟直通不同，虫洞将存储和带宽分配给flits而非整个包，允许每个路由器中使用相对较小的缓冲区。这种技术延迟低，可以用更少的缓冲实现，对面积和功率的要求低，成为了现在采用的主要技术。这种方式虽然有效使用了缓冲，但是对带宽的使用并不高效。尽管这种技术以flit的大小为单位分配存储和带宽，但一条链路会在路由器中的包的整个寿命期间都会被保留。因此，当一个包被停顿时，这个包所保留的所有物理链路都是空闲的。由于虫洞是以flit为单位分配缓冲，一个由许多flit组成的包可能跨越多个路由器，导致大量空闲的物理链路，导致吞吐量降低。如下图所示，每个路由器的缓冲大小为2个flit，节点1缓冲区空间不足导致停顿，整个通道被该数据包占用，如灰色所示。虫洞流量控制的例子虚拟通道（Virtual Channels）虚拟通道被称为互联网络的“瑞士军刀”，最早是作为避免死锁的解决方案被提出的，但也被用于缓解流量控制中的队列头部阻塞（head-of-line blocking）以提升吞吐量，这种阻塞在上面所有技术中都会出现，具体来说就是每个输入口都有一个队列，当队列头部的数据包被阻塞时，就会使排在后面的后续数据包停顿，即使停顿的数据包还有可用资源可以使用。从本质上来说，一个虚拟通道（VC）是路由器中的独立队列，多个VC共享两个路由器之间的物理连接，通过将多个独立的队列与每个输入端口连接起来，可以减少队列头部阻塞。VC在一个周期内对物理链路带宽进行仲裁，当一个占用某个虚拟通道的包被阻塞时，其他包仍可通过其他虚拟通道穿越物理链路。因此，VC提高了物理链路的利用率，提升了整个网络的吞吐量。从技术上来说，VC可以应用于上述所有的流量控制技术，以减轻队列的压力。比如说，电路交换可以用虚拟通道而非物理信道来实现，这种技术称为虚拟电路交换。存储和转发流量控制可以和VC一起使用，有多个数据包的缓冲队列，VC在链路上逐包复用，虚拟直通也类似。由于NoC绝大多数采用虫洞流量控制，之后我们提到虚拟通道流量控制时，我们就默认是和虫洞一起使用的，以flits的粒度对缓冲和链路进行管理和复用。VC流量控制的例子：A和B两个包各自被分解为4个flits（H：head，B：body，T：tail）上图是一个说明VC流量控制的例子。A最初占用VC 0，目的地是节点4，B最初占用VC 1，目的地是节点2。在T=0时，A和B都有flit在节点0的左边输入VC中等待。A的head flit被分配到路由器1的左边输入VC 0，并获得开关分配，在T=1时前往路由器1。在T=2时，B的head flit获得开关分配并前往路由器1，存储在VC 1，与此同时，A的head flit未能获得路由器4（A的下一条）的VC，节点4的两个VC都被其他包的flit占用。A的第一个body flit继承了VC 0，并在T=3时前往路由器1，与此同时，B的head flit可以在路由器2分配VC 0并继续前进。在T=4时，B的第一个body flit从head flit继承了VC 1，并获得开关分配，继续前往路由器1。到了T=7时，B的所有flit都到达了路由器2，但A的head flit仍然被阻塞，继续等待一个空闲的VC以前往路由器4。通过这个例子可见，首先在每个路由器上分配一个VC给head flit，其余的flit继承该VC，通过虚拟通道流量控制，不同的包可以在同一物理通道上交错传输。这种技术也被广泛用于解决死锁，包括网络内死锁以及协议级死锁。下表给出了上述各种技术关于资源分配和利用的总结。流量控制技术总结无死锁流量控制死锁可以通过使用有特定限制的路由算法来解决，也通过我们接下来讨论的无死锁流量控制来解决，可以允许使用任何路由算法。日期变更线（Dateline）和VC分区（VC Partitioning）两个VC有上图展示了两个VC如何解决路由协议中出现的死锁。每个VC都和单独的缓冲队列绑定，在物理链路上逐周期地进行时间多路复用。在途中，所有消息都通过VC 0发送，直到跨过日期变更线，消息被分配到VC 1，且不能重新被分配到VC 0，这就确保了信道依赖图（channel dependency graph, CDG）是无环的。类似的想法也适用于各种流量无关的或自适应的路由算法，例如一个在X-Y和Y-X路线之间随机选择的路由算法可以通过强制所有X-Y数据包使用VC 0以及所有Y-X数据包使用VC 1来避免死锁。逃逸虚拟通道（Escape VCs）之前例子中，所有的数据包最初都被分配到VC 0并保持在VC 0，导致VC 1利用率很低。A new theory of deadlock-free adaptive routing in wormhole networks一文中Duato提出了逃逸虚拟通道的概念来解决这个问题。只要有一个无死锁的逃逸VC，所有其他VC就可以使用没有路由限制的完全自适应路由，而不是在所有虚拟通道之间强制要求一个固定的顺序或优先级，逃逸VC通常使用无死锁路由技术。逃逸虚拟通道的例子（VC 1作为逃逸VC，采用维度顺序X-Y路由）气泡流量控制（Bubble Flow Control）另一个避免死锁的想法是，在不需要多个VC的情况下，确保缓冲之间的封闭循环依赖关系不会产生。即使使用DOR这样的无死锁路由，k元n维在每个维度存在一个环形网络，也可能产生循环依赖。气泡流量控制可以和虚拟直通结合使用，具体来说就是向网络中注入一些气泡，保证缓冲区中有充足的空间。下图展示了一个例子，R1有两个气泡，允许数据包P1输入。其余的路由器只有一个气泡，阻塞了P0和P2的输入。气泡流量控制的例子缓冲区反压（Buffer Backpressure）由于大多数NoC设计不能丢弃数据包，因此必须有缓冲区反压机制来进行停顿。当下一跳没有可用的缓冲区时，就不能继续传输flits。缓冲区反压的最小单位取决于流量控制协议，可以以包或flit为单位来管理，注意电路交换是一种无缓冲区的流量控制技术，因此不需要反压机制。两种常用的反压机制是基于信用（credit-based）和开关信号（on/off signaling）。 基于信用（credit-based）：信用（credits）记录了下一条可用的缓冲区数量或容量，buffer被空出时，往前一跳发出信号增加信用计数，当一个flit离开当前路由器发往下一跳时，向其发出信号递减信用计数。 开关信号（on/off signaling）：开关信号涉及相邻路由器之间的信号，当缓冲区的数量或容量低于阈值时，该信号被设置，以停止前一跳的传输。这个阈值需要提前设定。 实现流量控制协议的实现复杂度取决于路由器微架构的复杂性以及路由器之间资源信息的通信所需的布线开销。这一节中我们主要讨论后者。为周转时间（Turnaround Time）确定缓冲区大小由于缓冲区过小导致的流量限制（C：Credit send，C-LT：Credit link traversal， C-Up：Credit update）周转时间是指连续flits可以重复使用一个缓冲区之间的最小空闲时间，这个值如果较大会导致缓冲区的无效重复使用，从而导致网络吞吐量下降。如果缓冲区大小不能覆盖周转时间，那么网络就需要在每个路由器上限制流量，降低性能。周转时间会影响缓冲区大小，从而影响面积开销。如上图所示，两个路由器之间的连接在等待下游路由器空出缓冲区时，会空闲6个周期。缓冲区反压机制时间线基于信用和开关信号的反压机制时间线如上图所示。在基于信用的反压中，周转时间至少是一个flit到下一个节点的延迟、返回信用信号的延迟和流水线延迟之和，如上图a。相比之下，在开关信号的反压中，周转时间至少是开关信号传播延迟的2倍、数据flit传播延迟和流水线延迟之和，如上图b。反向信号线反向信号的开销虽然开关信号反压与基于信用的反压相比性能较差，但在信号线开销方面表现更好，如上图所示。不过，对NoC而言，面积和吞吐量通常更重要，因此基于信用的反压机制仍然更合适。" }, { "title": "《片上网络》笔记（四）：路由", "url": "/posts/NOC4/", "categories": "读书笔记, 片上网络", "tags": "", "date": "2022-05-23 21:19:00 +0000", "snippet": "在确定网络拓扑结构后，路由算法用来决定消息通过网络到达目的地的路径。路由算法的目标是在网络拓扑结构所提供的路径中均匀地分配流量，避免热点的出现并尽量减少竞争，改善网络的延迟和吞吐量。在实现这些性能目标的同时，遵守设计约束，避免关键路径过长、面积过大。虽然路由电路本身的功耗通常较低，但特定的路由算法会直接影响到跳数，从而大大影响到消息传输的能量消耗。此外，路由算法实现的路径多样性对于提高网络故...", "content": "在确定网络拓扑结构后，路由算法用来决定消息通过网络到达目的地的路径。路由算法的目标是在网络拓扑结构所提供的路径中均匀地分配流量，避免热点的出现并尽量减少竞争，改善网络的延迟和吞吐量。在实现这些性能目标的同时，遵守设计约束，避免关键路径过长、面积过大。虽然路由电路本身的功耗通常较低，但特定的路由算法会直接影响到跳数，从而大大影响到消息传输的能量消耗。此外，路由算法实现的路径多样性对于提高网络故障时的可用性也影响很大。路由算法的类型路由算法一般分为三类：确定性的、流量无关（oblivious）的和适应性的。虽然路由算法五花八门，但NoC最常用的路由算法是最简单的维度排序路由（dimension-ordered routing, DOR）。DOR是确定性路由算法的一个例子，在这种算法中，所有从A到B的信息总是途径同一路径，消息逐个维度地穿越网络，如下图所示。流量无关的路由算法中，消息可以选择不同的路径，但不考虑网络拥堵情况。更复杂的算法可以是自适应的，消息的路径取决于网络拥堵情况。不同的路由算法路由算法也可以分为最小化和非最小化的。最小化路由算法只选择源和目的地之间跳数最少的路径，非最小化路由算法允许选择可能会增加跳数的路径。在没有拥堵的情况下，非最小化路由会增加延迟和功耗，但在拥堵的情况下，可能会降低延迟。避免死锁在选择或设计一个路由算法时，不仅要考虑其对延迟、功耗、吞吐量和可靠性的影响，大多数应用还要求网络保证无死锁，下图就展示了一种可能出现的死锁。由于死锁，网络中的4个包无法继续前进确定性维度排序路由（DOR）DOR可以由允许哪些转弯来描述，如下图所示。a可能导致死锁，而b中没有循环，不会产生死锁。二维Mesh网络中的转弯DOR既简单又无死锁，但也消除了Mesh网络中的路径多样性，降低了吞吐量，在平衡网络负载方面效果很差。流量无关路由这种路由算法选择路径时不考虑网络的状态，实现也可以很简单。如Valiant在Universal schemes for parallel communication一文中提出的随机如路由算法，对于从s到d的一个包，随机选择一个中间目的地d’，首先从s路由到d’，再从d’路由到d。通过在路由到最终目的地之前先路由到一个随机选择的中间目的地，Valiant的算法能够在整个网络中实现负载平衡，但这是以牺牲一定局部最优性为代价的，增大最大通道负载，减小网络带宽。流量无关路由算法的例子这种算法可以被限制为只支持最小路径，如上图所示a和b的区别。这种算法在于X-Y路由一起使用时是无死锁的。自适应路由一个更复杂的路由算法可以是自适应的，也就是说，信息从A到B的路径取决于网络的流量情况。自适应的路由决策可以依赖于局部或全局的信息，如缓冲区占用率、延迟等信息。和之前一样，这种算法也可以被限制为只支持最小路径，但也可以采用misrouting，允许往其他非最小的路径路由，这种情况下可能出现活锁。对于一个完全自适应的路由算法，死锁可能是一个问题。另一个挑战是保持一致性协议可能需要的消息间排序，可以采用再目的地重新排序信息的机制，或者对某一类特定消息的路由进行限制，防止乱序。自适应路由中的转弯我们先前介绍了路由算法中的转弯，现在讨论转弯模型如何更广泛地应用于无死锁的自适应路由算法。转弯模型之前一个二维网络中的八个转弯只允许其中四个可能的转弯，现在我们允许其中六个转弯以提升算法的灵活性，如上图所示，这三种转弯模式都是无死锁的，但需要注意的是下面这种模型是可能产生死锁的。转弯模型中的死锁还有一些更复杂的模型，如奇偶转弯模型中，根据当前节点是在奇数列还是偶数列，消除一组的两个转弯，这种方式同样可以消除死锁，且提供更好的适应性。多播路由到目前为止，我们一直专注于单播（即一对一）的路由算法。然而，经常会出现这样的情况：一个核心需要向多个核心发送同一消息，这被称为广播（如果系统中的所有内核都需要得到信号）或多播（如果系统中的一个子集的内核需要得到信号），如在共享内存系统中基于广播和基于目录的缓存一致性协议。一个简单的组播实现是简单地发送多个单播，但这大大增加了流量，导致延迟高、吞吐量低。Virtual circuit tree mul- ticasting: A case for on-chip hardware multicast support和Towards the ideal on-chip fabric for 1-to-many and many-to-1 communication等文提供了NoC组播路由的一些方法。实现路由算法可以在源节点或每个路由器内使用查找表来实现，组合电路可以作为基于表的路由的替代方案。实现方式需要权衡利弊，并不是所有的路由算法都能通过每一种实现方式来实现，如下表所示。路由算法的实现方法源节点路由路由路径可以被嵌入到数据包的header。这种方式有几个好处，如延迟低（路由不需要计算或查询）、路由硬件少、源节点路由表可以灵活配置、支持不规则的拓扑结构、源-目的地的路由可以存储在表中供随机选择以提升负载平衡；也有一些缺点，如需要在数据包中额外存储路径（且根据网络变大而变大）、难以根据网络条件动态避免拥堵。节点表这种方法在每一跳时查询路由表，可以实现自适应算法，并且在做决定时可以利用每一跳的网络拥堵信息。路由表也可以是可编程的，可以通过改变路由表来绕过网络中的故障，但缺点在于增加了数据包的延迟。组合电路路由对简单的路由算法来说，消息可以对目的地的坐标进行编码，通过组合电路以确定接下来的路径，延迟可以非常低。通过在组合电路中实现路由决策，该算法被限定为一种拓扑结构和一种路由算法，牺牲了通用性。2-D Mesh网络中的路由电路自适应路由自适应路由算法需要跟踪网络拥堵程度的机制，并更新路由路径。路由的调整可以通过修改header、在电路中添加是否拥堵的信号作为输入、实时更新路由表条目等方法实现。这种方式的好处是通过基于网络条件的路由决策，网络可以实现更高的带宽，并减少数据包的拥堵延迟；缺点是实现复杂，基于拥堵的路由决策需要额外的电路，增加路由决策的延迟和路由器的面积，可能需要从相邻的路由器沟通额外的信息，这种额外的通信可能会增加网络面积和功耗。" }, { "title": "《片上网络》笔记（三）：拓扑结构", "url": "/posts/NOC3/", "categories": "读书笔记, 片上网络", "tags": "", "date": "2022-05-20 23:09:00 +0000", "snippet": "NoC的拓扑结构决定了网络中节点和通道之间的物理布局和连接，直接决定了消息的跳数（hops）、节点之间的距离、网络延迟、能耗、如何分散路径等。最简单的拓扑结构之一是总线，总线上每个信号都可以被所有总线上的设备观察到，但可扩展性差，带宽有限。指标我们在设计NoC时第一个做出的决定往往是对拓扑结构的选择，因此在网络的其他元素确定之前，有一种快速比较不同的拓扑结构的方法是非常有用的。下图展示了三种...", "content": "NoC的拓扑结构决定了网络中节点和通道之间的物理布局和连接，直接决定了消息的跳数（hops）、节点之间的距离、网络延迟、能耗、如何分散路径等。最简单的拓扑结构之一是总线，总线上每个信号都可以被所有总线上的设备观察到，但可扩展性差，带宽有限。指标我们在设计NoC时第一个做出的决定往往是对拓扑结构的选择，因此在网络的其他元素确定之前，有一种快速比较不同的拓扑结构的方法是非常有用的。下图展示了三种常用的拓扑结构，来说明一些常用的指标。常见的NoC拓扑结构流量无关的指标以下这些指标与流经网络的流量无关： 度数（Degree）：和图论中的度数是同一个概念，如上图中，Ring拓扑的度数是2，Torus拓扑的度数是4，Mesh网络中并非所有的switch都有统一的度数。度数一定程度上可以表示网络实现的成本，原因在于更高的度数需要每个节点有更多端口，面积和功耗会更大。 分割带宽（Bisection bandwidth）：分割带宽是指将网络分割成两个相等部分之间的带宽，如上图中，Ring拓扑有2条链接，Mesh是3条，Torus是6条。这个指标在确定一个特定网络的最坏情况下的性能时通常是有用的，因为分割带宽限制了可以从系统的一侧移动到另一侧的数据总量。分割带宽也可以表示网络布线的成本。 直径（Diameter）：和图论中的直径也是同一个概念，如上图中，Ring的直径是4，Mesh的直径是4，Torus的直径是2，在没有竞争的情况下，直径可以表示网络中的最大延迟。 流量相关的指标以下这些指标与流经网络的流量相关： 跳数（Hop count）：一个消息从源头到目的地所需的跳数，最大跳数即网络直径。除了最大跳数以外，平均跳数也可以表示网络的延迟。 最大通道负载（Maximum channel load）：这个指标对于估计网络能够支持的最大带宽很有用，或者说，在网络饱和之前，每个节点可以向网络注入的比特/秒（bps）的最大带宽值。即最大注入带宽（Maximum injection bandwidth） = 1 / 最大通道负载。直观地说，这个指标涉及到网络中的哪条链路或通道在特定的流量模式下将是最拥挤的，因为这条链路将限制整个网络带宽。信道负载可以通过各种方式计算，通常使用概率分析。 路径多样性（Path diversity）：如上图中，Ring没有路径多样性，但Mesh和Torus都提供了路径多样性。 直接拓扑结构：Ring、Mesh和Torus直接网络是指每个终端节点（例如，多处理器中的每个处理器核心或缓存）都与一个路由器相关联。目前大多数NoC的设计都采用直接网络。直接拓扑结构可以由k元n维立方体来描述，k是每个维度立方体边上节点的数量。比如说，一个4x4的mesh或torus是4元2维立方体，共16个节点，一个4x4x4的mesh或torus是一个4元3维立方体，共64个节点。大多数NoC用的都是2维拓扑结构，可以很好地映射到平面上，但片下网络就不一定了，因为机箱中的电缆可以在三维空间内连接。间接拓扑结构：Crossbar、蝴蝶、Clos网络、Fat Tree间接网络通过一个或多个中间阶段的交换节点连接终端节点。具体内容可以参考原书，这里只给出了一些书中的插图。一个2-ary 3-fly的蝴蝶网络一个4x4的平铺蝴蝶网络一个对称的Clos网络Fat tree网络不规则的拓扑结构MPSoC的设计可能会使用各种不同的IP核，上述对称结构可能并不合适，定制的拓扑结构往往比标准的结构更省电、性能更好。下图展示了一个视频解码器定制拓扑结构的例子。一个视频解码器的拓扑结构设计拆分与合并有两种技术来定制拓扑结构：拆分与合并。通过拆分，一个大型的全连接交换机（很可能违反设计约束）被逐渐拆分成较小的交换机（直到满足设计约束）。通过合并，拓扑结构中相邻的交换机可以被合并，以降低功耗和面积。拓扑综合算法拓扑的综合和映射是一个NP-hard问题，因此人们提出了一些启发式的方法来高效寻找最佳的拓扑结构。分层拓扑结构到目前为止，我们假设网络节点和终端节点之间是一对一的对应关系，且整个系统的拓扑结构是统一的。然而，在真实的系统中，多个节点可能在一个拓扑结构中放在一起形成集群，集群通过另一种拓扑结构连接在一起，形成一个分层的拓扑结构。分层拓扑结构分层拓扑结构的最简单形式是多个终端节点共享同一个路由器节点。如上图所示，a中4个终端节点共享一个路由器，只需要9个路由器就可以连接36个节点，但也可能增加网络的复杂性，b中32个节点被分为4个8节点的集群，通过环连接在一起，难点在于对进入中心环的带宽进行仲裁。" }, { "title": "《片上网络》笔记（二）：系统架构与接口", "url": "/posts/NOC2/", "categories": "读书笔记, 片上网络", "tags": "", "date": "2022-05-18 03:00:00 +0000", "snippet": "在这一章中我们研究三种主要的系统架构：共享内存芯片多处理器（shared-memory chip multiprocessor, CMP）、消息传递系统、多处理器SoC（multiprocessor SoC, MPSoC）。共享内存CMP并行编程中，一个全局的地址空间通常比分区的地址空间设计起来更简单。在现代SMP设计中，分区全局地址空间（partitioned global address...", "content": "在这一章中我们研究三种主要的系统架构：共享内存芯片多处理器（shared-memory chip multiprocessor, CMP）、消息传递系统、多处理器SoC（multiprocessor SoC, MPSoC）。共享内存CMP并行编程中，一个全局的地址空间通常比分区的地址空间设计起来更简单。在现代SMP设计中，分区全局地址空间（partitioned global address space, PGAS）很常见，也就是说地址的高位决定了内存位于哪个节点。相比之下，消息传递范式要求程序员明确地在节点和地址空间之间移动数据。在大规模并行架构中，同时利用这两种方法的混合方法很常见。在这一节，我们主要讨论共享内存CMP。和SMP一样，CMP通常也有一个共享的全局地址空间，但和SMP不同的是，CMP可能有不一致的内存访问延迟。在共享内存模型中，通信是通过load、store指令产生的，对程序员并不可见。逻辑上来说所有处理器都访问相同的共享内存，但物理上来说，需要使用缓存来提升性能，但在这种设计下缓存一致性的设计就变得复杂，缓存一致性协议决定了哪些通信是必要的。共享内存CMP架构上图是一个典型的有64节点的共享内存CMP架构，每个节点都包括一个处理器、私有的L1缓存、以及一个可能是私有或共享的L2缓存，同时可能存在一个所有处理器共享的L3缓存。一致性协议对网络性能的影响一致性协议对网络发出的请求多处理器系统通常依靠广播或目录来实现一致性协议，两种类型会导致不同的网络传输特性。广播协议中，一致性请求被发送到芯片上所有节点，对带宽要求高，可以依靠两个物理网络：一个用于排序，以及另一个高带宽但无序的用于数据传输。目录协议则依赖于点对点的通信，通过维护一个共享列表，避免了向整个系统广播无效请求。NoC一致性协议的要求一致性协议需要几种类型的信息：单播（unicast）、多播（multicast）和广播（broadcast）。目录协议中，大多数请求都是单播的，而广播协议则更复杂。一致性协议通常也需要两种消息大小。第一种用于不传输数据的一致性请求和响应，包括一个内存地址和一个一致性命令。第二种用于数据传输，包括一个完整的缓存行和内存地址。协议级网络死锁协议级死锁共享内存系统要求网络不存在协议级死锁，上图就展示了一种协议级死锁。协议可能需要几个不同的消息类别，一个类别中的请求消息不会导致同一类别中另一个请求消息的产生，但可以触发一个不同类别的消息。当不同类的消息之间存在资源依赖时，就会出现死锁。这里我们描述了三个类别：请求、干预、响应。请求消息包括load、store、更新和写回。干预是由目录发送的消息，请求将修改后的数据传输到新的节点。响应包括无效确认、否定确认（表明请求失败）和数据消息。可以使用多个虚拟通道来避免协议级死锁，如Alpha 21364为每个消息类别分配了一个虚拟通道，可以打破请求和响应之间的循环依赖。缓存层次结构对网络性能的影响节点的设计会对网络的带宽要求有很大影响，在本节中我们着重研究缓存层次结构的影响，包括多个层次的缓存、目录一致性缓存和内存控制器。在先前展示的共享内存CMP架构一图中，L2缓存是私有的，可以减少L2缓存命中的延迟，但缺点也很明显，几个L2缓存需要复制共享数据，导致缓存利用率降低。由于每个内核都有一个私有L2缓存，只有L2缓存Miss的请求才会进入网络，缓存之间的互联流量减少。在这种设计中，NoC只与每个内核的L2缓存连接，如下图(a)所示。私有/共享L2缓存私有L2缓存命中/未命中的网络访问流程上图展示了私有L2缓存在命中和未命中情况下的例子。私有L2缓存未命中的情况下会产生两次网络访问和一次片外的内存访问。相比之下，共享L2缓存可以更有效地利用存储空间，减少对内存的访问带宽的压力。下图展示了共享L2缓存在命中和未命中情况下的例子。共享L2缓存未命中的情况下会产生最多四次网络访问的请求。共享L2缓存命中/未命中的网络访问流程Home节点与内存控制器的设计问题在目录协议中，每个地址都会静态映射到一个Home节点，存储目录信息，并负责向所有映射到该节点的地址发出一致性请求。在之前提到的两级的缓存层次结构中，L2缓存缺失需要通过内存控制器访问内存，内存控制器也会成为网络传输的热点。如下图所示，内存控制器可以与缓存共享一个网络端口，也可以作为独立的节点放置于网络之中。内存控制器MSHR &amp; TSHR处理器到网络的接口对非阻塞缓存而言，上图展示了处理器MSHR到网络的接口。例如，在一个读请求中，MSHR初始为读等待状态，并向网络发出一个消息，消息内容包括目标地址、请求的缓存行地址、消息请求类型。之后MSHR将回复的消息与未完成的请求进行匹配。内存到网络的接口内存到网络的接口则负责接收来自处理器的内存请求并进行恢复，在内存端，由TSHR处理器未完成的内存请求。如果内存控制器可以保证按顺序为请求提供服务，TSHR可以简化为一个FIFO队列。当内存请求完成时，内存到网络的接口将消息发送到网络，发回原始请求者。消息传递在消息传递范式中，用户需要调用相关函数来进行消息的发送和接收，把数据从一个进程转移到另一个进程，实现进程之间的通信和同步。这里我们重点讨论消息传递和网络设计之间的关系。在共享内存范式中，访问共享数据很容易通过一个全局共享的地址空间实现。在消息传递范式中，则需要明确识别数据的拥有者。消息类型和大小是非常灵活的，由软件来承担解码和处理消息的工作，但也会带来较大的开销。在共享内存中，整个网络和消息的存储对程序员是透明的，完全由硬件管理，但消息传递通过减少不必要的通信和大块数据的高效通信来分摊通信相关的额外开销和延迟，其硬件成本和设计复杂性通常低于共享内存，但也在系统的其他地方引入了额外的复杂性。消息传递分为阻塞式和非阻塞式。阻塞或同步消息传递需要发送者等待，直到接收者确认消息，虽然概念上很简单，但可能导致死锁问题，例如，两个进程同时发出发送命令并等待，两个进程都无法继续执行接收命令，并将一直等待。非阻塞或异步的消息传递允许发送者在发送消息后立即进行之后的工作，消除了与死锁有关的问题，但也带来了存储未完成消息的额外复杂性。关于消息的存储，有几种不同的策略。消息可以直接写入专用的缓冲或寄存器，也可以通过MMIO存储在内存中。接收者可以通过中断或对内存映射的位置进行轮询来访问消息。NoC接口标准NoC需要遵循标准化的协议，才可以和其他具有相同标准接口的IP块互相通信、即插即用。目前SoC中广泛使用的片上通信标准有ARM的AMBA、意法半导体的STBus、Sonics的OCP、OpenCores的Wishbone等。在此我们以ARM的AMBA AXI为例，讨论这些标准的一些共同特征。更详细的内容可以参考 On-Chip Communication Architectures: System on Chip Interconnect，其中详细介绍了现有的片上总线的协议。总线事务的语义连接到网络的节点被定义为主设备和从设备，通过事务进行通信。主设备发出请求开始事务，从设备接收并随后处理事务，当从设备对原始请求做出响应时，事务就完成了。例如，处理器内核是主设备，通过向内存模块发出写请求来启动一个新的事务，内存模块是从设备，执行写请求并回复写确认响应。AXI读通道与写通道AMBA AXI协议中的每个事务都在地址通道上发送地址和控制信息，而数据则在数据通道上以突发方式发送，写入有一个额外的响应通道，如上图所示。这些通道的宽度可以从8位到1024位不等。主设备发起写请求时，网络接口在写地址通道封装并翻译地址为从设备的目标地址，作为消息的header，而写数据通道中的数据作为消息的主体。随后消息被分解为包，发送到网络中，在目的地重新组装为完整的消息，地址放入AXI写地址通道，数据放入AXI写数据通道。在收到消息的最后一个flit后，网络接口将生成一个写响应消息，并将其送回主设备，送入主设备的AXI写响应通道。乱序事务之前提到的标准中大多在最新版本放宽了总线事务的排序，例如AXI放款了请求和响应之间的顺序，即响应不需要按照与请求相同的顺序返回，如下图所示，允许多个未完成的请求同时存在，从设备以不同速度运行。AXI协议允许乱序事务一致性AMBA 4 ACE (AXI Coherency Extensions) 以及更新的 AMBA 5 CHI (Coherent Hub Interface) 提供了全系统的一致性支持，以额外通道的形式来支持各种一致性信息、snoop回复控制器、barrier的支持等，使ARM大小核之类的架构可以很方便地共享内存。" }, { "title": "《片上网络》笔记（一）：基础", "url": "/posts/NOC1/", "categories": "读书笔记, 片上网络", "tags": "", "date": "2022-05-11 17:08:00 +0000", "snippet": "《片上网络》原书英文名为 On-Chip Networks，和《处理器微架构实现》一样隶属于Synthesis Lectures on Computer Architecture系列，简单介绍了片上网络设计的一些关键概念。按照维基百科的描述，片上网络（NoC）是一种基于传统计算机网络概念的通信系统电路，最常见于SoC的各个模块之间，NoC技术将计算机网络的理论和方法应用于片上通信，提高了So...", "content": "《片上网络》原书英文名为 On-Chip Networks，和《处理器微架构实现》一样隶属于Synthesis Lectures on Computer Architecture系列，简单介绍了片上网络设计的一些关键概念。按照维基百科的描述，片上网络（NoC）是一种基于传统计算机网络概念的通信系统电路，最常见于SoC的各个模块之间，NoC技术将计算机网络的理论和方法应用于片上通信，提高了SoC的可扩展性和复杂SoC的能效。多核时代Intel Pentium 4的失败已经证明了越来越大的单核架构无法适应日益增长的性能和功耗要求，多核芯片随之出现，我们注意到现在有些面向高性能计算的处理器上已经有几十个甚至几百个核。随着内核数量增加，内核间可扩展、低延迟、高带宽的通信结构也非常重要。总线（Bus）是共享的物理通道，延迟低但带宽也低。Crossbar（交叉开关矩阵）延迟低且带宽高，但面积大且功耗高。片上网络兼顾了两者的优点，提供了可扩展性和高带宽。在多处理器片上系统（MPSoC）中，利用NoC可以帮助实现设计隔离，通过标准接口实现各个IP的即插即用的通信。基础概念NoC作为更广泛的互联网络概念的一个子集，可以看成是帮助处理器节点之间数据传输的一个可编程系统，包括各种通道、缓冲区、开关和控制器。NoC可以以低面积和低功耗的开销提供可扩展的网络，且开销和节点的数量有亚线性关系。NoC在布线方面也非常高效，可以复用同一线路传输不同的信号，提升带宽。NoC的拓扑结构通常比较规则，可以使用重复性结构进行构建和优化，减轻验证的负担。片上网络的设计可以分解为以下一些组件： 拓扑结构：网络中节点和通道之间的物理布局和连接。 路由：对一个给定的拓扑结构，路由算法决定了一个信号通过网络到达目的地的路径，直接影响了网络的吞吐量和性能。 流量控制：信号通过网络时如何分配资源，如缓冲和通道带宽。 路由器微架构：一个通用的路由器微架构由输入缓冲、路由器状态、路由逻辑、分配器和一个Crossbar（或开关）组成，通常包括一个流水线以提高吞吐量，通过片上网络中每个路由器的延迟是通信延迟的主要因素。 链接结构：节点之间如何被连接的结构。 我们也需要综合考虑网络的性能和成本。性能通常由网络延迟和带宽来衡量，成本主要包括面积和功耗。延迟与吞吐量的关系" }, { "title": "《处理器微架构实现》笔记（五）：提交", "url": "/posts/PM5/", "categories": "读书笔记, 处理器微架构实现", "tags": "", "date": "2022-05-10 23:03:00 +0000", "snippet": "在乱序流水线中，为了模拟指令的顺序执行，最常见的解决方案是在流水线的末尾实现额外的一个阶段，称为提交。在任何指令被提交之前，指令做出的任何行为都是推测性、或者叫投机性的，不会真正改变处理器的状态。通过这种方式也可以实现精确异常。对于x86等CISC处理器，一条CISC指令可能被拆分成多条微指令，当所有对应的微指令都已经执行完成时，该条CISC指令也会随之被提交并更新处理器状态。这条规则的唯一...", "content": "在乱序流水线中，为了模拟指令的顺序执行，最常见的解决方案是在流水线的末尾实现额外的一个阶段，称为提交。在任何指令被提交之前，指令做出的任何行为都是推测性、或者叫投机性的，不会真正改变处理器的状态。通过这种方式也可以实现精确异常。对于x86等CISC处理器，一条CISC指令可能被拆分成多条微指令，当所有对应的微指令都已经执行完成时，该条CISC指令也会随之被提交并更新处理器状态。这条规则的唯一例外是那些被分割成大量微操作的x86指令，如内存 拷贝指令，这类指令会在中途做周期性的部分提交。最后，由于提交是指令执行的最后阶段，一条指令所分配的各种硬件资源，如重排序缓冲（ROB）条目、内存顺序缓冲（MOB）条目或物理寄存器将会被回收。架构状态管理架构状态包括内存状态和每个寄存器的值。作为架构状态的一部分，内存不可以被随意修改。因此，所有的store操作都只有在真正提交之后才会把数据写入缓存或内存，在此之前，所有的相关信息都会被存放在store buffer中。所有的load操作都应该首先检查store buffer中是否有对应的数据，如果内存地址相匹配，那么应该直接从store buffer中获取数据，或者等待store buffer把数据写入缓存。寄存器的管理则取决于重命名的方式，我们介绍两种方法： 基于ROB与退休寄存器堆（RRF）的架构状态管理，如Intel P6、Core等处理器。 基于合并RF的管理，如Intel Pentium 4、Alpha 21264、MIPS R10000等处理器。 基于RRF的架构状态ROB与RRF在这种重命名方式中，指令提交时会把ROB中存储的值复制到RRF中，具体的实现细节在之前的章节中已经做过细致的讨论了，这里就不再展开。基于合并RF的架构状态在这种设计中我们使用一个统一的物理寄存器堆，相比基于ROB和RRF的方案，这种设计有三个主要优势： 指令提交时，不需要移动寄存器值的位置。 基于ROB的实现需要为临时的值提供存储空间，但有相当一部分指令不会写回寄存器，合并RF的设计中可以更高效地使用物理寄存器堆。 ROB本身是集中式的结构，基于ROB的实现通常都是在发射前读取RF，不利于流水线不同阶段的解耦，例如在分布式的分组执行单元与寄存器堆设计中，比较适合在发射后读取RF，合并RF可以灵活选择在发射前还是发射后读取RF。 不过这种设计也更为复杂。在ROB的设计中，可以直接按FIFO顺序分配临时空间，但是在合并RF的设计中需要一个额外的空闲列表来存储可用的物理寄存器，且物理寄存器的回收也更为复杂。推测性状态的记录执行的指令不一定会被提交，有可能是因为分支预测错误或之前的指令触发异常。对这两种情况，应该恢复推测性的状态，撤销推测性执行的指令做出的修改。从分支预测错误中恢复分支预测错误的恢复通常可以分成前端恢复和后端恢复，通常前者更为简单。前者包括冲刷所有的中间缓冲区、恢复分支预测器的历史、更新PC等。后者包括清除所有属于错误路径的指令，这些指令存放在后端的各种缓冲区，包括MOB、发射队列、ROB、重命名表等，物理寄存器与发射队列条目等资源也应该被及时回收。基于ROB和RRF处理分支预测错误在分支预测错误时，处理器不会立刻恢复推测性的状态，而是会等到错误预测的分支指令与其之前的所有指令都被提交。此时，重命名表被恢复，并从正确的路径重新开始对寄存器重命名。基于合并RF处理分支预测错误分支预测错误时，处理器可以立刻进行状态恢复。处理器会保留一份日志，记录了当一条指令被重命名时，重命名表的修改方式以及这条指令分配的资源。在分支预测错误时，处理器遍历这个日志来恢复分支指令被重命名时的正确状态。该日志的每个条目包括：这条指令写入的逻辑寄存器和分配的物理寄存器、逻辑寄存器之前对应的物理寄存器。MIPS R10000和Alpha 21264等处理器为了加速这个过程，实现了检查点机制，可以直接从检查点的指令开始遍历，实现快速分支预测错误恢复。从异常中恢复异常通常在提交时处理，原因有二：一是我们需要确定出发异常的指令不在分支预测错误的路径上，二是我们需要提供正确的处理器状态。" }, { "title": "《处理器微架构实现》笔记（四）：执行", "url": "/posts/PM4/", "categories": "读书笔记, 处理器微架构实现", "tags": "", "date": "2022-05-10 21:30:00 +0000", "snippet": "处理器执行单元执行阶段真正开始执行一条指令的计算，包括算术操作、访存、分支等，大多数乱序处理器都会采用上图中的执行单元组织结构，灰色阴影区域表示功能单元（FU），如FPU进行浮点运算，ALU进行整数的算术和逻辑运算等。除此之外，执行阶段的另一个重点是旁路网络，以支持指令背对背执行。功能单元整数算术和逻辑单元（ALU）ALU对两个整数输入进行操作，并产生一个整数结果。一些复杂的指令集在进行运算...", "content": "处理器执行单元执行阶段真正开始执行一条指令的计算，包括算术操作、访存、分支等，大多数乱序处理器都会采用上图中的执行单元组织结构，灰色阴影区域表示功能单元（FU），如FPU进行浮点运算，ALU进行整数的算术和逻辑运算等。除此之外，执行阶段的另一个重点是旁路网络，以支持指令背对背执行。功能单元整数算术和逻辑单元（ALU）ALU对两个整数输入进行操作，并产生一个整数结果。一些复杂的指令集在进行运算时还需要设置额外的标志位等，如x86。整数乘法和除法虽然也是整数操作，但由于乘法器或除法器的复杂性较高，通常被分离出来作为单独的执行单元。此外为了节省面积和功率，有些处理器使用浮点单元来执行整数乘法和除法， 如Intel Atom处理器。地址生成单元（AGU）AGU的目的在于为一个访存操作生成对应的地址。现代处理器通常有两种内存模型：线性和分段。在线性内存模型中，内存表现为一个单一的连续地址空间。在分段内存模型中，内存对程序来说是很多段的集合，一锅端定义了一个从段基地址开始的连续地址空间，程序通过逻辑地址来访问内存，一个逻辑地址包括一个段的标识符和一个段内偏移量，程序的逻辑地址需要由硬件转换为线性地址。在最复杂的x86指令集中，有六种不同的寻址模式，地址的计算由以下元素组成： Displacement：立即数，即访问内存地址的常数偏移量。 Base：一个通用寄存器的值。 Index：一个通用寄存器的值。 Scale： 常数1、2、4或8，乘上Index得到偏移量。地址的计算由如下公式表示：Offset = Base + (Index * Scale) + Displacement除了计算地址之外，AGU还需要检查偏移量是否在段的边界内。由此可见，x86计算地址非常复杂，在高频率下通常无法在一个周期内完成，可以切分成多级流水线，或者把地址的计算拆分成多个微指令。分支单元分支单元负责执行控制流指令，并产生正确的下一条指令地址，可以是有条件的或无条件的。浮点单元（FPU）FPU对来自浮点寄存器堆或内存的两个浮点值进行操作，并产生一个浮点结果，也可以进行浮点和整数之间的转换。FPU通常非常复杂，如在Pentium Pro上，FPU的面积和2个AGU、1个ALU、1个整数乘法器和1个整数除法器的总面积相同。SIMD单元SIMD即单指令多数据，在一组元素上并行执行相同操作。SIMD可以指早期一条指令处理包括大量元素的向量（对应RISC-V中的V扩展），也可以指现代处理器上的短向量（对应RISC-V中的P扩展），长度为4-16个元素。目前最流行的SIMD指令集是x86的SSE等，为x86指令集定义了16个新的SIMD寄存器，每个寄存器宽度128 bits。每个寄存器可以表示： A vector of 16 byte-sized (8b) elements. A vector of 8 word (16b) integer elements. A vector of 4 doubleword (32b) integer elements. A vector of 2 quadword (64b) integer elements. A vector of 4 single-precision floating-point (32b) elements. A vector of 2 double-precision floating-point (64b) elements.旁路网络在五级流水线中，旁路网络可以减少由于数据依赖引起的停顿，这一概念在流水线级数更多的乱序处理器中显得更为重要。如果没有旁路网络，会产生下图所示的大量气泡，严重影响性能。乱序流水线中存在数据依赖的两条指令根据流水线设计的不同，旁路网络可以相对简单，也可以非常复杂，其设计直接影响到执行阶段的面积、功耗、关键路径和物理布局，需要有大量的权衡和取舍。复杂的旁路网络可以提升IPC，但是会影响关键路径和/或功耗。乱序流水线中的数据前递两个FU的执行单元在没有旁路的情况下，FU的输入直接连接到RF的读端口，输出直接连到写端口。如果要加上旁路，每个FU的输入都要加上一个3-to-1的Mux。当设计的复杂度上升，旁路网络的复杂性也很快随之上升。一图胜千言，以下几张图直观地说明了旁路网络的设计及其效果。两个FU的执行单元，但流水线级数更多数据前递路径数据前递路径顺序流水线中的数据前递顺序流水线的旁路网络设计不一定比乱序更简单，下面几张图展示了Atom处理器的流水线与可能存在的数据前递路径。Intel Atom顺序流水线中的可能存在的数据前递路径Intel Atom顺序流水线中的可能存在的数据前递路径分组一个已经被证明有效的设计理念是在可行的情况下对关键硬件组件的布局进行分组，在不牺牲并行性的情况下尽可能提高可扩展性，如缓存设计中的阵列复制、分布式的发射队列等。旁路网络的分组下图展示了旁路网络的分组，FU0的结果不允许转发到FU1，牺牲了一定性能但是大大简化了电路。分组可以简化旁路网络寄存器堆复制的分组随着寄存器堆的读写端口数量的增加，访问延迟也会增加，可以通过分组设计解决这个问题。具体设计如下图，Alpha 21264采用这种设计，可以大大简化旁路的设计，并达到在当时看来非常高的时钟频率。简化的Alpha 21264执行单元，采用了寄存器堆复制的分组分布式队列和寄存器堆的分组在这种设计中，寄存器堆不是复制，而是分布式的分组，每组的大小都是原来的一半（包括端口数量和寄存器堆大小），这种设计也将整个执行单元、发射队列和逻辑进行了分配。这种架构提供了明确的机制来把数据从一个寄存器堆传送到另一个寄存器堆。这种类型的分组结构另一个特点是，发射队列也是分布式的，在分配阶段，指令就会被分派到不同的分组中，这个过程称为instruction steering。这种设计降低了功耗和复杂性，但是也提升了instruction steering的设计复杂度。具体的流水线如下图所示。包含了两个寄存器堆与发射队列的分组设计" }, { "title": "《处理器微架构实现》笔记（三）：分配与发射", "url": "/posts/PM3/", "categories": "读书笔记, 处理器微架构实现", "tags": "", "date": "2022-05-10 19:50:00 +0000", "snippet": "分配阶段包括两个任务：寄存器重命名和指令分派，前者的目的是消除由于寄存器的重复使用而产生的不必要的数据依赖，而后者的目的是保留一些指令执行所需的资源。发射阶段负责向FU发射指令并执行，通常可以分为顺序发射和乱序发射。分配如上所述，分配阶段由两大任务。首先是寄存器重命名，此处我们讨论的重命名只限于寄存器的范畴，事实上这个概念也适用于内存操作数，会在之后进一步讨论。寄存器重命名最早是在Tomas...", "content": "分配阶段包括两个任务：寄存器重命名和指令分派，前者的目的是消除由于寄存器的重复使用而产生的不必要的数据依赖，而后者的目的是保留一些指令执行所需的资源。发射阶段负责向FU发射指令并执行，通常可以分为顺序发射和乱序发射。分配如上所述，分配阶段由两大任务。首先是寄存器重命名，此处我们讨论的重命名只限于寄存器的范畴，事实上这个概念也适用于内存操作数，会在之后进一步讨论。寄存器重命名最早是在Tomasulo在上世纪60年代在An Efficient Algorithm for Exploiting Multiple Arithmetic Units中提出的，应用于IBM 360/91的浮点单元。现代处理器通常有三种重命名的方式：ROB重命名、Rename Buffer重命名与合并RF重命名。ROB重命名ROB重命名在这个设计中，寄存器的值既有可能存储在RF中，也可能存储在ROB中。如上图所示，Register Map Table会指明操作数应该从RF还是ROB中获取。当一条指令仍在执行时，目的操作数（即rd）的值会被存储在ROB中，在指令提交时，会把ROB中的值复制到RF中。这一方案被Intel Core 2等处理器所采用。Rename Buffer重命名这个方案的动机在于，程序中有相当一部分比例的指令（大约为三分之一，但也要看具体的程序和指令集）不会写入寄存器。在之前的方案中，ROB每一行都要留出空间存储寄存器的结果，也就是说大约有三分之一的存储空间都被浪费了。这一设计的想法在于为每条正在执行的指令的结果提供一个单独的存储空间结构，也就是Rename Buffer。和前一种方案一样，可以使用一个简单的FIFO来存储临时的寄存器数据，但也同样需要写入两次数据，第一次往Buffer里写入，第二次在指令提交时往RF写入。这一方案被IBM Power 3等处理器所采用。合并RF重命名合并RF重命名在这个设计中，只有一个单独的物理寄存器堆来存放所有的寄存器值，每个寄存器都有一个状态（空闲或者被分配，也可以有更多的状态）。空闲寄存器被记录在空闲列表中，寄存器映射关系被记录在映射表中，具体的组织结构如上图所示。在重命名阶段，我们首先查找映射表，找到rs1和rs2对应的物理寄存器。如果这条指令还需要写回寄存器，我们会从空闲列表中找到一个空闲的物理寄存器，分配给rd并写入映射表。如果没有空闲的寄存器，就需要停顿流水线。这一设计的优势在于寄存器只需要被写入一次，且源操作数的值来自单一的RF，省去了往ROB或Rename Buffer寻找的过程，缩小了连线的面积。这一方案被Alpha 21264、MIPS R12000、Pentium 4等处理器所采用。读取RF什么时候读取RF也很重要，会对设计的几个关键部分有重要影响，通常有两个选择：发射前读取和发射后读取。 发射前读取：注意到指令发射时，可能并不是所有的源操作数都已经准备好了，这些操作数需要通过旁路网络获得。这种设计的优点在于RF需要的端口比较少，但缺点在于发射队列需要存储寄存器的值，面积较大，且需要不断的移动数据，功耗较高。 发射后读取：和前一种设计相反，这种设计要求RF有更多的端口，但操作数被读取一次就不用再反复挪动了。 理论上，这两个选择和重命名方案是正交的，但也有一些需要考虑的“协同作用”，使得有些组合非常常见。比如说，ROB重命名或Rename Buffer重命名通常会和发射前读取共同使用，具体的原因如下。 In particular, the challenge comes from the fact that the register values eventually move from one location (reorder buffer or rename buffer) to another (architectural register file). In the read- after-issue scheme, the issue queue stores the identifier of the source operands. If when an instruc- tion is renamed, a source operand is in the reorder buffer or the rename buffer, the issue queue will store a pointer to that location. If the instruction that produces this source operand commits before the operand is read by the consumer, the value will be moved to the architectural register file, and the pointer stored in the issue queue will not be correct anymore, since this entry may be allocated by a different instruction. In order to correct it, it would be necessary to do an associative search in the issue queue for every committed instruction to check if any entry is pointing to its destination register. If this is the case, the pointer should be changed to the corresponding architectural register file entry. All of this is very complex in hardware. The associative search is similar to the wakeup logic described later and, on top of that, additional write ports would be required to store the new pointer. Because of this, processors that use renaming through the reorder buffer or through the rename buffer normally opt for the read-before-issue scheme.发射顺序发射顺序发射逻辑有时可以在解码阶段一并实现，通常使用Scoreboard实现，存储数据依赖和可用资源，前者对每个寄存器都存储了一个状态，表明该寄存器是否可用，后者记录了FU是否空闲，有些资源如加法器或Pipelined的乘法器总是空闲的，但有些资源如多周期的除法器则不一定。乱序发射发射逻辑决定了乱序处理器是否可以高效利用指令级别并行性（ILP），一旦源操作数可用，就立即把指令发射到对应的FU中。由于其功能的复杂性，实现一个高效且不影响关键路径的发射逻辑是非常重要的。此处我们讨论那两种单一发射队列的设计，即发射前读取RF和发射后读取RF，我们假设采用合并RF重命名方案。除了单一发射队列之外也有其他的方案，如分布式发布队列。发射前读取RF的发射过程发射前读取RF发射队列的硬件组成上图中每一个矩形都代表了一个表，行数等于发射队列中容纳的指令数量。我们假设采用RISC，最多可以有两个源操作数或一个源操作数加上一个立即数。Ctrl info保存了控制信号（如使用什么ALU、访存指令数据大小、是否使用立即数等），R1和R2表示两个源操作数是否已经可用，如果两个都为1，那么这条指令就可以发射了。接下来我们讨论具体的发射逻辑，包括发射队列分配、指令唤醒、指令选择和发射队列回收。发射队列分配发射前读取RF发射队列的流水线设计指令首先在分配阶段会被分派到发射队列，如果没有空闲的行，分配阶段就会停顿。接下来在下一个周期，指令访问寄存器堆。最后重命名后的指令被放入发射队列，并附上重命名相关的控制信号。指令唤醒唤醒是通知发射队列哪些源操作数已经准备好的一个事件，通常包括目标寄存器的物理寄存器ID、寄存器值以及一个有效位。CAM会把ID和发射队列中的每一个寄存器ID匹配，并设置相应的Ready位、把数据放到对应的位置中。这其中可能有一些细节需要处理，如对于发射前读RF的设计通常在重命名和发射队列分配之间要有一个额外的时钟周期，否则会产生死锁，具体的细节可以参考原书第6章。唤醒信号产生的时机唤醒这个概念比较有意思的一点在于，唤醒信号其实可以在实际产生一个值之前就发出，以尽量缩短Producer和Consumer之间的距离。在上图的例子中，与其到Write Back再发出唤醒信号，可以在指令在发射队列中被选中的那个周期就向其依赖的指令发出唤醒信号，之后在执行阶段可以直接通过旁路网络读取输入，以达到指令背对背的执行。需要注意的是这里我们假设FU只需要一个周期完成、且Producer的选择和Consumer的唤醒也可以在一个周期内完成。这一优化对性能至关重要，有两种常见的实现方式来产生唤醒信号。第一种方法是在指令执行完成前的三个周期在指令所在的流水线产生唤醒信号，第二种方法则是使用移位寄存器或Scoreboard。另外也需要注意的是，这种机制只有在一条指令需要的周期数是确定的情况下才有效，也就是说对于访存指令并不适用。对访存指令也有一些单独的优化技巧，比如Load指令可以用保守的方式处理，但其实也可以用更激进的方式处理。指令选择这一部分负责在发射队列中选择一部分已经准备好可以发射的指令，发射到对应的FU。由于时许的原因，选择逻辑通常会放在唤醒逻辑之后的一个周期进行，并使用仲裁器或调度器之类的组件来完成。具体的选择逻辑也有很多选项，如可以根据指令的顺序，或者也可以简单地根据发射队列中的位置进行排序等。基于仲裁器的选择逻辑的实现发射队列回收一旦一条指令被发射到FU，对应的发射队列行就可以被回收，但是也有一些激进的优化如推测性唤醒，可能需要等待这条指令真正被发射成功之后才可以回收（类似于ROB的逻辑）。发射后读取RF的发射过程发射后读取RF发射队列的硬件组成（灰色表示相比于之前的设计去掉的部分）发射后读取RF发射队列的流水线设计在这种设计中，发射队列不再需要存储寄存器的值。重命名和发射队列分配之间的流水线级数减少了，但唤醒逻辑和指令执行阶段之间需要多一个周期来读取RF。和前一种设计最重要的区别在于寄存器堆需要的端口数量，由发射宽度决定，且一般比前段的宽度更大。减少读端口寄存器堆的面积、功耗和访问延迟随着读端口的数量增加而增加，因此为了实现高能效的设计，需要尽可能减少读端口的数量。Alpha 21264分成了两个一样的寄存器堆，以减少单个寄存器堆的读端口的数量，代价是访问不同寄存器堆的Producer和Consumer无法背对背地执行。不过，大部分寄存器源操作数其实是从旁路网络读取的，且发射宽度通常不会被完全利用，因此其实可以减少寄存器读端口的数量。有两种方法：主动或被动。主动的方法中，我们计算每条被选择的指令需要的读端口的数量，如果太多，就取消发射其中一部分指令。被动的方法中，只有在需要读端口数量过多的情况下才进行处理，一部分指令被取消、放回发射队列并重新发射。注意在被动的方法中，可能会发生starvation甚至live lock，因而需要定义一个公平的发射策略。其他乱序发射的实现 分布式发射队列：每一个FU集群都有自己的发射队列，如Intel Pentium 4有两个队列：访存与非访存。 保留站：每一个FU都有自己的私有缓冲区，也就是Tomasulo在IBM 360/91中使用的方案。 访存操作的发射逻辑访存指令的数据依赖性是基于内存地址的，在重命名阶段无法确定，只有计算出地址之后才可以确定依赖关系，这种机制称为内存消歧（memory disambiguation policy），上图中展示了不同的方案，主要可以分为两类：非推测性和推测性，前者不允许在不确定依赖关系的情况下执行访存操作，而后者会对依赖关系做出预测并执行。选择一个合适的策略很重要，一方面如果过于保守会导致不必要的访存指令按序执行，降低性能，另一方面如果过于激进则会导致复杂的错误恢复机制，增大功耗。非推测性内存消歧在不确定依赖关系的情况下，这种策略不会执行任何访存操作，主要分为三种相关的策略：total ordering、load ordering with store ordering以及partial ordering。 Total ordering：所有访存操作都是按序执行的，目前没有按照这种方式实现的乱序处理器。 Load ordering with store ordering：load与store分别按序执行，但load不需要等待之前的store，如AMD K6等。 Partial ordering：load可以乱序执行，只要源操作数准备好，且之前所有的store都已经计算出访存地址，就可以发出load请求，如MIPS R10000、AMD K8等。 只要计算出访存地址，就可以进行内存消歧，因此一些处理器将store操作分为两个子任务：计算地址与存储数据。原书中给出了AMD K6和MIPS R10000的案例研究，此处就不再展开了。推测性内存消歧这种方案可以通过推测性地执行load指令来提升性能，load指令不需要等待先前的store来计算地址。注意到这个方案对依赖性进行推测，也就是说可能会发生错误，因此需要额外的硬件来识别错误并恢复正确执行。原书中给出了Alpha 21264的案例研究，此处也就不再展开了。Load指令Consumers的推测唤醒Load指令与其Consumer的流水线Load指令需要的周期数是不固定的。如上图所示，在保守的情况中，在缓存命中时才会唤醒下一条指令，会产生两个周期的气泡，但如果使用推测唤醒，就可以实现两条指令的背对背执行。不过，如果缓存未命中，Load指令的Consumer需要被放回发射队列并重新发射，且可能会导致死锁。死锁有几种解决方案。一种是冲刷流水线中所有比放回发射队列的指令年轻的指令，类似Alpha 21264中实现的机制。另一种是推迟发射队列中条目的回收，直到确定这条指令已经被发射成功，但这种方式也需要增加发射队列的大小。" }, { "title": "《处理器微架构实现》笔记（二）：取指与解码", "url": "/posts/PM2/", "categories": "读书笔记, 处理器微架构实现", "tags": "", "date": "2022-05-10 01:57:00 +0000", "snippet": "取指单元负责向处理器提供接下来要执行的指令，解码单元负责理解一条指令的语义并定义这条指令将如何被执行。取指单元主要包括一个指令缓存和相关的逻辑。高性能处理器也需要在这一阶段预测下一条指令地址，包括两个部分：分支方向与分支目标地址。下图中是一个简单的取指流水线。简单的取指流水线解码单元则负责将原始字节流分割成有效的指令，为指令生成一系列流水线控制信号，包括指令类型、执行什么操作、需要什么资源等...", "content": "取指单元负责向处理器提供接下来要执行的指令，解码单元负责理解一条指令的语义并定义这条指令将如何被执行。取指单元主要包括一个指令缓存和相关的逻辑。高性能处理器也需要在这一阶段预测下一条指令地址，包括两个部分：分支方向与分支目标地址。下图中是一个简单的取指流水线。简单的取指流水线解码单元则负责将原始字节流分割成有效的指令，为指令生成一系列流水线控制信号，包括指令类型、执行什么操作、需要什么资源等。取指指令缓存Trace缓存传统指令缓存与Trace缓存传统的缓存是按照指令在Binary中出现的顺序来存储的，但还有另一种组织方式是按照指令出现的动态顺序来存储的，也就是Trace缓存。上图说明了这两种组织的关键区别，包括数据重复与内存端口的带宽。在传统缓存中，一个数据只会出现一次，但是在Trace缓存中可能出现多次。同时，传统缓存中一个端口的带宽受限于分支的频率，相比之下Trace缓存可以连续读取数据。Branch Target Buffer (BTB) 与Return Address Stack (RAS)我们需要不断预测分支的目标地址，这一任务主要由BTB和RAS两大组件来完成，BTB负责预测普通的分支指令，而RAS主要负责预测函数的返回指令。分支方向预测分支方向预测可以通过静态（编译器或程序员完成）、动态或两者结合的方式进行。静态预测可以通过程序的profiling信息完成，但如果没有profiling信息，也可以对循环等特殊的分支做出静态预测。从硬件角度来看，静态预测很容易实现，需要对指令进行一些编码。相比之下，动态预测需要一些硬件来存储程序运行时的一些额外信息，如2位饱和计数器等。现代处理器也会使用更复杂的动态分支预测器，如2级分支预测器、GSHARE预测器、混合分支预测器等。混合分支预测器在某些场景下很有效，比如上下文切换之后，全局预测器的效果更好，但过了一段时间之后，局部预测器效果会更好，可以使用混合预测器在两种模式之间灵活切换。解码x86 ISAx86指令格式RISC的解码相对来说非常简单，且通常可以在一个周期内完成，事实上这也是RISC的最初设计目标之一。相比之下，作为CISC代表的x86指令集，解码则会更复杂一些。上图展示了x86指令的格式，一条x86指令最多包括4个prefix字节，1-3个opcode，可选的寻址指定符，有些指令还有立即数。如果我们想并行解码超过一条指令，我们就必须首先知道每条指令的长度，因此能够快速计算指令长度对x86指令集来说很重要，这个过程中存在两个问题。一是opcode不总是在指令开头的同一偏移量中，可以是前5个字节的任何地方；二是opcode本身大小也是可变的，最多是3个字节。对于寄存器编码也非常复杂，如用3个bit我们只能对8个通用寄存器编码，但是在32位模式下，一个数值为0的操作数可以解释为AL、AX、EAX、MM0或XMM0。可见，CISC的解码非常复杂，接下来我们将会讨论高性能乱序超标量处理器中的解码单元实现。动态翻译一条x86指令可以包含很多信息，如 add [ax], ebx 这条指令包括了以下一些操作： 使用EAX和数据段寄存器DS计算出内存操作数的地址。 找到对应内存位置的值，加上寄存器EBX的值。 将加法的结果写回1中计算的内存位置。 乱序处理器需要大量的控制信号来跟踪每个时间点上指令执行的阶段，如果我们想提升性能，最好把指令执行的某些部分与其他指令并行化，比如地址的计算不应该依赖于前面的指令为寄存器EBX产生正确的值。相比之下，RISC指令集就会把这一条指令分解成三条简单的指令，可以更容易地被乱序处理器处理。由于x86指令集的复杂性，x86处理器会在解码单元动态地将指令流翻译成类似RISC的指令，最早可以追溯至AMD K5和Intel P6，Intel将这种类似RISC的内部指令称为micro-operation，即微指令。在P6中，微指令有固定长度（118位）和格式，注意到微指令并不是真正的一条RISC指令，而更类似于解码之后的RISC指令。高性能x86解码Intel Nehalem解码流水线上图展示了Intel Nehalem架构的解码单元，可以看出，X86解码是一个多周期的操作。这个过程被分为两个阶段：指令长度解码（ILD）和动态翻译，这两个阶段通过一个指令队列（IQ）解耦，可以隐藏ILD中的气泡。指令长度解码ILD单元从prefetch buffers中取出对齐的16字节，执行一些基本的预编码，确定指令长度，对前缀进行解码，标记一些指令属性。这一过程是顺序的，大多数常见指令都可以在一个周期内完成，但有如下两种情况需要6个周期来处理： An operand-size override prefix, preceding an instruction with a word immediate. An address-size override prefix, preceding an instruction with a ModR /M byte. 动态翻译单元在这个阶段，指令从IQ中取出，并翻译为微指令，上图的设计中包括3个简单解码器（一条指令转换为一个微指令）和一个复杂解码器（一条指令转换为四个微指令）。有一些特殊的指令（如字符串指令）可能需要超过4个微指令，这些指令会被发送到复杂解码器，停顿解码流水线，并由MSROM单元控制。MSROM会接收这样的复杂指令，并输出一串微指令，来模拟复杂的x86指令。" }, { "title": "《处理器微架构实现》笔记（一）：缓存", "url": "/posts/PM1/", "categories": "读书笔记, 处理器微架构实现", "tags": "", "date": "2022-05-10 00:25:00 +0000", "snippet": "《处理器微架构实现》（英文名为Processor Microarchitecture: An Implementation Perspective）是CMU 18-447 Introduction to Computer Architecture推荐阅读的一本教材。首先介绍的内容是缓存，关于缓存的基本概念在网络上可以找到很多介绍，这里就不再详细展开了。我们在此只讨论一些缓存具体的实现问题。地...", "content": "《处理器微架构实现》（英文名为Processor Microarchitecture: An Implementation Perspective）是CMU 18-447 Introduction to Computer Architecture推荐阅读的一本教材。首先介绍的内容是缓存，关于缓存的基本概念在网络上可以找到很多介绍，这里就不再详细展开了。我们在此只讨论一些缓存具体的实现问题。地址翻译我们访问内存的地址通常分为虚拟地址（Virtual Address）和物理地址（Physical Address）。其实还有有效地址（Effective Address），但是这个概念现在已经很少有人提到了，就不再单独讨论了。虚拟地址有诸多好处，如减少了程序员访问内存的工作量、实现对内存的保护等，现在最主流的实现虚拟地址的方式是分页，采用页表进行管理。由于不同的虚拟页面可能指向同一个物理页面，会发生别名（Alias）的问题，缓存需要正确处理这一问题。当程序发出load或store指令时，我们需要多次访问内存。因此除了缓存之外，我们还需要TLB来对地址翻译的过程进行加速。在一些处理器（如Alpha系列处理器）中，TLB完全是由软件控制的，而在x86架构中，TLB由硬件控制，对操作系统透明。缓存结构并行与串行访问并行访问Tag与Data的流水线串行访问Tag与Data的流水线对Tag与Data的访问可以并行或串行进行。在第一种情况下，我们在访问Tag Array的同时，从Data Array中读取数据，并判断缓存是否命中，以及选择Set中的哪一个缓存行。最后，地址的偏移量用来决定取出数据中的哪些部分将被送回处理器。在第二种情况下，我们首先访问Tag Array，然后再根据Tag来决定访问哪一行数据，我们可以注意到这种设计的一大好处是减少了Mux的使用，关键路径更短，功耗更低。非阻塞缓存对阻塞式的缓存而言，在L1缓存未命中时，整个流水线都需要等待其从更高级的缓存或内存中取回数据。这样的设计比较简单，但是会严重影响性能（尤其是对于乱序处理器而言）。相比之下，非阻塞缓存允许缓存未命中时，流水线仍然可以继续处理其他的内存访问，这就需要一些跟踪这些未命中的指令的机制（如Scoreboard）。这一概念由Kroft在Lockup-Free Instruction Fetch/Prefetch Cache Organization一文中首先提出，采用了名为miss status holding registers (MSHR)的寄存器来存放Cache Miss指令的信息。除此之外还有一个input stack来存放取回但还没有写入到缓存中的数据（因此也叫做fill buffer）。在这样的设计中，Cache Miss会存在以下三种情况： 首次Miss：第一次Miss一个缓存行，将访问更高一级的缓存或内存。 第二次Miss：和之前的Miss访问同一个缓存行。 结构性Miss：由于MSHR容量有限，无法容纳更多的Miss指令，需要停顿流水线。 MSHR也有多种不同的实现方式，复杂性与性能各不相同，我们讨论三种不同的设计。Implicitly addressed MSHR第一种是implicitly addressed MSHR，这是最简单的MSHR设计，由Kroft提出，如上图所示。每个MSHR包括一个Miss缓存行的地址与valid bit，以及相应的比较器。如果一个缓存行有N个words，那么MSHR也包括N个条目，每个条目都包含Miss指令的目标寄存器与一些格式信息（如果load数据的大小，是否为零扩展或符号扩展等）。Explicitly addressed MSHR第二种是explicitly addressed MSHR。我们注意到第一种设计中，对每个缓存行只能容纳一条Miss指令，如果有第二条Miss的指令，就会有结构性Miss。在第二种设计中，每个MSHR都对应了M个条目，也就是说可以容纳M条同时Miss同一个缓存行的指令，每个条目都有对应的缓存块内偏移量。第三种设计是in-cache MSHR，优点是可以减少保存MSHR信息所需的存储量。我们注意到缓存行在等待被重新填充时，也可以用作MSHR的存储。在这样的设计中，Tag Array需要添加额外的一位（transient bit）来表示当前这一缓存行是否被作为MSHR来使用。多端口缓存为了提升带宽，很多现代处理器都可以在每个周期内发出多条访存指令。我们首先研究一个真正的双端口缓存设计，接下来研究商业处理器如何尝试尽可能接近这种理想设计。理想的多端口缓存设计对于一个真正的双端口缓存来说，所有的东西都有两份：有两个地址解码器、Tag比较器等等，可以实现在每个周期内对缓存读取两次（注意写入两次不一定代表有两个写入端口，只要保证一个缓存行在同一周期内不会被写入两次就可以了）。这样的设计会大大增加缓存访问时间，可能会影响关键路径，增大面积等。Array Replication这种设计类似理想设计，但我们重复了Tag与Data Array，也就是用两个单端口的缓存来模拟一个双端口缓存。这种设计面积和真正的双端口设计面积相差不大，但面积还是非常大。Virtual MultiportingIBM Power2和Alpha 21264采用了这种设计。原理是利用了时分多路复用，在一个周期内对一个单端口缓存进行多次访问。在Alpha 21264中，有两个读端口，第一个读端口在时钟周期的前半部分访问缓存，第二个则在后半周期。这种设计无法扩展，现代处理器的时钟频率很高，没办法把一个周期拆成两半来用。Multibanking这种设计将缓存分成多个小的阵列（叫做bank）。在一个周期内，如果多个load请求访问了不同的bank，那么就可以同时处理这些load请求。我们仍然需要两个地址解码器、Mux、Tag比较器等，但是和理想设计相比，Tag与Data array不需要有多端口。这种设计是当今高性能处理器中模拟多端口的首选方法。MIPS R10000实现了这种方法，允许每个周期有两个load和一个store请求，32KB的数据缓存被分为两个bank，2路组相联，每个缓存块为32B，每个缓存bank逻辑上被分为两个物理阵列，如下图所示。Data array bank中的数据组织方式指令缓存由于程序访问指令缓存的模式更简单，指令缓存通常设计也更为简单。指令缓存通常都是单端口的、阻塞式的，后者是因为指令的访问必须是按照顺序的，实现非阻塞缓存没有任何意义。当然，我们也需要仔细权衡，采用并行还是串行的方式来访问Tag和Data，以及采用什么样的相联度。" } ]
